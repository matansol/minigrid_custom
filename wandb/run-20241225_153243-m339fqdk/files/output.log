Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Loaded model from models\orig_easy8_20241111\iter_1000000_steps. Continuing training.
Logging to ./logs/ppo/minigrid_custom_tensorboard/20241225_2
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000001DF37DB1C60> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000001DF37DE4A90>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
reached max steps=250
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.7     |
|    ep_rew_mean     | 4.41     |
| time/              |          |
|    fps             | 259      |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 27.1       |
|    ep_rew_mean          | 4.88       |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 2          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.13261588 |
|    clip_fraction        | 0.212      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.289     |
|    explained_variance   | -0.0265    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 9770       |
|    policy_gradient_loss | 0.00291    |
|    value_loss           | 4.02       |
----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.7       |
|    ep_rew_mean          | 4.83       |
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 3          |
|    time_elapsed         | 38         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.11114141 |
|    clip_fraction        | 0.21       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.279     |
|    explained_variance   | 0.191      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 9780       |
|    policy_gradient_loss | -0.000118  |
|    value_loss           | 4.34       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.8       |
|    ep_rew_mean          | 5.07       |
| time/                   |            |
|    fps                  | 137        |
|    iterations           | 4          |
|    time_elapsed         | 59         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.17310919 |
|    clip_fraction        | 0.249      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.392     |
|    explained_variance   | 0.258      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 9790       |
|    policy_gradient_loss | 0.00637    |
|    value_loss           | 4.33       |
----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=10000, episode_reward=-10.67 +/- 17.81
Episode length: 85.00 +/- 48.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 85         |
|    mean_reward          | -10.7      |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.07658549 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.343     |
|    explained_variance   | 0.175      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.63       |
|    n_updates            | 9800       |
|    policy_gradient_loss | -0.00847   |
|    value_loss           | 4.36       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 4.55     |
| time/              |          |
|    fps             | 122      |
|    iterations      | 5        |
|    time_elapsed    | 83       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.9       |
|    ep_rew_mean          | 4.44       |
| time/                   |            |
|    fps                  | 119        |
|    iterations           | 6          |
|    time_elapsed         | 102        |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.11231369 |
|    clip_fraction        | 0.282      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.417     |
|    explained_variance   | 0.25       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.89       |
|    n_updates            | 9810       |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 3.99       |
----------------------------------------
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.2       |
|    ep_rew_mean          | 5.78       |
| time/                   |            |
|    fps                  | 115        |
|    iterations           | 7          |
|    time_elapsed         | 124        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.10087946 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.332     |
|    explained_variance   | 0.0819     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.26       |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.00188   |
|    value_loss           | 4.35       |
----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.7       |
|    ep_rew_mean          | 3.83       |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 8          |
|    time_elapsed         | 145        |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.12650318 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.347     |
|    explained_variance   | -0.0205    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 9830       |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 3.69       |
----------------------------------------
reached max steps=250
reached max steps=250
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 30.9      |
|    ep_rew_mean          | 3.89      |
| time/                   |           |
|    fps                  | 111       |
|    iterations           | 9         |
|    time_elapsed         | 165       |
|    total_timesteps      | 18432     |
| train/                  |           |
|    approx_kl            | 0.1598326 |
|    clip_fraction        | 0.375     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.468    |
|    explained_variance   | 0.227     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.15      |
|    n_updates            | 9840      |
|    policy_gradient_loss | -0.0153   |
|    value_loss           | 2.72      |
---------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=20000, episode_reward=-24.00 +/- 2.00
Episode length: 125.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | -24         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.093455434 |
|    clip_fraction        | 0.314       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.486      |
|    explained_variance   | 0.0417      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.58        |
|    n_updates            | 9850        |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 3.53        |
-----------------------------------------
reached max steps=250
reached max steps=250
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.1     |
|    ep_rew_mean     | 2.8      |
| time/              |          |
|    fps             | 108      |
|    iterations      | 10       |
|    time_elapsed    | 189      |
|    total_timesteps | 20480    |
---------------------------------
reached max steps=250
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 36         |
|    ep_rew_mean          | 2.53       |
| time/                   |            |
|    fps                  | 108        |
|    iterations           | 11         |
|    time_elapsed         | 207        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.09678364 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.486     |
|    explained_variance   | 0.0637     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 9860       |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 3.95       |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 30.3     |
|    ep_rew_mean          | 3.78     |
| time/                   |          |
|    fps                  | 108      |
|    iterations           | 12       |
|    time_elapsed         | 226      |
|    total_timesteps      | 24576    |
| train/                  |          |
|    approx_kl            | 0.103614 |
|    clip_fraction        | 0.308    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.476   |
|    explained_variance   | -0.244   |
|    learning_rate        | 0.0003   |
|    loss                 | 1.32     |
|    n_updates            | 9870     |
|    policy_gradient_loss | -0.00888 |
|    value_loss           | 3.93     |
--------------------------------------
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.6       |
|    ep_rew_mean          | 4.4        |
| time/                   |            |
|    fps                  | 107        |
|    iterations           | 13         |
|    time_elapsed         | 247        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.07874274 |
|    clip_fraction        | 0.257      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.483     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.6        |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.00619   |
|    value_loss           | 3.59       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.1       |
|    ep_rew_mean          | 4.57       |
| time/                   |            |
|    fps                  | 107        |
|    iterations           | 14         |
|    time_elapsed         | 267        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.11959808 |
|    clip_fraction        | 0.259      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.455     |
|    explained_variance   | 0.0721     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.14       |
|    n_updates            | 9890       |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 3.71       |
----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=30000, episode_reward=-13.17 +/- 12.60
Episode length: 82.60 +/- 52.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 82.6        |
|    mean_reward          | -13.2       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.052269578 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.464      |
|    explained_variance   | 0.117       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.37        |
|    n_updates            | 9900        |
|    policy_gradient_loss | -0.0031     |
|    value_loss           | 3.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.7     |
|    ep_rew_mean     | 4.33     |
| time/              |          |
|    fps             | 105      |
|    iterations      | 15       |
|    time_elapsed    | 290      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 26.3        |
|    ep_rew_mean          | 4.6         |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 16          |
|    time_elapsed         | 311         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.056214646 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.45       |
|    explained_variance   | 0.0833      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 9910        |
|    policy_gradient_loss | -0.00471    |
|    value_loss           | 3.25        |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 25.1      |
|    ep_rew_mean          | 5.1       |
| time/                   |           |
|    fps                  | 105       |
|    iterations           | 17        |
|    time_elapsed         | 330       |
|    total_timesteps      | 34816     |
| train/                  |           |
|    approx_kl            | 0.0761939 |
|    clip_fraction        | 0.199     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.451    |
|    explained_variance   | 0.131     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.883     |
|    n_updates            | 9920      |
|    policy_gradient_loss | -0.00539  |
|    value_loss           | 2.94      |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.2        |
|    ep_rew_mean          | 5           |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 18          |
|    time_elapsed         | 349         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.060483307 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.46       |
|    explained_variance   | 0.101       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.03        |
|    n_updates            | 9930        |
|    policy_gradient_loss | -0.00679    |
|    value_loss           | 3.48        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.9       |
|    ep_rew_mean          | 5.42       |
| time/                   |            |
|    fps                  | 105        |
|    iterations           | 19         |
|    time_elapsed         | 370        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.07218112 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.466     |
|    explained_variance   | -0.0368    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.65       |
|    n_updates            | 9940       |
|    policy_gradient_loss | -0.00441   |
|    value_loss           | 3.97       |
----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=40000, episode_reward=-13.01 +/- 12.78
Episode length: 81.80 +/- 52.91
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 81.8       |
|    mean_reward          | -13        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.08701806 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.431     |
|    explained_variance   | 0.0815     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.35       |
|    n_updates            | 9950       |
|    policy_gradient_loss | -0.00501   |
|    value_loss           | 3.97       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 5.15     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 20       |
|    time_elapsed    | 394      |
|    total_timesteps | 40960    |
---------------------------------
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 27.2       |
|    ep_rew_mean          | 4.32       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 21         |
|    time_elapsed         | 414        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.06270565 |
|    clip_fraction        | 0.213      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.469     |
|    explained_variance   | -0.116     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.56       |
|    n_updates            | 9960       |
|    policy_gradient_loss | -0.00665   |
|    value_loss           | 3.25       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.8       |
|    ep_rew_mean          | 4.62       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 22         |
|    time_elapsed         | 434        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.08727826 |
|    clip_fraction        | 0.251      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.478     |
|    explained_variance   | 0.0728     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.598      |
|    n_updates            | 9970       |
|    policy_gradient_loss | 0.00269    |
|    value_loss           | 3.08       |
----------------------------------------
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.5       |
|    ep_rew_mean          | 4.81       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 23         |
|    time_elapsed         | 454        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06452094 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.466     |
|    explained_variance   | -0.0387    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 9980       |
|    policy_gradient_loss | 0.00625    |
|    value_loss           | 3.34       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 25.9       |
|    ep_rew_mean          | 5.33       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 24         |
|    time_elapsed         | 474        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.17905876 |
|    clip_fraction        | 0.312      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.446     |
|    explained_variance   | 0.000532   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 9990       |
|    policy_gradient_loss | -0.0222    |
|    value_loss           | 3.14       |
----------------------------------------
reached max steps=250
reached max steps=250
Eval num_timesteps=50000, episode_reward=-0.89 +/- 12.28
Episode length: 38.00 +/- 43.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38          |
|    mean_reward          | -0.894      |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.094505936 |
|    clip_fraction        | 0.274       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.455      |
|    explained_variance   | -0.343      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.905       |
|    n_updates            | 10000       |
|    policy_gradient_loss | 0.000426    |
|    value_loss           | 3.62        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.2     |
|    ep_rew_mean     | 3.64     |
| time/              |          |
|    fps             | 103      |
|    iterations      | 25       |
|    time_elapsed    | 495      |
|    total_timesteps | 51200    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.9       |
|    ep_rew_mean          | 4.88       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 26         |
|    time_elapsed         | 516        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.07353194 |
|    clip_fraction        | 0.24       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.485     |
|    explained_variance   | 0.057      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.47       |
|    n_updates            | 10010      |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 3.95       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.1       |
|    ep_rew_mean          | 4.99       |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 27         |
|    time_elapsed         | 537        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.05606202 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.416     |
|    explained_variance   | 0.112      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.12       |
|    n_updates            | 10020      |
|    policy_gradient_loss | -0.00407   |
|    value_loss           | 3.4        |
----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.7       |
|    ep_rew_mean          | 4.33       |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 28         |
|    time_elapsed         | 557        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.08319798 |
|    clip_fraction        | 0.209      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.434     |
|    explained_variance   | 0.146      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.07       |
|    n_updates            | 10030      |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 3.69       |
----------------------------------------
reached max steps=250
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.6        |
|    ep_rew_mean          | 3.95        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 29          |
|    time_elapsed         | 577         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.086729795 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.463      |
|    explained_variance   | 0.256       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.22        |
|    n_updates            | 10040       |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 3.59        |
-----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=60000, episode_reward=-24.00 +/- 2.00
Episode length: 125.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 125         |
|    mean_reward          | -24         |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.049497038 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.419      |
|    explained_variance   | 0.254       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.07        |
|    n_updates            | 10050       |
|    policy_gradient_loss | -0.00235    |
|    value_loss           | 4.05        |
-----------------------------------------
reached max steps=250
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 30.1     |
|    ep_rew_mean     | 3.91     |
| time/              |          |
|    fps             | 102      |
|    iterations      | 30       |
|    time_elapsed    | 601      |
|    total_timesteps | 61440    |
---------------------------------
reached max steps=250
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.6        |
|    ep_rew_mean          | 3.48        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 31          |
|    time_elapsed         | 620         |
|    total_timesteps      | 63488       |
| train/                  |             |
|    approx_kl            | 0.075837426 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.043       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 10060       |
|    policy_gradient_loss | -0.0042     |
|    value_loss           | 3.69        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25          |
|    ep_rew_mean          | 5.03        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 32          |
|    time_elapsed         | 642         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.059808277 |
|    clip_fraction        | 0.227       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.388      |
|    explained_variance   | -0.109      |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.00657    |
|    value_loss           | 3.18        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.8        |
|    ep_rew_mean          | 5.12        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 33          |
|    time_elapsed         | 661         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.054716267 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.388      |
|    explained_variance   | 0.148       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.26        |
|    n_updates            | 10080       |
|    policy_gradient_loss | 0.000425    |
|    value_loss           | 3.49        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 5.36       |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 34         |
|    time_elapsed         | 680        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.06913011 |
|    clip_fraction        | 0.225      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.379     |
|    explained_variance   | -0.0292    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.8        |
|    n_updates            | 10090      |
|    policy_gradient_loss | 0.00529    |
|    value_loss           | 3.52       |
----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=70000, episode_reward=-11.80 +/- 16.17
Episode length: 80.80 +/- 54.14
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 80.8        |
|    mean_reward          | -11.8       |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.069606796 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.35       |
|    explained_variance   | -0.0338     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 10100       |
|    policy_gradient_loss | 0.0004      |
|    value_loss           | 3.08        |
-----------------------------------------
reached max steps=250
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.2     |
|    ep_rew_mean     | 4.75     |
| time/              |          |
|    fps             | 102      |
|    iterations      | 35       |
|    time_elapsed    | 700      |
|    total_timesteps | 71680    |
---------------------------------
reached max steps=250
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.4        |
|    ep_rew_mean          | 5.34        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 36          |
|    time_elapsed         | 719         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.050178744 |
|    clip_fraction        | 0.214       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.369      |
|    explained_variance   | 0.207       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.484       |
|    n_updates            | 10110       |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 2.39        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.6       |
|    ep_rew_mean          | 5.1        |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 37         |
|    time_elapsed         | 740        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.06613533 |
|    clip_fraction        | 0.219      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.384     |
|    explained_variance   | -0.0989    |
|    learning_rate        | 0.0003     |
|    loss                 | 1.42       |
|    n_updates            | 10120      |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 3.14       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 5.43       |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 38         |
|    time_elapsed         | 760        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.05248951 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.38      |
|    explained_variance   | -0.00268   |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 10130      |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 2.86       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.2        |
|    ep_rew_mean          | 4.83        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 39          |
|    time_elapsed         | 779         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.049531832 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.375      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.751       |
|    n_updates            | 10140       |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 2.64        |
-----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=80000, episode_reward=-11.76 +/- 14.38
Episode length: 80.60 +/- 54.38
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 80.6      |
|    mean_reward          | -11.8     |
| time/                   |           |
|    total_timesteps      | 80000     |
| train/                  |           |
|    approx_kl            | 0.0457794 |
|    clip_fraction        | 0.206     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.392    |
|    explained_variance   | 0.16      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.52      |
|    n_updates            | 10150     |
|    policy_gradient_loss | -0.00781  |
|    value_loss           | 2.9       |
---------------------------------------
reached max steps=250
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 5.1      |
| time/              |          |
|    fps             | 102      |
|    iterations      | 40       |
|    time_elapsed    | 799      |
|    total_timesteps | 81920    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.4       |
|    ep_rew_mean          | 5.46       |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 41         |
|    time_elapsed         | 817        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.12528603 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.373     |
|    explained_variance   | 0.206      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.778      |
|    n_updates            | 10160      |
|    policy_gradient_loss | -0.0227    |
|    value_loss           | 2.72       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 5.25        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 42          |
|    time_elapsed         | 836         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.054906007 |
|    clip_fraction        | 0.198       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.394      |
|    explained_variance   | 0.413       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.569       |
|    n_updates            | 10170       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 2.76        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 5.28        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 43          |
|    time_elapsed         | 857         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.040834963 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.355      |
|    explained_variance   | 0.12        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.66        |
|    n_updates            | 10180       |
|    policy_gradient_loss | -0.00117    |
|    value_loss           | 2.1         |
-----------------------------------------
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
reached max steps=250
Eval num_timesteps=90000, episode_reward=-19.30 +/- 11.41
Episode length: 102.40 +/- 45.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 102         |
|    mean_reward          | -19.3       |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.032136917 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.336      |
|    explained_variance   | 0.162       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.961       |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.00137    |
|    value_loss           | 2.46        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 4.78     |
| time/              |          |
|    fps             | 102      |
|    iterations      | 44       |
|    time_elapsed    | 879      |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 5.85        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 45          |
|    time_elapsed         | 899         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.041223843 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.341      |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.566       |
|    n_updates            | 10200       |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 1.84        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 5.47        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 46          |
|    time_elapsed         | 917         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.029674465 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.323      |
|    explained_variance   | 0.286       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.892       |
|    n_updates            | 10210       |
|    policy_gradient_loss | -0.00669    |
|    value_loss           | 2.24        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 6.35        |
| time/                   |             |
|    fps                  | 102         |
|    iterations           | 47          |
|    time_elapsed         | 934         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.039283864 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.481       |
|    n_updates            | 10220       |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 1.98        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | 5.85       |
| time/                   |            |
|    fps                  | 0          |
|    iterations           | 48         |
|    time_elapsed         | 323446     |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.03438633 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.317     |
|    explained_variance   | 0.327      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.585      |
|    n_updates            | 10230      |
|    policy_gradient_loss | -0.00956   |
|    value_loss           | 2.22       |
----------------------------------------
Traceback (most recent call last):
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 367, in <module>
    main()
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 323, in main
    model.learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 202, in collect_rollouts
    actions, values, log_probs = self.policy(obs_tensor)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\torch\nn\modules\module.py", line 1844, in _call_impl
    return inner()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\torch\nn\modules\module.py", line 1790, in inner
    result = forward_call(*args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\policies.py", line 654, in forward
    distribution = self._get_action_dist_from_latent(latent_pi)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\policies.py", line 697, in _get_action_dist_from_latent
    return self.action_dist.proba_distribution(action_logits=mean_actions)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\distributions.py", line 288, in proba_distribution
    self.distribution = Categorical(logits=action_logits)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\torch\distributions\categorical.py", line 66, in __init__
    self.logits = logits - logits.logsumexp(dim=-1, keepdim=True)
KeyboardInterrupt
