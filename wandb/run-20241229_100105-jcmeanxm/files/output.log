Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Loaded model from models\orig_easy8_20241111\iter_1000000_steps. Continuing training.
Logging to ./logs/ppo/minigrid_custom_tensorboard/20241229_1
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x0000018B798A48E0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000018B73414EE0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.6     |
|    ep_rew_mean     | 6.54     |
| time/              |          |
|    fps             | 52       |
|    iterations      | 1        |
|    time_elapsed    | 39       |
|    total_timesteps | 2048     |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 27.8     |
|    ep_rew_mean          | 5.43     |
| time/                   |          |
|    fps                  | 67       |
|    iterations           | 2        |
|    time_elapsed         | 60       |
|    total_timesteps      | 4096     |
| train/                  |          |
|    approx_kl            | 0.113225 |
|    clip_fraction        | 0.194    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.234   |
|    explained_variance   | 0.279    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.526    |
|    n_updates            | 9770     |
|    policy_gradient_loss | -0.00537 |
|    value_loss           | 2.04     |
--------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.8       |
|    ep_rew_mean          | 6.59       |
| time/                   |            |
|    fps                  | 71         |
|    iterations           | 3          |
|    time_elapsed         | 85         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.17454194 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.276     |
|    explained_variance   | 0.463      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.05       |
|    n_updates            | 9780       |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 1.93       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.2        |
|    ep_rew_mean          | 6.59        |
| time/                   |             |
|    fps                  | 78          |
|    iterations           | 4           |
|    time_elapsed         | 103         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.056306317 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | 0.486       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.773       |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.00388    |
|    value_loss           | 1.59        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=10000, episode_reward=0.24 +/- 7.25
Episode length: 29.33 +/- 15.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 0.239       |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.078489736 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.241      |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.493       |
|    n_updates            | 9800        |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 1.05        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 6.89     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 5        |
|    time_elapsed    | 123      |
|    total_timesteps | 10240    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.8        |
|    ep_rew_mean          | 6.35        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 6           |
|    time_elapsed         | 142         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.043124944 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.271       |
|    n_updates            | 9810        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.785       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.4       |
|    ep_rew_mean          | 6.66       |
| time/                   |            |
|    fps                  | 89         |
|    iterations           | 7          |
|    time_elapsed         | 161        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.05969201 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.237     |
|    explained_variance   | 0.541      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.678      |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.0202    |
|    value_loss           | 1.28       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.1        |
|    ep_rew_mean          | 6.93        |
| time/                   |             |
|    fps                  | 90          |
|    iterations           | 8           |
|    time_elapsed         | 180         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.069423854 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.49        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.399       |
|    n_updates            | 9830        |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 1.03        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.7       |
|    ep_rew_mean          | 6.31       |
| time/                   |            |
|    fps                  | 91         |
|    iterations           | 9          |
|    time_elapsed         | 201        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06430821 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.206     |
|    explained_variance   | 0.391      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.322      |
|    n_updates            | 9840       |
|    policy_gradient_loss | -0.0221    |
|    value_loss           | 1.09       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=20000, episode_reward=-2.57 +/- 6.73
Episode length: 39.00 +/- 15.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39         |
|    mean_reward          | -2.57      |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.11262526 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.675      |
|    n_updates            | 9850       |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 1.06       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 6.33     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 10       |
|    time_elapsed    | 220      |
|    total_timesteps | 20480    |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 6.99        |
| time/                   |             |
|    fps                  | 92          |
|    iterations           | 11          |
|    time_elapsed         | 242         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.044965904 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.328       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.32        |
|    n_updates            | 9860        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 1.17        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 6.92       |
| time/                   |            |
|    fps                  | 93         |
|    iterations           | 12         |
|    time_elapsed         | 263        |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.03713198 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.229     |
|    explained_variance   | 0.473      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.283      |
|    n_updates            | 9870       |
|    policy_gradient_loss | -0.0273    |
|    value_loss           | 0.67       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.2       |
|    ep_rew_mean          | 7.2        |
| time/                   |            |
|    fps                  | 93         |
|    iterations           | 13         |
|    time_elapsed         | 283        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.08088489 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.194     |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.448      |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 0.757      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.6       |
|    ep_rew_mean          | 6.93       |
| time/                   |            |
|    fps                  | 93         |
|    iterations           | 14         |
|    time_elapsed         | 305        |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.12613815 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.179     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.413      |
|    n_updates            | 9890       |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.879      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=30000, episode_reward=2.28 +/- 7.29
Episode length: 30.67 +/- 13.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.7        |
|    mean_reward          | 2.28        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.051575147 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.455       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.307       |
|    n_updates            | 9900        |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.996       |
-----------------------------------------
New best mean reward!
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 6.97     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 15       |
|    time_elapsed    | 329      |
|    total_timesteps | 30720    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | 6.6        |
| time/                   |            |
|    fps                  | 93         |
|    iterations           | 16         |
|    time_elapsed         | 349        |
|    total_timesteps      | 32768      |
| train/                  |            |
|    approx_kl            | 0.08119105 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.402      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.247      |
|    n_updates            | 9910       |
|    policy_gradient_loss | 0.00813    |
|    value_loss           | 0.769      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.17       |
| time/                   |            |
|    fps                  | 94         |
|    iterations           | 17         |
|    time_elapsed         | 367        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.09121659 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.172     |
|    explained_variance   | 0.509      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.293      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 1.1        |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 6.61        |
| time/                   |             |
|    fps                  | 95          |
|    iterations           | 18          |
|    time_elapsed         | 386         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.048325013 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.243       |
|    n_updates            | 9930        |
|    policy_gradient_loss | -0.00471    |
|    value_loss           | 0.798       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.2        |
|    ep_rew_mean          | 6.96        |
| time/                   |             |
|    fps                  | 96          |
|    iterations           | 19          |
|    time_elapsed         | 403         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.087218724 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.52        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.481       |
|    n_updates            | 9940        |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.96        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=40000, episode_reward=2.64 +/- 6.21
Episode length: 29.00 +/- 14.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 2.64        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.053860754 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.37        |
|    n_updates            | 9950        |
|    policy_gradient_loss | 0.00252     |
|    value_loss           | 0.792       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.04     |
| time/              |          |
|    fps             | 97       |
|    iterations      | 20       |
|    time_elapsed    | 422      |
|    total_timesteps | 40960    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 6.59        |
| time/                   |             |
|    fps                  | 97          |
|    iterations           | 21          |
|    time_elapsed         | 441         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.052555084 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.222       |
|    n_updates            | 9960        |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.656       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.16        |
| time/                   |             |
|    fps                  | 97          |
|    iterations           | 22          |
|    time_elapsed         | 460         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.062001012 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 9970        |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.827       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 6.96        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 23          |
|    time_elapsed         | 477         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.049700372 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.196       |
|    n_updates            | 9980        |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.645       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 24          |
|    time_elapsed         | 497         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.053922754 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.236       |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.933       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=8.67 +/- 0.74
Episode length: 18.33 +/- 1.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 8.67       |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.04242594 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.241      |
|    n_updates            | 10000      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.446      |
----------------------------------------
New best mean reward!
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 6.92     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 25       |
|    time_elapsed    | 516      |
|    total_timesteps | 51200    |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.3       |
|    ep_rew_mean          | 6.8        |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 26         |
|    time_elapsed         | 533        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.13379821 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.434      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.366      |
|    n_updates            | 10010      |
|    policy_gradient_loss | -0.0263    |
|    value_loss           | 0.987      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.9       |
|    ep_rew_mean          | 6.87       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 27         |
|    time_elapsed         | 552        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.07060942 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.22       |
|    n_updates            | 10020      |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 0.863      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.39        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 28          |
|    time_elapsed         | 571         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.040083617 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.539       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 10030       |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.773       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.37        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 29          |
|    time_elapsed         | 589         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.079227425 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0863      |
|    n_updates            | 10040       |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.615       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=60000, episode_reward=7.14 +/- 0.37
Episode length: 25.33 +/- 4.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.3       |
|    mean_reward          | 7.14       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.09665842 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.128     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.237      |
|    n_updates            | 10050      |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.597      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 7.08     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 30       |
|    time_elapsed    | 608      |
|    total_timesteps | 61440    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.3       |
|    ep_rew_mean          | 6.89       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 31         |
|    time_elapsed         | 630        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.19763047 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.333      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.347      |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.753      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 32          |
|    time_elapsed         | 649         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.066423565 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.497       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.306       |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.871       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 7.04       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 33         |
|    time_elapsed         | 674        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.07216138 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.129     |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.164      |
|    n_updates            | 10080      |
|    policy_gradient_loss | 0.0596     |
|    value_loss           | 0.752      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.2       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 34         |
|    time_elapsed         | 697        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.17269622 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.173      |
|    n_updates            | 10090      |
|    policy_gradient_loss | -0.0242    |
|    value_loss           | 0.478      |
----------------------------------------
Eval num_timesteps=70000, episode_reward=7.93 +/- 0.10
Episode length: 18.67 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.7       |
|    mean_reward          | 7.93       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.06106094 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.496      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.12       |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 0.733      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 99       |
|    iterations      | 35       |
|    time_elapsed    | 717      |
|    total_timesteps | 71680    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.5       |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 36         |
|    time_elapsed         | 737        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.06747074 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 0.535      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.254      |
|    n_updates            | 10110      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.597      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.3       |
|    ep_rew_mean          | 7.13       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 37         |
|    time_elapsed         | 756        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.06041726 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.111     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0938     |
|    n_updates            | 10120      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.507      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 38         |
|    time_elapsed         | 775        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.06276928 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.175      |
|    n_updates            | 10130      |
|    policy_gradient_loss | 0.00827    |
|    value_loss           | 0.652      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.3        |
|    ep_rew_mean          | 7.14        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 39          |
|    time_elapsed         | 795         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.056673583 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 10140       |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.447       |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=8.02 +/- 1.28
Episode length: 21.33 +/- 3.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.3       |
|    mean_reward          | 8.02       |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.08535123 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.418      |
|    n_updates            | 10150      |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.849      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 40       |
|    time_elapsed    | 814      |
|    total_timesteps | 81920    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22         |
|    ep_rew_mean          | 7.13       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 41         |
|    time_elapsed         | 833        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.07254666 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.198      |
|    n_updates            | 10160      |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.823      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.56        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 42          |
|    time_elapsed         | 853         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.055479277 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.129       |
|    n_updates            | 10170       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.559       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.35       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 43         |
|    time_elapsed         | 873        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.08029152 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.197      |
|    n_updates            | 10180      |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 0.508      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=90000, episode_reward=6.77 +/- 0.34
Episode length: 16.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 6.77        |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.055925764 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.16       |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.282       |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.622       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 6.74     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 44       |
|    time_elapsed    | 892      |
|    total_timesteps | 90112    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.2         |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 45          |
|    time_elapsed         | 913         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.040019304 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.479       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.266       |
|    n_updates            | 10200       |
|    policy_gradient_loss | -0.00894    |
|    value_loss           | 0.909       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.24        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 46          |
|    time_elapsed         | 932         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.049712636 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.273       |
|    n_updates            | 10210       |
|    policy_gradient_loss | 0.000366    |
|    value_loss           | 0.779       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.44        |
| time/                   |             |
|    fps                  | 101         |
|    iterations           | 47          |
|    time_elapsed         | 950         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.033373266 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.228       |
|    n_updates            | 10220       |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.883       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.9       |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 101        |
|    iterations           | 48         |
|    time_elapsed         | 972        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.04410579 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.131     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0831     |
|    n_updates            | 10230      |
|    policy_gradient_loss | -0.02      |
|    value_loss           | 0.666      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=100000, episode_reward=2.43 +/- 6.07
Episode length: 30.00 +/- 14.24
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 2.43       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.05281841 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.183      |
|    n_updates            | 10240      |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.652      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 6.83     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 49       |
|    time_elapsed    | 994      |
|    total_timesteps | 100352   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.38       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 50         |
|    time_elapsed         | 1014       |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.05082782 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.546      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.336      |
|    n_updates            | 10250      |
|    policy_gradient_loss | -0.0245    |
|    value_loss           | 1.21       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.4       |
|    ep_rew_mean          | 6.72       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 51         |
|    time_elapsed         | 1035       |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.04822378 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.505      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.166      |
|    n_updates            | 10260      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.651      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 52         |
|    time_elapsed         | 1056       |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.08579026 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.457      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.306      |
|    n_updates            | 10270      |
|    policy_gradient_loss | -0.0278    |
|    value_loss           | 1.19       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.6       |
|    ep_rew_mean          | 7.08       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 53         |
|    time_elapsed         | 1075       |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.07159664 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.604      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.136      |
|    n_updates            | 10280      |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.652      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=110000, episode_reward=6.92 +/- 2.26
Episode length: 15.67 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 6.92       |
| time/                   |            |
|    total_timesteps      | 110000     |
| train/                  |            |
|    approx_kl            | 0.04529833 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.178      |
|    n_updates            | 10290      |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.883      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 100      |
|    iterations      | 54       |
|    time_elapsed    | 1095     |
|    total_timesteps | 110592   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.5       |
|    ep_rew_mean          | 7.2        |
| time/                   |            |
|    fps                  | 101        |
|    iterations           | 55         |
|    time_elapsed         | 1114       |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.04909023 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.243      |
|    n_updates            | 10300      |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.828      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.3       |
|    ep_rew_mean          | 6.26       |
| time/                   |            |
|    fps                  | 101        |
|    iterations           | 56         |
|    time_elapsed         | 1132       |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.05602791 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.432      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.251      |
|    n_updates            | 10310      |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 0.942      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 6.97        |
| time/                   |             |
|    fps                  | 101         |
|    iterations           | 57          |
|    time_elapsed         | 1155        |
|    total_timesteps      | 116736      |
| train/                  |             |
|    approx_kl            | 0.048951585 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.537       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.365       |
|    n_updates            | 10320       |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 1.35        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 58         |
|    time_elapsed         | 1177       |
|    total_timesteps      | 118784     |
| train/                  |            |
|    approx_kl            | 0.03974229 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.138     |
|    explained_variance   | 0.478      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.29       |
|    n_updates            | 10330      |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 0.81       |
----------------------------------------
reached max steps=100
Eval num_timesteps=120000, episode_reward=7.00 +/- 2.33
Episode length: 18.33 +/- 3.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 7           |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.041083947 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.423       |
|    n_updates            | 10340       |
|    policy_gradient_loss | -0.00991    |
|    value_loss           | 0.766       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 7.21     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 59       |
|    time_elapsed    | 1200     |
|    total_timesteps | 120832   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 60          |
|    time_elapsed         | 1221        |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.055268336 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.227       |
|    n_updates            | 10350       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.599       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.05        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 61          |
|    time_elapsed         | 1242        |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.044659976 |
|    clip_fraction        | 0.0973      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.428       |
|    n_updates            | 10360       |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.729       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 100        |
|    iterations           | 62         |
|    time_elapsed         | 1261       |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.05082068 |
|    clip_fraction        | 0.095      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.608      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.252      |
|    n_updates            | 10370      |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.651      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.8         |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 63          |
|    time_elapsed         | 1282        |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.043779157 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 10380       |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.444       |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=7.48 +/- 1.76
Episode length: 17.67 +/- 2.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 7.48        |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.034687325 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.365       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 64       |
|    time_elapsed    | 1303     |
|    total_timesteps | 131072   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 7.42        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 65          |
|    time_elapsed         | 1322        |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.054541588 |
|    clip_fraction        | 0.0881      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.173       |
|    n_updates            | 10400       |
|    policy_gradient_loss | -0.00373    |
|    value_loss           | 0.556       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 7.1         |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 66          |
|    time_elapsed         | 1343        |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.034250148 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.262       |
|    n_updates            | 10410       |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.649       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.55        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 67          |
|    time_elapsed         | 1366        |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.058564685 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.179       |
|    n_updates            | 10420       |
|    policy_gradient_loss | -0.00558    |
|    value_loss           | 0.986       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.22        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 68          |
|    time_elapsed         | 1388        |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.035985187 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 10430       |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.55        |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=7.92 +/- 0.69
Episode length: 15.67 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 7.92       |
| time/                   |            |
|    total_timesteps      | 140000     |
| train/                  |            |
|    approx_kl            | 0.06042304 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.153     |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.472      |
|    n_updates            | 10440      |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.915      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 7.03     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 69       |
|    time_elapsed    | 1411     |
|    total_timesteps | 141312   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.01        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 70          |
|    time_elapsed         | 1433        |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.038533174 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.22        |
|    n_updates            | 10450       |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.885       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 6.84        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 71          |
|    time_elapsed         | 1454        |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.028290715 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.38        |
|    n_updates            | 10460       |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.819       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 72         |
|    time_elapsed         | 1474       |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.03712099 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.314      |
|    n_updates            | 10470      |
|    policy_gradient_loss | -0.0216    |
|    value_loss           | 0.742      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 6.98        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 73          |
|    time_elapsed         | 1497        |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.042746257 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.559       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.136       |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.418       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=150000, episode_reward=7.86 +/- 1.78
Episode length: 19.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 7.86        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.087703966 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.173       |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.65        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.25     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 74       |
|    time_elapsed    | 1519     |
|    total_timesteps | 151552   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 75          |
|    time_elapsed         | 1539        |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.055562414 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0753      |
|    n_updates            | 10500       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.617       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.52       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 76         |
|    time_elapsed         | 1558       |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.03645287 |
|    clip_fraction        | 0.0988     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.592      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.185      |
|    n_updates            | 10510      |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.655      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 6.98       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 77         |
|    time_elapsed         | 1577       |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.03551398 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.185      |
|    n_updates            | 10520      |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.539      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | 7.31       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 78         |
|    time_elapsed         | 1597       |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.08169572 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.178     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.204      |
|    n_updates            | 10530      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.636      |
----------------------------------------
Eval num_timesteps=160000, episode_reward=8.45 +/- 1.47
Episode length: 19.33 +/- 3.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.3       |
|    mean_reward          | 8.45       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.06442298 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.17      |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.197      |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.0164    |
|    value_loss           | 0.746      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 99       |
|    iterations      | 79       |
|    time_elapsed    | 1621     |
|    total_timesteps | 161792   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 7.24        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 80          |
|    time_elapsed         | 1643        |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.032914467 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.512       |
|    n_updates            | 10550       |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.821       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.9        |
|    ep_rew_mean          | 7.29        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 81          |
|    time_elapsed         | 1666        |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.029334862 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.506       |
|    n_updates            | 10560       |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.866       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.33        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 82          |
|    time_elapsed         | 1688        |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.064930715 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.417       |
|    n_updates            | 10570       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 1.04        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.58        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 83          |
|    time_elapsed         | 1708        |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.035489045 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.208       |
|    n_updates            | 10580       |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.538       |
-----------------------------------------
Eval num_timesteps=170000, episode_reward=7.80 +/- 0.92
Episode length: 22.33 +/- 5.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.3        |
|    mean_reward          | 7.8         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.020242319 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0968      |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.373       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 7.77     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 84       |
|    time_elapsed    | 1730     |
|    total_timesteps | 172032   |
---------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.2      |
|    ep_rew_mean          | 7.63      |
| time/                   |           |
|    fps                  | 99        |
|    iterations           | 85        |
|    time_elapsed         | 1753      |
|    total_timesteps      | 174080    |
| train/                  |           |
|    approx_kl            | 0.0441553 |
|    clip_fraction        | 0.0868    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.116    |
|    explained_variance   | 0.681     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0873    |
|    n_updates            | 10600     |
|    policy_gradient_loss | 0.00136   |
|    value_loss           | 0.484     |
---------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.22        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 86          |
|    time_elapsed         | 1775        |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.033918217 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.555       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0794      |
|    n_updates            | 10610       |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.532       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 87         |
|    time_elapsed         | 1795       |
|    total_timesteps      | 178176     |
| train/                  |            |
|    approx_kl            | 0.05566505 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.428      |
|    n_updates            | 10620      |
|    policy_gradient_loss | -0.0139    |
|    value_loss           | 0.885      |
----------------------------------------
reached max steps=100
Eval num_timesteps=180000, episode_reward=8.02 +/- 0.55
Episode length: 21.33 +/- 6.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.3        |
|    mean_reward          | 8.02        |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.051354513 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0697      |
|    n_updates            | 10630       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.515       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 88       |
|    time_elapsed    | 1815     |
|    total_timesteps | 180224   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 89          |
|    time_elapsed         | 1837        |
|    total_timesteps      | 182272      |
| train/                  |             |
|    approx_kl            | 0.048918754 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 10640       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.568       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.17       |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 90         |
|    time_elapsed         | 1862       |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.06243797 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.142     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.157      |
|    n_updates            | 10650      |
|    policy_gradient_loss | -0.0186    |
|    value_loss           | 0.593      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 91         |
|    time_elapsed         | 1883       |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.05779732 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.248      |
|    n_updates            | 10660      |
|    policy_gradient_loss | -0.023     |
|    value_loss           | 0.542      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 92         |
|    time_elapsed         | 1904       |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.04492742 |
|    clip_fraction        | 0.0892     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.126     |
|    explained_variance   | 0.705      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.154      |
|    n_updates            | 10670      |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 0.528      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=190000, episode_reward=5.24 +/- 5.13
Episode length: 29.33 +/- 14.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.3       |
|    mean_reward          | 5.24       |
| time/                   |            |
|    total_timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.04656486 |
|    clip_fraction        | 0.0854     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.147      |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.345      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 93       |
|    time_elapsed    | 1925     |
|    total_timesteps | 190464   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.74        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 94          |
|    time_elapsed         | 1945        |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.036148954 |
|    clip_fraction        | 0.0977      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0934      |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.475       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.13       |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 95         |
|    time_elapsed         | 1969       |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.03554157 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 10700      |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 0.459      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.11        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 96          |
|    time_elapsed         | 1988        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.045839302 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.553       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.227       |
|    n_updates            | 10710       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.984       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 97         |
|    time_elapsed         | 2010       |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.04347603 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.192     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.289      |
|    n_updates            | 10720      |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 0.883      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=200000, episode_reward=7.86 +/- 1.57
Episode length: 19.00 +/- 3.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 7.86        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.042858586 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.453       |
|    n_updates            | 10730       |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.927       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 98       |
|    iterations      | 98       |
|    time_elapsed    | 2031     |
|    total_timesteps | 200704   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 99          |
|    time_elapsed         | 2050        |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.036125902 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 10740       |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.722       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.34        |
| time/                   |             |
|    fps                  | 98          |
|    iterations           | 100         |
|    time_elapsed         | 2069        |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.041368075 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0687      |
|    n_updates            | 10750       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.431       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.44       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 101        |
|    time_elapsed         | 2088       |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.10114513 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.101      |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.0283    |
|    value_loss           | 0.637      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.43       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 102        |
|    time_elapsed         | 2108       |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.04288033 |
|    clip_fraction        | 0.0997     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 10770      |
|    policy_gradient_loss | -0.0072    |
|    value_loss           | 0.415      |
----------------------------------------
reached max steps=100
Eval num_timesteps=210000, episode_reward=3.53 +/- 6.83
Episode length: 28.00 +/- 15.75
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | 3.53        |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.041543696 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.233       |
|    n_updates            | 10780       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.588       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.44     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 103      |
|    time_elapsed    | 2130     |
|    total_timesteps | 210944   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 104         |
|    time_elapsed         | 2148        |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.052775387 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.238       |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.601       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.08        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 105         |
|    time_elapsed         | 2167        |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.043466516 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0856      |
|    n_updates            | 10800       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.38        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 106         |
|    time_elapsed         | 2188        |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.059994694 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.758       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.22        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 107         |
|    time_elapsed         | 2207        |
|    total_timesteps      | 219136      |
| train/                  |             |
|    approx_kl            | 0.039255463 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.154       |
|    n_updates            | 10820       |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.619       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=220000, episode_reward=5.38 +/- 5.26
Episode length: 28.67 +/- 15.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.7       |
|    mean_reward          | 5.38       |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.04977209 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.213     |
|    explained_variance   | 0.633      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.281      |
|    n_updates            | 10830      |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.941      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 7.68     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 108      |
|    time_elapsed    | 2226     |
|    total_timesteps | 221184   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.57        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 109         |
|    time_elapsed         | 2246        |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.053870246 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.236       |
|    n_updates            | 10840       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.47        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 110        |
|    time_elapsed         | 2265       |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.06817913 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.132      |
|    n_updates            | 10850      |
|    policy_gradient_loss | -0.0205    |
|    value_loss           | 0.435      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.3         |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 111         |
|    time_elapsed         | 2285        |
|    total_timesteps      | 227328      |
| train/                  |             |
|    approx_kl            | 0.055188205 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.083       |
|    n_updates            | 10860       |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 0.781       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 112        |
|    time_elapsed         | 2308       |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.05408206 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.174     |
|    explained_variance   | 0.625      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.207      |
|    n_updates            | 10870      |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 0.858      |
----------------------------------------
Eval num_timesteps=230000, episode_reward=8.96 +/- 0.96
Episode length: 17.00 +/- 0.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17        |
|    mean_reward          | 8.96      |
| time/                   |           |
|    total_timesteps      | 230000    |
| train/                  |           |
|    approx_kl            | 0.0872852 |
|    clip_fraction        | 0.12      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.137    |
|    explained_variance   | 0.584     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0383    |
|    n_updates            | 10880     |
|    policy_gradient_loss | -0.0182   |
|    value_loss           | 0.435     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 113      |
|    time_elapsed    | 2326     |
|    total_timesteps | 231424   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.5        |
|    ep_rew_mean          | 7.08        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 114         |
|    time_elapsed         | 2345        |
|    total_timesteps      | 233472      |
| train/                  |             |
|    approx_kl            | 0.036635116 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.186       |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.417       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 115        |
|    time_elapsed         | 2365       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.09194315 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.213     |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.413      |
|    n_updates            | 10900      |
|    policy_gradient_loss | -0.0233    |
|    value_loss           | 0.762      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.25       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 116        |
|    time_elapsed         | 2383       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.05493562 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.139      |
|    n_updates            | 10910      |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.541      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 117        |
|    time_elapsed         | 2402       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.13079944 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.373      |
|    n_updates            | 10920      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.637      |
----------------------------------------
reached max steps=100
Eval num_timesteps=240000, episode_reward=3.67 +/- 5.45
Episode length: 27.33 +/- 16.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.3        |
|    mean_reward          | 3.67        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.048820022 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.645       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 10930       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.715       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.16     |
| time/              |          |
|    fps             | 99       |
|    iterations      | 118      |
|    time_elapsed    | 2424     |
|    total_timesteps | 241664   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 119         |
|    time_elapsed         | 2454        |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.036814682 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.222       |
|    n_updates            | 10940       |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.677       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 120        |
|    time_elapsed         | 2472       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.03919418 |
|    clip_fraction        | 0.0986     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0756     |
|    n_updates            | 10950      |
|    policy_gradient_loss | -0.0126    |
|    value_loss           | 0.578      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 121         |
|    time_elapsed         | 2491        |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.029662233 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0682      |
|    n_updates            | 10960       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.37        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 122        |
|    time_elapsed         | 2512       |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.05815948 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.144     |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.147      |
|    n_updates            | 10970      |
|    policy_gradient_loss | -0.0186    |
|    value_loss           | 0.367      |
----------------------------------------
Eval num_timesteps=250000, episode_reward=7.64 +/- 0.80
Episode length: 20.00 +/- 4.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 7.64        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.030385194 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.503       |
-----------------------------------------
Traceback (most recent call last):
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 358, in <module>
    main()
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 314, in main
    model.learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 224, in collect_rollouts
    if not callback.on_step():
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 223, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\integration\sb3\sb3.py", line 136, in _on_step
    self.save_model()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\integration\sb3\sb3.py", line 145, in save_model
    wandb.save(self.path, base_path=self.model_save_path)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 392, in wrapper_fn
    return func(self, *args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 382, in wrapper
    return func(self, *args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 1963, in save
    return self._save(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 2022, in _save
    target_path.symlink_to(source_path)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\pathlib.py", line 1255, in symlink_to
    self._accessor.symlink(target, self, target_is_directory)
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\Users\\matan\\master_thesis\\minigrid_custom\\models\\wandb_models\\(2, 2, 2, -3, 0.2)\\model.zip' -> 'C:\\Users\\matan\\master_thesis\\minigrid_custom\\wandb\\run-20241229_100105-jcmeanxm\\files\\model.zip'
