Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Loaded model from models\orig_easy8_20241111\iter_1000000_steps. Continuing training.
Logging to ./logs/ppo/minigrid_custom_tensorboard/20241230_1
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x0000020909D44340> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000020909AB4EE0>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 6.56     |
| time/              |          |
|    fps             | 117      |
|    iterations      | 1        |
|    time_elapsed    | 17       |
|    total_timesteps | 2048     |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.5       |
|    ep_rew_mean          | 6.08       |
| time/                   |            |
|    fps                  | 136        |
|    iterations           | 2          |
|    time_elapsed         | 30         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.05222504 |
|    clip_fraction        | 0.181      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.292      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.672      |
|    n_updates            | 9770       |
|    policy_gradient_loss | -0.00854   |
|    value_loss           | 1.53       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 26.6        |
|    ep_rew_mean          | 5.64        |
| time/                   |             |
|    fps                  | 143         |
|    iterations           | 3           |
|    time_elapsed         | 42          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.080891594 |
|    clip_fraction        | 0.189       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.252      |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.365       |
|    n_updates            | 9780        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 1.45        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.3        |
|    ep_rew_mean          | 6.01        |
| time/                   |             |
|    fps                  | 145         |
|    iterations           | 4           |
|    time_elapsed         | 56          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.059728116 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.329      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.672       |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 1.58        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=10000, episode_reward=-0.48 +/- 6.44
Episode length: 41.67 +/- 11.79
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41.7       |
|    mean_reward          | -0.483     |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.09429833 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.275     |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.471      |
|    n_updates            | 9800       |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 1.51       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.3     |
|    ep_rew_mean     | 6.25     |
| time/              |          |
|    fps             | 144      |
|    iterations      | 5        |
|    time_elapsed    | 70       |
|    total_timesteps | 10240    |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.2        |
|    ep_rew_mean          | 7.08        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 6           |
|    time_elapsed         | 83          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.079591736 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.219      |
|    explained_variance   | 0.442       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.535       |
|    n_updates            | 9810        |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 1.39        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.9       |
|    ep_rew_mean          | 6.47       |
| time/                   |            |
|    fps                  | 147        |
|    iterations           | 7          |
|    time_elapsed         | 97         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.06216326 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.184     |
|    explained_variance   | 0.432      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.292      |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 0.808      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 148         |
|    iterations           | 8           |
|    time_elapsed         | 110         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.050200194 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.211      |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.287       |
|    n_updates            | 9830        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 1.25        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.9       |
|    ep_rew_mean          | 7.18       |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 9          |
|    time_elapsed         | 121        |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.06089959 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.473      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.219      |
|    n_updates            | 9840       |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.642      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=20000, episode_reward=-1.00 +/- 6.16
Episode length: 41.00 +/- 12.73
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 41         |
|    mean_reward          | -1         |
| time/                   |            |
|    total_timesteps      | 20000      |
| train/                  |            |
|    approx_kl            | 0.07348058 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.194     |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.226      |
|    n_updates            | 9850       |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.589      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 7.23     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 10       |
|    time_elapsed    | 133      |
|    total_timesteps | 20480    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.48        |
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 11          |
|    time_elapsed         | 146         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.061850064 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.361       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0967      |
|    n_updates            | 9860        |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.788       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.9        |
|    ep_rew_mean          | 6.83        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 12          |
|    time_elapsed         | 159         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.056497596 |
|    clip_fraction        | 0.0954      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.536       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.196       |
|    n_updates            | 9870        |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.753       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.9       |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 13         |
|    time_elapsed         | 173        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.08230705 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.273      |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 1.1        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.8      |
|    ep_rew_mean          | 6.94      |
| time/                   |           |
|    fps                  | 151       |
|    iterations           | 14        |
|    time_elapsed         | 188       |
|    total_timesteps      | 28672     |
| train/                  |           |
|    approx_kl            | 0.0975057 |
|    clip_fraction        | 0.126     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.165    |
|    explained_variance   | 0.454     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.294     |
|    n_updates            | 9890      |
|    policy_gradient_loss | -0.0234   |
|    value_loss           | 0.826     |
---------------------------------------
Eval num_timesteps=30000, episode_reward=5.79 +/- 0.74
Episode length: 19.33 +/- 3.40
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.3       |
|    mean_reward          | 5.79       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.06486012 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.25       |
|    n_updates            | 9900       |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.782      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 7.14     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 15       |
|    time_elapsed    | 201      |
|    total_timesteps | 30720    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.36        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 16          |
|    time_elapsed         | 214         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.091903985 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.237       |
|    n_updates            | 9910        |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.802       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.7       |
|    ep_rew_mean          | 6.87       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 17         |
|    time_elapsed         | 226        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.08336902 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.13      |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.0199    |
|    value_loss           | 0.685      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.9        |
|    ep_rew_mean          | 6.82        |
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 18          |
|    time_elapsed         | 239         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.047233343 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.426       |
|    n_updates            | 9930        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 1.04        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.8      |
|    ep_rew_mean          | 7.17      |
| time/                   |           |
|    fps                  | 154       |
|    iterations           | 19        |
|    time_elapsed         | 251       |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.0759579 |
|    clip_fraction        | 0.135     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.147    |
|    explained_variance   | 0.489     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.514     |
|    n_updates            | 9940      |
|    policy_gradient_loss | -0.0129   |
|    value_loss           | 1.13      |
---------------------------------------
reached max steps=100
Eval num_timesteps=40000, episode_reward=3.25 +/- 6.55
Episode length: 32.33 +/- 13.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.3      |
|    mean_reward          | 3.25      |
| time/                   |           |
|    total_timesteps      | 40000     |
| train/                  |           |
|    approx_kl            | 0.0442679 |
|    clip_fraction        | 0.0876    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.12     |
|    explained_variance   | 0.538     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.514     |
|    n_updates            | 9950      |
|    policy_gradient_loss | -0.0025   |
|    value_loss           | 1.19      |
---------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.47     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 20       |
|    time_elapsed    | 264      |
|    total_timesteps | 40960    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 21         |
|    time_elapsed         | 275        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.06700265 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.422      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.178      |
|    n_updates            | 9960       |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.676      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.1        |
|    ep_rew_mean          | 7.04        |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 22          |
|    time_elapsed         | 288         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.048127145 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.09        |
|    n_updates            | 9970        |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.453       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 23          |
|    time_elapsed         | 301         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.112160355 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.564       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.243       |
|    n_updates            | 9980        |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.784       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 7.5        |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 24         |
|    time_elapsed         | 315        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.04730732 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.248      |
|    n_updates            | 9990       |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.563      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=7.35 +/- 0.54
Episode length: 21.33 +/- 5.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 21.3      |
|    mean_reward          | 7.35      |
| time/                   |           |
|    total_timesteps      | 50000     |
| train/                  |           |
|    approx_kl            | 0.0865293 |
|    clip_fraction        | 0.0831    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.12     |
|    explained_variance   | 0.62      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.139     |
|    n_updates            | 10000     |
|    policy_gradient_loss | -0.00814  |
|    value_loss           | 0.79      |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 25       |
|    time_elapsed    | 328      |
|    total_timesteps | 51200    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.1        |
|    ep_rew_mean          | 7.19        |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 26          |
|    time_elapsed         | 342         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.045722574 |
|    clip_fraction        | 0.085       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.326       |
|    n_updates            | 10010       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.542       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 27         |
|    time_elapsed         | 354        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.33859012 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.347      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.146      |
|    n_updates            | 10020      |
|    policy_gradient_loss | -0.0301    |
|    value_loss           | 0.692      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 28         |
|    time_elapsed         | 365        |
|    total_timesteps      | 57344      |
| train/                  |            |
|    approx_kl            | 0.12964997 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.217      |
|    n_updates            | 10030      |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 0.532      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.1         |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 29          |
|    time_elapsed         | 377         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.037364345 |
|    clip_fraction        | 0.0931      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 10040       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.607       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=60000, episode_reward=8.22 +/- 1.47
Episode length: 17.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 8.22       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.05240085 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.596      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.162      |
|    n_updates            | 10050      |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 0.925      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.8     |
|    ep_rew_mean     | 7.41     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 30       |
|    time_elapsed    | 390      |
|    total_timesteps | 61440    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.53       |
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 31         |
|    time_elapsed         | 403        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.05110722 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.326      |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 0.913      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23          |
|    ep_rew_mean          | 7.24        |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 32          |
|    time_elapsed         | 418         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.058616098 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.195       |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.785       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 33         |
|    time_elapsed         | 431        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.03988094 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.185      |
|    n_updates            | 10080      |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 0.638      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 34         |
|    time_elapsed         | 443        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.16497551 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0578     |
|    n_updates            | 10090      |
|    policy_gradient_loss | -0.0195    |
|    value_loss           | 0.558      |
----------------------------------------
reached max steps=100
Eval num_timesteps=70000, episode_reward=8.96 +/- 1.43
Episode length: 17.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 8.96        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.042003438 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0833      |
|    n_updates            | 10100       |
|    policy_gradient_loss | -0.00988    |
|    value_loss           | 0.367       |
-----------------------------------------
New best mean reward!
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 7.16     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 35       |
|    time_elapsed    | 456      |
|    total_timesteps | 71680    |
---------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.7      |
|    ep_rew_mean          | 7.19      |
| time/                   |           |
|    fps                  | 156       |
|    iterations           | 36        |
|    time_elapsed         | 471       |
|    total_timesteps      | 73728     |
| train/                  |           |
|    approx_kl            | 0.0714601 |
|    clip_fraction        | 0.148     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.178    |
|    explained_variance   | 0.566     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.2       |
|    n_updates            | 10110     |
|    policy_gradient_loss | -0.0264   |
|    value_loss           | 0.723     |
---------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 6.95        |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 37          |
|    time_elapsed         | 485         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.061680865 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.273       |
|    n_updates            | 10120       |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.654       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 38         |
|    time_elapsed         | 499        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.08004761 |
|    clip_fraction        | 0.158      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.324      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.164      |
|    n_updates            | 10130      |
|    policy_gradient_loss | -0.0271    |
|    value_loss           | 0.704      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 7.13        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 39          |
|    time_elapsed         | 516         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.060896166 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 10140       |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.646       |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=6.44 +/- 1.34
Episode length: 16.33 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 6.44        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.064138114 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.277       |
|    n_updates            | 10150       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.804       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 7.26     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 40       |
|    time_elapsed    | 534      |
|    total_timesteps | 81920    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.3        |
|    ep_rew_mean          | 7.37        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 41          |
|    time_elapsed         | 554         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.057428986 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.378       |
|    n_updates            | 10160       |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.823       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.1      |
|    ep_rew_mean          | 7.35      |
| time/                   |           |
|    fps                  | 150       |
|    iterations           | 42        |
|    time_elapsed         | 572       |
|    total_timesteps      | 86016     |
| train/                  |           |
|    approx_kl            | 0.1160167 |
|    clip_fraction        | 0.148     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.179    |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.25      |
|    n_updates            | 10170     |
|    policy_gradient_loss | -0.0231   |
|    value_loss           | 0.618     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.49        |
| time/                   |             |
|    fps                  | 149         |
|    iterations           | 43          |
|    time_elapsed         | 589         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.040653445 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0816      |
|    n_updates            | 10180       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.427       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=90000, episode_reward=9.05 +/- 1.03
Episode length: 19.67 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 9.05        |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.063647635 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.208       |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.554       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.6     |
|    ep_rew_mean     | 7.29     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 44       |
|    time_elapsed    | 607      |
|    total_timesteps | 90112    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 147        |
|    iterations           | 45         |
|    time_elapsed         | 624        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.07182507 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.191     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.213      |
|    n_updates            | 10200      |
|    policy_gradient_loss | -0.0242    |
|    value_loss           | 0.583      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.52        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 46          |
|    time_elapsed         | 641         |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.075026326 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0495      |
|    n_updates            | 10210       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.515       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 145        |
|    iterations           | 47         |
|    time_elapsed         | 660        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.07434486 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.454      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 144        |
|    iterations           | 48         |
|    time_elapsed         | 678        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.08086125 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.133      |
|    n_updates            | 10230      |
|    policy_gradient_loss | -0.0231    |
|    value_loss           | 0.754      |
----------------------------------------
Eval num_timesteps=100000, episode_reward=7.87 +/- 0.49
Episode length: 22.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22         |
|    mean_reward          | 7.87       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.06011447 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.558      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.163      |
|    n_updates            | 10240      |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.806      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.98     |
| time/              |          |
|    fps             | 144      |
|    iterations      | 49       |
|    time_elapsed    | 695      |
|    total_timesteps | 100352   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 143         |
|    iterations           | 50          |
|    time_elapsed         | 713         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.038354382 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 10250       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.442       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23         |
|    ep_rew_mean          | 7.22       |
| time/                   |            |
|    fps                  | 143        |
|    iterations           | 51         |
|    time_elapsed         | 729        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.07597599 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.562      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.227      |
|    n_updates            | 10260      |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 0.737      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 142        |
|    iterations           | 52         |
|    time_elapsed         | 749        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.04556809 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.552      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.288      |
|    n_updates            | 10270      |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.752      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.49       |
| time/                   |            |
|    fps                  | 141        |
|    iterations           | 53         |
|    time_elapsed         | 766        |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.07937694 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.605      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.277      |
|    n_updates            | 10280      |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.72       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=110000, episode_reward=2.12 +/- 8.68
Episode length: 28.33 +/- 15.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.3        |
|    mean_reward          | 2.12        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.027908951 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.263       |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.649       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 7.38     |
| time/              |          |
|    fps             | 141      |
|    iterations      | 54       |
|    time_elapsed    | 783      |
|    total_timesteps | 110592   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 6.64        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 55          |
|    time_elapsed         | 801         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.037266046 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.221       |
|    n_updates            | 10300       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.823       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 6.94        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 56          |
|    time_elapsed         | 821         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.039293166 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.287       |
|    n_updates            | 10310       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 1.03        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 57         |
|    time_elapsed         | 838        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.15456098 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.397      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.141      |
|    n_updates            | 10320      |
|    policy_gradient_loss | -0.0248    |
|    value_loss           | 0.706      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 58          |
|    time_elapsed         | 854         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.049819984 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.529       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.215       |
|    n_updates            | 10330       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.644       |
-----------------------------------------
Eval num_timesteps=120000, episode_reward=8.45 +/- 1.53
Episode length: 19.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 8.45        |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.058574554 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 10340       |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.491       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.7     |
|    ep_rew_mean     | 6.94     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 59       |
|    time_elapsed    | 872      |
|    total_timesteps | 120832   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 60         |
|    time_elapsed         | 890        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.13786387 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.138     |
|    explained_variance   | 0.464      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.173      |
|    n_updates            | 10350      |
|    policy_gradient_loss | -0.00903   |
|    value_loss           | 1.03       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.45        |
| time/                   |             |
|    fps                  | 137         |
|    iterations           | 61          |
|    time_elapsed         | 908         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.034648653 |
|    clip_fraction        | 0.094       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.353       |
|    n_updates            | 10360       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.707       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 137         |
|    iterations           | 62          |
|    time_elapsed         | 926         |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.041644078 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.211       |
|    n_updates            | 10370       |
|    policy_gradient_loss | -0.00206    |
|    value_loss           | 0.803       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 136        |
|    iterations           | 63         |
|    time_elapsed         | 942        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.04087563 |
|    clip_fraction        | 0.0956     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 10380      |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 0.627      |
----------------------------------------
Eval num_timesteps=130000, episode_reward=8.52 +/- 0.63
Episode length: 19.00 +/- 1.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19         |
|    mean_reward          | 8.52       |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.08344163 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.466      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.111      |
|    n_updates            | 10390      |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.775      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 136      |
|    iterations      | 64       |
|    time_elapsed    | 959      |
|    total_timesteps | 131072   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.2      |
|    ep_rew_mean          | 7.33      |
| time/                   |           |
|    fps                  | 136       |
|    iterations           | 65        |
|    time_elapsed         | 977       |
|    total_timesteps      | 133120    |
| train/                  |           |
|    approx_kl            | 0.0413535 |
|    clip_fraction        | 0.0985    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.143    |
|    explained_variance   | 0.613     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.403     |
|    n_updates            | 10400     |
|    policy_gradient_loss | -0.011    |
|    value_loss           | 0.884     |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.52       |
| time/                   |            |
|    fps                  | 135        |
|    iterations           | 66         |
|    time_elapsed         | 993        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.05846471 |
|    clip_fraction        | 0.0922     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.193      |
|    n_updates            | 10410      |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.565      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 135        |
|    iterations           | 67         |
|    time_elapsed         | 1011       |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.06633092 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.146     |
|    explained_variance   | 0.477      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.15       |
|    n_updates            | 10420      |
|    policy_gradient_loss | -0.0262    |
|    value_loss           | 0.491      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 135         |
|    iterations           | 68          |
|    time_elapsed         | 1031        |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.046161465 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0985     |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 10430       |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 0.458       |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=7.45 +/- 0.75
Episode length: 19.33 +/- 2.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 7.45        |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.059577998 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0932     |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.353       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.9      |
| time/              |          |
|    fps             | 134      |
|    iterations      | 69       |
|    time_elapsed    | 1048     |
|    total_timesteps | 141312   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.56        |
| time/                   |             |
|    fps                  | 134         |
|    iterations           | 70          |
|    time_elapsed         | 1066        |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.095821515 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.181       |
|    n_updates            | 10450       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.456       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.4         |
| time/                   |             |
|    fps                  | 134         |
|    iterations           | 71          |
|    time_elapsed         | 1083        |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.031600352 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0824      |
|    n_updates            | 10460       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.425       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.16       |
| time/                   |            |
|    fps                  | 133        |
|    iterations           | 72         |
|    time_elapsed         | 1100       |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.13184372 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.17      |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.362      |
|    n_updates            | 10470      |
|    policy_gradient_loss | -0.0267    |
|    value_loss           | 0.738      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 133        |
|    iterations           | 73         |
|    time_elapsed         | 1117       |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.06266138 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.196      |
|    n_updates            | 10480      |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.58       |
----------------------------------------
reached max steps=100
Eval num_timesteps=150000, episode_reward=2.31 +/- 8.78
Episode length: 29.00 +/- 14.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29         |
|    mean_reward          | 2.31       |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.04281863 |
|    clip_fraction        | 0.0925     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.214      |
|    n_updates            | 10490      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.38       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 7.28     |
| time/              |          |
|    fps             | 133      |
|    iterations      | 74       |
|    time_elapsed    | 1137     |
|    total_timesteps | 151552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 132        |
|    iterations           | 75         |
|    time_elapsed         | 1156       |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.04677856 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.164     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.249      |
|    n_updates            | 10500      |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.691      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.39       |
| time/                   |            |
|    fps                  | 132        |
|    iterations           | 76         |
|    time_elapsed         | 1173       |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.05254069 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.175     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.196      |
|    n_updates            | 10510      |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 0.587      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 132        |
|    iterations           | 77         |
|    time_elapsed         | 1192       |
|    total_timesteps      | 157696     |
| train/                  |            |
|    approx_kl            | 0.03850147 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.17       |
|    n_updates            | 10520      |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.565      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 131         |
|    iterations           | 78          |
|    time_elapsed         | 1210        |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.030153684 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.17        |
|    n_updates            | 10530       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.463       |
-----------------------------------------
Eval num_timesteps=160000, episode_reward=6.89 +/- 0.29
Episode length: 17.33 +/- 3.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 6.89        |
| time/                   |             |
|    total_timesteps      | 160000      |
| train/                  |             |
|    approx_kl            | 0.046668388 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.189       |
|    n_updates            | 10540       |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.416       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 131      |
|    iterations      | 79       |
|    time_elapsed    | 1229     |
|    total_timesteps | 161792   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.44        |
| time/                   |             |
|    fps                  | 131         |
|    iterations           | 80          |
|    time_elapsed         | 1248        |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.027408397 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 10550       |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.454       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.29        |
| time/                   |             |
|    fps                  | 130         |
|    iterations           | 81          |
|    time_elapsed         | 1268        |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.036102068 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 10560       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.636       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.24        |
| time/                   |             |
|    fps                  | 130         |
|    iterations           | 82          |
|    time_elapsed         | 1285        |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.073495775 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.177       |
|    n_updates            | 10570       |
|    policy_gradient_loss | 0.014       |
|    value_loss           | 0.607       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 6.95        |
| time/                   |             |
|    fps                  | 130         |
|    iterations           | 83          |
|    time_elapsed         | 1302        |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.059617985 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.464       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.154       |
|    n_updates            | 10580       |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.571       |
-----------------------------------------
Eval num_timesteps=170000, episode_reward=7.18 +/- 0.66
Episode length: 16.00 +/- 1.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 7.18       |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.05222765 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.339      |
|    n_updates            | 10590      |
|    policy_gradient_loss | -0.0199    |
|    value_loss           | 1.02       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.69     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 84       |
|    time_elapsed    | 1319     |
|    total_timesteps | 172032   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 130        |
|    iterations           | 85         |
|    time_elapsed         | 1336       |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.04368732 |
|    clip_fraction        | 0.0866     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.617      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.251      |
|    n_updates            | 10600      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.413      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.3         |
| time/                   |             |
|    fps                  | 130         |
|    iterations           | 86          |
|    time_elapsed         | 1353        |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.058842327 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 10610       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.499       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.39        |
| time/                   |             |
|    fps                  | 129         |
|    iterations           | 87          |
|    time_elapsed         | 1372        |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.057986896 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 10620       |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 0.526       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=180000, episode_reward=2.64 +/- 8.95
Episode length: 29.00 +/- 14.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 2.64        |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.045643665 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.631       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 10630       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.716       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 88       |
|    time_elapsed    | 1391     |
|    total_timesteps | 180224   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.4         |
| time/                   |             |
|    fps                  | 129         |
|    iterations           | 89          |
|    time_elapsed         | 1408        |
|    total_timesteps      | 182272      |
| train/                  |             |
|    approx_kl            | 0.056491204 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 10640       |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.51        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.5         |
| time/                   |             |
|    fps                  | 129         |
|    iterations           | 90          |
|    time_elapsed         | 1425        |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.034695882 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 10650       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.739       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.3         |
| time/                   |             |
|    fps                  | 129         |
|    iterations           | 91          |
|    time_elapsed         | 1442        |
|    total_timesteps      | 186368      |
| train/                  |             |
|    approx_kl            | 0.038856104 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.327       |
|    n_updates            | 10660       |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.713       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.66       |
| time/                   |            |
|    fps                  | 129        |
|    iterations           | 92         |
|    time_elapsed         | 1460       |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.04162318 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.194     |
|    explained_variance   | 0.494      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.185      |
|    n_updates            | 10670      |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.785      |
----------------------------------------
Eval num_timesteps=190000, episode_reward=7.57 +/- 1.28
Episode length: 20.33 +/- 1.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20.3       |
|    mean_reward          | 7.57       |
| time/                   |            |
|    total_timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.05982274 |
|    clip_fraction        | 0.0986     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.146     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.127      |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.449      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.71     |
| time/              |          |
|    fps             | 129      |
|    iterations      | 93       |
|    time_elapsed    | 1473     |
|    total_timesteps | 190464   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 129        |
|    iterations           | 94         |
|    time_elapsed         | 1485       |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.11043093 |
|    clip_fraction        | 0.0946     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0746     |
|    n_updates            | 10690      |
|    policy_gradient_loss | -0.02      |
|    value_loss           | 0.383      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 129        |
|    iterations           | 95         |
|    time_elapsed         | 1499       |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.04465882 |
|    clip_fraction        | 0.0917     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.727      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 10700      |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 0.369      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.36        |
| time/                   |             |
|    fps                  | 129         |
|    iterations           | 96          |
|    time_elapsed         | 1513        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.062222824 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.171       |
|    n_updates            | 10710       |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.372       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 7.18        |
| time/                   |             |
|    fps                  | 130         |
|    iterations           | 97          |
|    time_elapsed         | 1525        |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.040042818 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.609       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.23        |
|    n_updates            | 10720       |
|    policy_gradient_loss | -0.000569   |
|    value_loss           | 0.68        |
-----------------------------------------
Eval num_timesteps=200000, episode_reward=8.14 +/- 0.84
Episode length: 14.67 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | 8.14        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.040598378 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.25       |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.25        |
|    n_updates            | 10730       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.772       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.23     |
| time/              |          |
|    fps             | 130      |
|    iterations      | 98       |
|    time_elapsed    | 1539     |
|    total_timesteps | 200704   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 130         |
|    iterations           | 99          |
|    time_elapsed         | 1552        |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.037282392 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.47        |
|    n_updates            | 10740       |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.778       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.46        |
| time/                   |             |
|    fps                  | 130         |
|    iterations           | 100         |
|    time_elapsed         | 1563        |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.058896907 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.201      |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.21        |
|    n_updates            | 10750       |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.55        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.19        |
| time/                   |             |
|    fps                  | 131         |
|    iterations           | 101         |
|    time_elapsed         | 1575        |
|    total_timesteps      | 206848      |
| train/                  |             |
|    approx_kl            | 0.043447386 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.103       |
|    n_updates            | 10760       |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.513       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.4        |
|    ep_rew_mean          | 6.94        |
| time/                   |             |
|    fps                  | 131         |
|    iterations           | 102         |
|    time_elapsed         | 1586        |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.065407515 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 10770       |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.753       |
-----------------------------------------
Eval num_timesteps=210000, episode_reward=8.74 +/- 0.79
Episode length: 18.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 8.74        |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.051783793 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.358       |
|    n_updates            | 10780       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.638       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 132      |
|    iterations      | 103      |
|    time_elapsed    | 1597     |
|    total_timesteps | 210944   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.52       |
| time/                   |            |
|    fps                  | 132        |
|    iterations           | 104        |
|    time_elapsed         | 1609       |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.03688589 |
|    clip_fraction        | 0.0966     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0565     |
|    n_updates            | 10790      |
|    policy_gradient_loss | -0.0164    |
|    value_loss           | 0.391      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 132        |
|    iterations           | 105        |
|    time_elapsed         | 1623       |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.05420555 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.206      |
|    n_updates            | 10800      |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 0.669      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 132        |
|    iterations           | 106        |
|    time_elapsed         | 1634       |
|    total_timesteps      | 217088     |
| train/                  |            |
|    approx_kl            | 0.02970111 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.193      |
|    n_updates            | 10810      |
|    policy_gradient_loss | -0.0199    |
|    value_loss           | 0.678      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.3       |
|    ep_rew_mean          | 7.24       |
| time/                   |            |
|    fps                  | 133        |
|    iterations           | 107        |
|    time_elapsed         | 1645       |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.13533433 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.378      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.165      |
|    n_updates            | 10820      |
|    policy_gradient_loss | -0.0342    |
|    value_loss           | 0.714      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=220000, episode_reward=-1.97 +/- 7.52
Episode length: 39.33 +/- 15.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 39.3      |
|    mean_reward          | -1.97     |
| time/                   |           |
|    total_timesteps      | 220000    |
| train/                  |           |
|    approx_kl            | 0.0605135 |
|    clip_fraction        | 0.146     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.2      |
|    explained_variance   | 0.594     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.199     |
|    n_updates            | 10830     |
|    policy_gradient_loss | -0.0149   |
|    value_loss           | 0.634     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    fps             | 133      |
|    iterations      | 108      |
|    time_elapsed    | 1661     |
|    total_timesteps | 221184   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.25        |
| time/                   |             |
|    fps                  | 133         |
|    iterations           | 109         |
|    time_elapsed         | 1672        |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.049408704 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.189       |
|    n_updates            | 10840       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.51        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 133         |
|    iterations           | 110         |
|    time_elapsed         | 1684        |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.046722796 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.125       |
|    n_updates            | 10850       |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.518       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 8.05       |
| time/                   |            |
|    fps                  | 134        |
|    iterations           | 111        |
|    time_elapsed         | 1694       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.04611789 |
|    clip_fraction        | 0.0953     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0873     |
|    n_updates            | 10860      |
|    policy_gradient_loss | -0.00974   |
|    value_loss           | 0.417      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 134         |
|    iterations           | 112         |
|    time_elapsed         | 1706        |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.025659695 |
|    clip_fraction        | 0.0967      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0864      |
|    n_updates            | 10870       |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.302       |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=8.09 +/- 1.89
Episode length: 21.00 +/- 5.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 8.09       |
| time/                   |            |
|    total_timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.05198226 |
|    clip_fraction        | 0.0944     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.148     |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.119      |
|    n_updates            | 10880      |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 0.457      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 7.82     |
| time/              |          |
|    fps             | 134      |
|    iterations      | 113      |
|    time_elapsed    | 1718     |
|    total_timesteps | 231424   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 134        |
|    iterations           | 114        |
|    time_elapsed         | 1730       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.07156679 |
|    clip_fraction        | 0.0829     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.109     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.101      |
|    n_updates            | 10890      |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.375      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 134        |
|    iterations           | 115        |
|    time_elapsed         | 1745       |
|    total_timesteps      | 235520     |
| train/                  |            |
|    approx_kl            | 0.04184504 |
|    clip_fraction        | 0.0955     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.355      |
|    n_updates            | 10900      |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.483      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 135        |
|    iterations           | 116        |
|    time_elapsed         | 1756       |
|    total_timesteps      | 237568     |
| train/                  |            |
|    approx_kl            | 0.05917035 |
|    clip_fraction        | 0.0998     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.131     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.109      |
|    n_updates            | 10910      |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.448      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.87       |
| time/                   |            |
|    fps                  | 135        |
|    iterations           | 117        |
|    time_elapsed         | 1768       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.08508909 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.348      |
|    n_updates            | 10920      |
|    policy_gradient_loss | -0.0216    |
|    value_loss           | 0.709      |
----------------------------------------
reached max steps=100
Eval num_timesteps=240000, episode_reward=8.08 +/- 1.31
Episode length: 18.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 8.08        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.039444953 |
|    clip_fraction        | 0.0876      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 10930       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.471       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 135      |
|    iterations      | 118      |
|    time_elapsed    | 1781     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.52        |
| time/                   |             |
|    fps                  | 135         |
|    iterations           | 119         |
|    time_elapsed         | 1793        |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.031013947 |
|    clip_fraction        | 0.0885      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 10940       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.572       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.47       |
| time/                   |            |
|    fps                  | 136        |
|    iterations           | 120        |
|    time_elapsed         | 1804       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.05852975 |
|    clip_fraction        | 0.0939     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.648      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.135      |
|    n_updates            | 10950      |
|    policy_gradient_loss | -0.00902   |
|    value_loss           | 0.455      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 136        |
|    iterations           | 121        |
|    time_elapsed         | 1815       |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.13410743 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.56       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0734     |
|    n_updates            | 10960      |
|    policy_gradient_loss | -0.0376    |
|    value_loss           | 0.487      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 22       |
|    ep_rew_mean          | 7.21     |
| time/                   |          |
|    fps                  | 136      |
|    iterations           | 122      |
|    time_elapsed         | 1828     |
|    total_timesteps      | 249856   |
| train/                  |          |
|    approx_kl            | 0.068001 |
|    clip_fraction        | 0.123    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.136   |
|    explained_variance   | 0.446    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.132    |
|    n_updates            | 10970    |
|    policy_gradient_loss | -0.0235  |
|    value_loss           | 0.882    |
--------------------------------------
Eval num_timesteps=250000, episode_reward=8.82 +/- 0.74
Episode length: 17.67 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 8.82        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.037768744 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0611      |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.023      |
|    value_loss           | 0.614       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 7.26     |
| time/              |          |
|    fps             | 136      |
|    iterations      | 123      |
|    time_elapsed    | 1842     |
|    total_timesteps | 251904   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.52        |
| time/                   |             |
|    fps                  | 136         |
|    iterations           | 124         |
|    time_elapsed         | 1856        |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.044968724 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.166      |
|    explained_variance   | 0.452       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.756       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.53       |
| time/                   |            |
|    fps                  | 136        |
|    iterations           | 125        |
|    time_elapsed         | 1872       |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.04328224 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.198     |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.134      |
|    n_updates            | 11000      |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.641      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 137        |
|    iterations           | 126        |
|    time_elapsed         | 1883       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.03768137 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.196     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.254      |
|    n_updates            | 11010      |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 0.743      |
----------------------------------------
reached max steps=100
Eval num_timesteps=260000, episode_reward=1.40 +/- 8.09
Episode length: 31.67 +/- 13.72
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31.7        |
|    mean_reward          | 1.4         |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.038207933 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.118       |
|    n_updates            | 11020       |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.609       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 137      |
|    iterations      | 127      |
|    time_elapsed    | 1897     |
|    total_timesteps | 260096   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.34        |
| time/                   |             |
|    fps                  | 137         |
|    iterations           | 128         |
|    time_elapsed         | 1908        |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.043582886 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 11030       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.407       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 137        |
|    iterations           | 129        |
|    time_elapsed         | 1919       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.06916027 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.173      |
|    n_updates            | 11040      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.519      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 137         |
|    iterations           | 130         |
|    time_elapsed         | 1931        |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.044657063 |
|    clip_fraction        | 0.0966      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.059       |
|    n_updates            | 11050       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.295       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.81        |
| time/                   |             |
|    fps                  | 137         |
|    iterations           | 131         |
|    time_elapsed         | 1944        |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.028687479 |
|    clip_fraction        | 0.0843      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 11060       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.35        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=270000, episode_reward=2.43 +/- 8.84
Episode length: 30.00 +/- 14.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 2.43        |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.034541797 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 11070       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.59        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.53     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 132      |
|    time_elapsed    | 1956     |
|    total_timesteps | 270336   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 133         |
|    time_elapsed         | 1968        |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.048165493 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.203       |
|    n_updates            | 11080       |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.389       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 134        |
|    time_elapsed         | 1981       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.06431156 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.148     |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0436     |
|    n_updates            | 11090      |
|    policy_gradient_loss | -0.0212    |
|    value_loss           | 0.305      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 135         |
|    time_elapsed         | 1994        |
|    total_timesteps      | 276480      |
| train/                  |             |
|    approx_kl            | 0.041669056 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 11100       |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.404       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 136        |
|    time_elapsed         | 2007       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.05405296 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.539      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.179      |
|    n_updates            | 11110      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.482      |
----------------------------------------
reached max steps=100
Eval num_timesteps=280000, episode_reward=5.24 +/- 5.16
Episode length: 29.33 +/- 14.64
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 5.24        |
| time/                   |             |
|    total_timesteps      | 280000      |
| train/                  |             |
|    approx_kl            | 0.039184067 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0935      |
|    n_updates            | 11120       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.37        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 137      |
|    time_elapsed    | 2020     |
|    total_timesteps | 280576   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 138         |
|    time_elapsed         | 2032        |
|    total_timesteps      | 282624      |
| train/                  |             |
|    approx_kl            | 0.044204533 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 11130       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.306       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 139         |
|    time_elapsed         | 2043        |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.044378266 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 11140       |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.402       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.46        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 140         |
|    time_elapsed         | 2055        |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.088493936 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0952      |
|    n_updates            | 11150       |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.5         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 141         |
|    time_elapsed         | 2067        |
|    total_timesteps      | 288768      |
| train/                  |             |
|    approx_kl            | 0.059786804 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0965      |
|    n_updates            | 11160       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.451       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=290000, episode_reward=6.76 +/- 0.04
Episode length: 21.00 +/- 8.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 6.76        |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.047700755 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.179       |
|    n_updates            | 11170       |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.665       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 142      |
|    time_elapsed    | 2079     |
|    total_timesteps | 290816   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 143        |
|    time_elapsed         | 2090       |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.03952634 |
|    clip_fraction        | 0.0878     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0945     |
|    n_updates            | 11180      |
|    policy_gradient_loss | -0.0122    |
|    value_loss           | 0.535      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.59        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 144         |
|    time_elapsed         | 2102        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.088215634 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0879      |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.55        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 145        |
|    time_elapsed         | 2114       |
|    total_timesteps      | 296960     |
| train/                  |            |
|    approx_kl            | 0.03709118 |
|    clip_fraction        | 0.0928     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0902     |
|    n_updates            | 11200      |
|    policy_gradient_loss | -0.0126    |
|    value_loss           | 0.385      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.78        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 146         |
|    time_elapsed         | 2130        |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.062812194 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.189       |
|    n_updates            | 11210       |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.506       |
-----------------------------------------
Eval num_timesteps=300000, episode_reward=8.22 +/- 0.37
Episode length: 17.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.22        |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.047841247 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0947      |
|    n_updates            | 11220       |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.339       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 147      |
|    time_elapsed    | 2145     |
|    total_timesteps | 301056   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 148         |
|    time_elapsed         | 2158        |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.036949553 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.083       |
|    n_updates            | 11230       |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.312       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.25        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 149         |
|    time_elapsed         | 2173        |
|    total_timesteps      | 305152      |
| train/                  |             |
|    approx_kl            | 0.036979653 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0128      |
|    n_updates            | 11240       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.333       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 150         |
|    time_elapsed         | 2188        |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.047676772 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.174      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.264       |
|    n_updates            | 11250       |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.724       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 151        |
|    time_elapsed         | 2205       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.03662864 |
|    clip_fraction        | 0.098      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.692      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0745     |
|    n_updates            | 11260      |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.374      |
----------------------------------------
reached max steps=100
Eval num_timesteps=310000, episode_reward=8.61 +/- 0.73
Episode length: 21.67 +/- 3.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.7        |
|    mean_reward          | 8.61        |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.037544243 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 11270       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.321       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 152      |
|    time_elapsed    | 2221     |
|    total_timesteps | 311296   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 153        |
|    time_elapsed         | 2238       |
|    total_timesteps      | 313344     |
| train/                  |            |
|    approx_kl            | 0.04152252 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.026      |
|    n_updates            | 11280      |
|    policy_gradient_loss | -0.0283    |
|    value_loss           | 0.3        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.23        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 154         |
|    time_elapsed         | 2254        |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.063124485 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0106      |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.49        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.36        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 155         |
|    time_elapsed         | 2270        |
|    total_timesteps      | 317440      |
| train/                  |             |
|    approx_kl            | 0.047401935 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.301       |
|    n_updates            | 11300       |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.797       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.94       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 156        |
|    time_elapsed         | 2285       |
|    total_timesteps      | 319488     |
| train/                  |            |
|    approx_kl            | 0.08334525 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.173     |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0963     |
|    n_updates            | 11310      |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.584      |
----------------------------------------
reached max steps=100
Eval num_timesteps=320000, episode_reward=8.45 +/- 0.39
Episode length: 19.33 +/- 2.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 8.45        |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.049835354 |
|    clip_fraction        | 0.0994      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 11320       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.362       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.45     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 157      |
|    time_elapsed    | 2301     |
|    total_timesteps | 321536   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 158         |
|    time_elapsed         | 2316        |
|    total_timesteps      | 323584      |
| train/                  |             |
|    approx_kl            | 0.047877245 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.388       |
|    n_updates            | 11330       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.639       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 159         |
|    time_elapsed         | 2332        |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.046384297 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 11340       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.395       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.18        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 160         |
|    time_elapsed         | 2347        |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.054794457 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0696      |
|    n_updates            | 11350       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.487       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 161        |
|    time_elapsed         | 2363       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.06882912 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.203     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.136      |
|    n_updates            | 11360      |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 0.874      |
----------------------------------------
Eval num_timesteps=330000, episode_reward=8.08 +/- 0.31
Episode length: 18.00 +/- 1.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18         |
|    mean_reward          | 8.08       |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.19134635 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.173     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.172      |
|    n_updates            | 11370      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.908      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.93     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 162      |
|    time_elapsed    | 2379     |
|    total_timesteps | 331776   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 163        |
|    time_elapsed         | 2394       |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.06972547 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0748     |
|    n_updates            | 11380      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.447      |
----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.9      |
|    ep_rew_mean          | 7.54      |
| time/                   |           |
|    fps                  | 139       |
|    iterations           | 164       |
|    time_elapsed         | 2410      |
|    total_timesteps      | 335872    |
| train/                  |           |
|    approx_kl            | 0.0644082 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.224    |
|    explained_variance   | 0.606     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.513     |
|    n_updates            | 11390     |
|    policy_gradient_loss | -0.018    |
|    value_loss           | 0.938     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 165         |
|    time_elapsed         | 2425        |
|    total_timesteps      | 337920      |
| train/                  |             |
|    approx_kl            | 0.046228644 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.606       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.124       |
|    n_updates            | 11400       |
|    policy_gradient_loss | -0.0259     |
|    value_loss           | 0.729       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 166         |
|    time_elapsed         | 2440        |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.032689855 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    n_updates            | 11410       |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.476       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=340000, episode_reward=4.05 +/- 7.18
Episode length: 28.67 +/- 15.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.7       |
|    mean_reward          | 4.05       |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.05041482 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.142     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.142      |
|    n_updates            | 11420      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.426      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 139      |
|    iterations      | 167      |
|    time_elapsed    | 2457     |
|    total_timesteps | 342016   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 168        |
|    time_elapsed         | 2474       |
|    total_timesteps      | 344064     |
| train/                  |            |
|    approx_kl            | 0.09501079 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.21       |
|    n_updates            | 11430      |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 0.487      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.97       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 169        |
|    time_elapsed         | 2491       |
|    total_timesteps      | 346112     |
| train/                  |            |
|    approx_kl            | 0.06581969 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 11440      |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.448      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.35       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 170        |
|    time_elapsed         | 2507       |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.04891861 |
|    clip_fraction        | 0.0857     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0803     |
|    n_updates            | 11450      |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.312      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=350000, episode_reward=7.73 +/- 0.53
Episode length: 22.67 +/- 6.65
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22.7       |
|    mean_reward          | 7.73       |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.04950951 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.323      |
|    n_updates            | 11460      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.551      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.76     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 171      |
|    time_elapsed    | 2522     |
|    total_timesteps | 350208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 172         |
|    time_elapsed         | 2537        |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.045136698 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.124       |
|    n_updates            | 11470       |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.465       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 173         |
|    time_elapsed         | 2553        |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.058307864 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0973     |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0665      |
|    n_updates            | 11480       |
|    policy_gradient_loss | 0.00179     |
|    value_loss           | 0.339       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | 7.35       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 174        |
|    time_elapsed         | 2568       |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.03701654 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0957     |
|    n_updates            | 11490      |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 0.439      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.52       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 175        |
|    time_elapsed         | 2584       |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.04452484 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.166     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 11500      |
|    policy_gradient_loss | -0.0209    |
|    value_loss           | 0.61       |
----------------------------------------
Eval num_timesteps=360000, episode_reward=7.45 +/- 2.04
Episode length: 19.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 7.45        |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.036018156 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0749      |
|    n_updates            | 11510       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.494       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 7.9      |
| time/              |          |
|    fps             | 138      |
|    iterations      | 176      |
|    time_elapsed    | 2600     |
|    total_timesteps | 360448   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 177         |
|    time_elapsed         | 2615        |
|    total_timesteps      | 362496      |
| train/                  |             |
|    approx_kl            | 0.036820043 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.328       |
|    n_updates            | 11520       |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.524       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.69       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 178        |
|    time_elapsed         | 2631       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.07862565 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.144     |
|    explained_variance   | 0.404      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.142      |
|    n_updates            | 11530      |
|    policy_gradient_loss | -0.026     |
|    value_loss           | 0.55       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.55        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 179         |
|    time_elapsed         | 2646        |
|    total_timesteps      | 366592      |
| train/                  |             |
|    approx_kl            | 0.039862994 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0668      |
|    n_updates            | 11540       |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.504       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.4      |
|    ep_rew_mean          | 7.55      |
| time/                   |           |
|    fps                  | 138       |
|    iterations           | 180       |
|    time_elapsed         | 2662      |
|    total_timesteps      | 368640    |
| train/                  |           |
|    approx_kl            | 0.0375828 |
|    clip_fraction        | 0.118     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.165    |
|    explained_variance   | 0.691     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.186     |
|    n_updates            | 11550     |
|    policy_gradient_loss | -0.019    |
|    value_loss           | 0.408     |
---------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=370000, episode_reward=2.94 +/- 6.34
Episode length: 27.67 +/- 15.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27.7       |
|    mean_reward          | 2.94       |
| time/                   |            |
|    total_timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.03455919 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.186     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.13       |
|    n_updates            | 11560      |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.507      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 181      |
|    time_elapsed    | 2677     |
|    total_timesteps | 370688   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 182         |
|    time_elapsed         | 2690        |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.033147648 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.259       |
|    n_updates            | 11570       |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.534       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 7.19        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 183         |
|    time_elapsed         | 2704        |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.029092487 |
|    clip_fraction        | 0.0955      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.174       |
|    n_updates            | 11580       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.589       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 7.36        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 184         |
|    time_elapsed         | 2717        |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.033325367 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.161       |
|    n_updates            | 11590       |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.822       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 185         |
|    time_elapsed         | 2730        |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.039133154 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.318       |
|    n_updates            | 11600       |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.801       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=380000, episode_reward=6.89 +/- 0.39
Episode length: 17.33 +/- 2.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 6.89       |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.03934764 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.2       |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.168      |
|    n_updates            | 11610      |
|    policy_gradient_loss | -0.02      |
|    value_loss           | 0.561      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 7.38     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 186      |
|    time_elapsed    | 2743     |
|    total_timesteps | 380928   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 187        |
|    time_elapsed         | 2755       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.05801142 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.173      |
|    n_updates            | 11620      |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.524      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 188        |
|    time_elapsed         | 2767       |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.05077926 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.173     |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.154      |
|    n_updates            | 11630      |
|    policy_gradient_loss | -0.0213    |
|    value_loss           | 0.53       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8           |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 189         |
|    time_elapsed         | 2779        |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.029055048 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.18        |
|    n_updates            | 11640       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.442       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.73        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 190         |
|    time_elapsed         | 2792        |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.053334877 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 11650       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.359       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=390000, episode_reward=-3.90 +/- 8.62
Episode length: 39.00 +/- 15.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39         |
|    mean_reward          | -3.9       |
| time/                   |            |
|    total_timesteps      | 390000     |
| train/                  |            |
|    approx_kl            | 0.04226962 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.148     |
|    explained_variance   | 0.706      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.196      |
|    n_updates            | 11660      |
|    policy_gradient_loss | -0.0227    |
|    value_loss           | 0.519      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 7.42     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 191      |
|    time_elapsed    | 2804     |
|    total_timesteps | 391168   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 7.55        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 192         |
|    time_elapsed         | 2816        |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.050090156 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.234       |
|    n_updates            | 11670       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.623       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 8           |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 193         |
|    time_elapsed         | 2829        |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.047204696 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 11680       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.805       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.49        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 194         |
|    time_elapsed         | 2841        |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.031121215 |
|    clip_fraction        | 0.099       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 11690       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.348       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.97        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 195         |
|    time_elapsed         | 2854        |
|    total_timesteps      | 399360      |
| train/                  |             |
|    approx_kl            | 0.051822584 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 11700       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.465       |
-----------------------------------------
Eval num_timesteps=400000, episode_reward=7.34 +/- 0.76
Episode length: 18.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 7.34       |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.03851038 |
|    clip_fraction        | 0.0852     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0958    |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 11710      |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 0.295      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.89     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 196      |
|    time_elapsed    | 2867     |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.81        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 197         |
|    time_elapsed         | 2884        |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.046126418 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0644      |
|    n_updates            | 11720       |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.301       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 198         |
|    time_elapsed         | 2904        |
|    total_timesteps      | 405504      |
| train/                  |             |
|    approx_kl            | 0.038262084 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0846     |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0899      |
|    n_updates            | 11730       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.251       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 199         |
|    time_elapsed         | 2924        |
|    total_timesteps      | 407552      |
| train/                  |             |
|    approx_kl            | 0.032881036 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 11740       |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.377       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 200         |
|    time_elapsed         | 2940        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.042139202 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.141       |
|    n_updates            | 11750       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.369       |
-----------------------------------------
Eval num_timesteps=410000, episode_reward=8.74 +/- 0.49
Episode length: 18.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 8.74        |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.042707272 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.22        |
|    n_updates            | 11760       |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.595       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 7.62     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 201      |
|    time_elapsed    | 2953     |
|    total_timesteps | 411648   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.7      |
|    ep_rew_mean          | 7.58      |
| time/                   |           |
|    fps                  | 139       |
|    iterations           | 202       |
|    time_elapsed         | 2967      |
|    total_timesteps      | 413696    |
| train/                  |           |
|    approx_kl            | 0.0370442 |
|    clip_fraction        | 0.119     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.195    |
|    explained_variance   | 0.608     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.195     |
|    n_updates            | 11770     |
|    policy_gradient_loss | -0.00806  |
|    value_loss           | 0.649     |
---------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.65       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 203        |
|    time_elapsed         | 2980       |
|    total_timesteps      | 415744     |
| train/                  |            |
|    approx_kl            | 0.03684974 |
|    clip_fraction        | 0.099      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.125      |
|    n_updates            | 11780      |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.434      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.57        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 204         |
|    time_elapsed         | 2994        |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.038891174 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.376       |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.623       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 205        |
|    time_elapsed         | 3009       |
|    total_timesteps      | 419840     |
| train/                  |            |
|    approx_kl            | 0.10729234 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.202     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.26       |
|    n_updates            | 11800      |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.98       |
----------------------------------------
reached max steps=100
Eval num_timesteps=420000, episode_reward=5.98 +/- 5.65
Episode length: 29.00 +/- 14.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29         |
|    mean_reward          | 5.98       |
| time/                   |            |
|    total_timesteps      | 420000     |
| train/                  |            |
|    approx_kl            | 0.06427869 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.184     |
|    explained_variance   | 0.525      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.192      |
|    n_updates            | 11810      |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.752      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 7.32     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 206      |
|    time_elapsed    | 3023     |
|    total_timesteps | 421888   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.72       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 207        |
|    time_elapsed         | 3038       |
|    total_timesteps      | 423936     |
| train/                  |            |
|    approx_kl            | 0.06348923 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.207     |
|    explained_variance   | 0.569      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.221      |
|    n_updates            | 11820      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.771      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.26        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 208         |
|    time_elapsed         | 3051        |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.050198708 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 11830       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.551       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 209        |
|    time_elapsed         | 3065       |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.04749123 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.21      |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0529     |
|    n_updates            | 11840      |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.615      |
----------------------------------------
Eval num_timesteps=430000, episode_reward=8.22 +/- 1.19
Episode length: 17.33 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.22        |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.047414348 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0691      |
|    n_updates            | 11850       |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.435       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 7.72     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 210      |
|    time_elapsed    | 3081     |
|    total_timesteps | 430080   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.13        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 211         |
|    time_elapsed         | 3097        |
|    total_timesteps      | 432128      |
| train/                  |             |
|    approx_kl            | 0.055860937 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.192       |
|    n_updates            | 11860       |
|    policy_gradient_loss | -0.00391    |
|    value_loss           | 0.395       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 212        |
|    time_elapsed         | 3112       |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.44646785 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.172     |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 11870      |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.612      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 8.03       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 213        |
|    time_elapsed         | 3127       |
|    total_timesteps      | 436224     |
| train/                  |            |
|    approx_kl            | 0.06969287 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.148     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.153      |
|    n_updates            | 11880      |
|    policy_gradient_loss | -0.0232    |
|    value_loss           | 0.454      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.2      |
|    ep_rew_mean          | 8.05      |
| time/                   |           |
|    fps                  | 139       |
|    iterations           | 214       |
|    time_elapsed         | 3141      |
|    total_timesteps      | 438272    |
| train/                  |           |
|    approx_kl            | 0.0370135 |
|    clip_fraction        | 0.0785    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.101    |
|    explained_variance   | 0.763     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0834    |
|    n_updates            | 11890     |
|    policy_gradient_loss | -0.0141   |
|    value_loss           | 0.358     |
---------------------------------------
reached max steps=100
Eval num_timesteps=440000, episode_reward=7.48 +/- 1.80
Episode length: 17.67 +/- 3.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 7.48        |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.031336486 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0938     |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.302       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 7.57     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 215      |
|    time_elapsed    | 3155     |
|    total_timesteps | 440320   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 216         |
|    time_elapsed         | 3169        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.038452797 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.115       |
|    n_updates            | 11910       |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.359       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 217         |
|    time_elapsed         | 3184        |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.032896094 |
|    clip_fraction        | 0.0884      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.244       |
|    n_updates            | 11920       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.355       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.12       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 218        |
|    time_elapsed         | 3200       |
|    total_timesteps      | 446464     |
| train/                  |            |
|    approx_kl            | 0.04670272 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.139      |
|    n_updates            | 11930      |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.499      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.56        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 219         |
|    time_elapsed         | 3216        |
|    total_timesteps      | 448512      |
| train/                  |             |
|    approx_kl            | 0.089630276 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.153       |
|    n_updates            | 11940       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.431       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=450000, episode_reward=8.61 +/- 0.58
Episode length: 21.67 +/- 3.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.7       |
|    mean_reward          | 8.61       |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.83027744 |
|    clip_fraction        | 0.202      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.171      |
|    n_updates            | 11950      |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 0.542      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.11     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 220      |
|    time_elapsed    | 3232     |
|    total_timesteps | 450560   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.27        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 221         |
|    time_elapsed         | 3247        |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.031366337 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 11960       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.355       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 222        |
|    time_elapsed         | 3261       |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.06695184 |
|    clip_fraction        | 0.0808     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0747     |
|    n_updates            | 11970      |
|    policy_gradient_loss | -0.0167    |
|    value_loss           | 0.29       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 223         |
|    time_elapsed         | 3274        |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.043068007 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.096       |
|    n_updates            | 11980       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.279       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 224         |
|    time_elapsed         | 3287        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.023518477 |
|    clip_fraction        | 0.0622      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0839      |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.271       |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=7.96 +/- 1.50
Episode length: 17.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 7.96        |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.050291386 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0711      |
|    n_updates            | 12000       |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.35        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.84     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 225      |
|    time_elapsed    | 3300     |
|    total_timesteps | 460800   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.03       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 226        |
|    time_elapsed         | 3314       |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.03425257 |
|    clip_fraction        | 0.0829     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.132      |
|    n_updates            | 12010      |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.388      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 227        |
|    time_elapsed         | 3328       |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.03452363 |
|    clip_fraction        | 0.0814     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0891     |
|    n_updates            | 12020      |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.355      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.77       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 228        |
|    time_elapsed         | 3342       |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.04455394 |
|    clip_fraction        | 0.0754     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.777      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.118      |
|    n_updates            | 12030      |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.318      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 8.06        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 229         |
|    time_elapsed         | 3355        |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.034835994 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0606      |
|    n_updates            | 12040       |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.469       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=470000, episode_reward=3.91 +/- 7.03
Episode length: 29.33 +/- 14.64
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.3       |
|    mean_reward          | 3.91       |
| time/                   |            |
|    total_timesteps      | 470000     |
| train/                  |            |
|    approx_kl            | 0.10808946 |
|    clip_fraction        | 0.091      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 12050      |
|    policy_gradient_loss | -0.00141   |
|    value_loss           | 0.609      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.72     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 230      |
|    time_elapsed    | 3370     |
|    total_timesteps | 471040   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 231        |
|    time_elapsed         | 3383       |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.05206945 |
|    clip_fraction        | 0.0873     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.055      |
|    n_updates            | 12060      |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.43       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 232         |
|    time_elapsed         | 3397        |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.085677356 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0474      |
|    n_updates            | 12070       |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.425       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 233        |
|    time_elapsed         | 3411       |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.03555653 |
|    clip_fraction        | 0.0861     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.129     |
|    explained_variance   | 0.706      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.197      |
|    n_updates            | 12080      |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.439      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.71       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 234        |
|    time_elapsed         | 3425       |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.03902915 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0594     |
|    n_updates            | 12090      |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.558      |
----------------------------------------
Eval num_timesteps=480000, episode_reward=7.64 +/- 0.64
Episode length: 20.00 +/- 2.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 20        |
|    mean_reward          | 7.64      |
| time/                   |           |
|    total_timesteps      | 480000    |
| train/                  |           |
|    approx_kl            | 0.0639812 |
|    clip_fraction        | 0.107     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.15     |
|    explained_variance   | 0.627     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.117     |
|    n_updates            | 12100     |
|    policy_gradient_loss | 0.0299    |
|    value_loss           | 0.444     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 235      |
|    time_elapsed    | 3439     |
|    total_timesteps | 481280   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 236         |
|    time_elapsed         | 3452        |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.032206036 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 12110       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.376       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.1      |
|    ep_rew_mean          | 8.05      |
| time/                   |           |
|    fps                  | 139       |
|    iterations           | 237       |
|    time_elapsed         | 3467      |
|    total_timesteps      | 485376    |
| train/                  |           |
|    approx_kl            | 0.0329063 |
|    clip_fraction        | 0.086     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.123    |
|    explained_variance   | 0.741     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0778    |
|    n_updates            | 12120     |
|    policy_gradient_loss | -0.0169   |
|    value_loss           | 0.405     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 238         |
|    time_elapsed         | 3480        |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.037129678 |
|    clip_fraction        | 0.0847      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0798      |
|    n_updates            | 12130       |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.302       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.73        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 239         |
|    time_elapsed         | 3494        |
|    total_timesteps      | 489472      |
| train/                  |             |
|    approx_kl            | 0.033040356 |
|    clip_fraction        | 0.0763      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 12140       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.301       |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=8.91 +/- 0.54
Episode length: 23.33 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.3        |
|    mean_reward          | 8.91        |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.034924615 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00509    |
|    n_updates            | 12150       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.272       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 240      |
|    time_elapsed    | 3509     |
|    total_timesteps | 491520   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.84        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 241         |
|    time_elapsed         | 3523        |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.045775034 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.411       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0775      |
|    n_updates            | 12160       |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.492       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 242        |
|    time_elapsed         | 3537       |
|    total_timesteps      | 495616     |
| train/                  |            |
|    approx_kl            | 0.07385202 |
|    clip_fraction        | 0.0926     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.102      |
|    n_updates            | 12170      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.401      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 243         |
|    time_elapsed         | 3551        |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.048340186 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0452      |
|    n_updates            | 12180       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.252       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 244         |
|    time_elapsed         | 3565        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.049780503 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0828      |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.325       |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=8.22 +/- 0.10
Episode length: 17.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 8.22       |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.04198761 |
|    clip_fraction        | 0.0887     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0948     |
|    n_updates            | 12200      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.478      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 7.93     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 245      |
|    time_elapsed    | 3579     |
|    total_timesteps | 501760   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.96       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 246        |
|    time_elapsed         | 3592       |
|    total_timesteps      | 503808     |
| train/                  |            |
|    approx_kl            | 0.04019846 |
|    clip_fraction        | 0.0791     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0831    |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0678     |
|    n_updates            | 12210      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.324      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 247         |
|    time_elapsed         | 3604        |
|    total_timesteps      | 505856      |
| train/                  |             |
|    approx_kl            | 0.041945286 |
|    clip_fraction        | 0.0815      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0947     |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0736      |
|    n_updates            | 12220       |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.379       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 248         |
|    time_elapsed         | 3617        |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.049295112 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0864      |
|    n_updates            | 12230       |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.566       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 249         |
|    time_elapsed         | 3630        |
|    total_timesteps      | 509952      |
| train/                  |             |
|    approx_kl            | 0.042738996 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 12240       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.399       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=510000, episode_reward=-1.24 +/- 6.93
Episode length: 39.00 +/- 15.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39         |
|    mean_reward          | -1.24      |
| time/                   |            |
|    total_timesteps      | 510000     |
| train/                  |            |
|    approx_kl            | 0.04338414 |
|    clip_fraction        | 0.0937     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.165      |
|    n_updates            | 12250      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.417      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.9      |
| time/              |          |
|    fps             | 140      |
|    iterations      | 250      |
|    time_elapsed    | 3645     |
|    total_timesteps | 512000   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23         |
|    ep_rew_mean          | 7.14       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 251        |
|    time_elapsed         | 3660       |
|    total_timesteps      | 514048     |
| train/                  |            |
|    approx_kl            | 0.04933235 |
|    clip_fraction        | 0.098      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.118      |
|    n_updates            | 12260      |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.297      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 252         |
|    time_elapsed         | 3674        |
|    total_timesteps      | 516096      |
| train/                  |             |
|    approx_kl            | 0.066180594 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.234       |
|    n_updates            | 12270       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.732       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.3        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 253         |
|    time_elapsed         | 3688        |
|    total_timesteps      | 518144      |
| train/                  |             |
|    approx_kl            | 0.056936372 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 12280       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.586       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=520000, episode_reward=3.17 +/- 7.91
Episode length: 29.67 +/- 14.43
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.7       |
|    mean_reward          | 3.17       |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.04951511 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.202     |
|    explained_variance   | 0.595      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.16       |
|    n_updates            | 12290      |
|    policy_gradient_loss | -0.0259    |
|    value_loss           | 0.616      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.89     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 254      |
|    time_elapsed    | 3702     |
|    total_timesteps | 520192   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.44       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 255        |
|    time_elapsed         | 3716       |
|    total_timesteps      | 522240     |
| train/                  |            |
|    approx_kl            | 0.04346873 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.121      |
|    n_updates            | 12300      |
|    policy_gradient_loss | -0.0205    |
|    value_loss           | 0.545      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.2      |
|    ep_rew_mean          | 7.99      |
| time/                   |           |
|    fps                  | 140       |
|    iterations           | 256       |
|    time_elapsed         | 3729      |
|    total_timesteps      | 524288    |
| train/                  |           |
|    approx_kl            | 0.0354719 |
|    clip_fraction        | 0.125     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.197    |
|    explained_variance   | 0.653     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.142     |
|    n_updates            | 12310     |
|    policy_gradient_loss | -0.0171   |
|    value_loss           | 0.796     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.56        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 257         |
|    time_elapsed         | 3742        |
|    total_timesteps      | 526336      |
| train/                  |             |
|    approx_kl            | 0.028713807 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.199       |
|    n_updates            | 12320       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.678       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.26        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 258         |
|    time_elapsed         | 3756        |
|    total_timesteps      | 528384      |
| train/                  |             |
|    approx_kl            | 0.048306465 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.542       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 12330       |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.506       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=530000, episode_reward=2.12 +/- 8.57
Episode length: 28.33 +/- 15.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.3       |
|    mean_reward          | 2.12       |
| time/                   |            |
|    total_timesteps      | 530000     |
| train/                  |            |
|    approx_kl            | 0.06684609 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.193     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 12340      |
|    policy_gradient_loss | -0.0299    |
|    value_loss           | 0.618      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 259      |
|    time_elapsed    | 3769     |
|    total_timesteps | 530432   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 260         |
|    time_elapsed         | 3783        |
|    total_timesteps      | 532480      |
| train/                  |             |
|    approx_kl            | 0.037320286 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 12350       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.369       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 261         |
|    time_elapsed         | 3798        |
|    total_timesteps      | 534528      |
| train/                  |             |
|    approx_kl            | 0.052123975 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.095       |
|    n_updates            | 12360       |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.317       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.45        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 262         |
|    time_elapsed         | 3815        |
|    total_timesteps      | 536576      |
| train/                  |             |
|    approx_kl            | 0.037122957 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.07        |
|    n_updates            | 12370       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.345       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 263        |
|    time_elapsed         | 3831       |
|    total_timesteps      | 538624     |
| train/                  |            |
|    approx_kl            | 0.04954881 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.135      |
|    n_updates            | 12380      |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.646      |
----------------------------------------
reached max steps=100
Eval num_timesteps=540000, episode_reward=4.43 +/- 6.04
Episode length: 30.00 +/- 14.17
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30         |
|    mean_reward          | 4.43       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.04220966 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.216     |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.191      |
|    n_updates            | 12390      |
|    policy_gradient_loss | -0.0217    |
|    value_loss           | 0.659      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 264      |
|    time_elapsed    | 3848     |
|    total_timesteps | 540672   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 265         |
|    time_elapsed         | 3862        |
|    total_timesteps      | 542720      |
| train/                  |             |
|    approx_kl            | 0.038565423 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0468      |
|    n_updates            | 12400       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.289       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.58        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 266         |
|    time_elapsed         | 3879        |
|    total_timesteps      | 544768      |
| train/                  |             |
|    approx_kl            | 0.047226813 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0959      |
|    n_updates            | 12410       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.315       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.7        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 267         |
|    time_elapsed         | 3899        |
|    total_timesteps      | 546816      |
| train/                  |             |
|    approx_kl            | 0.052255876 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0888      |
|    n_updates            | 12420       |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.486       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.32        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 268         |
|    time_elapsed         | 3919        |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.032651767 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 12430       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.531       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=550000, episode_reward=5.46 +/- 5.29
Episode length: 28.33 +/- 15.46
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.3        |
|    mean_reward          | 5.46        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.054089345 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.205       |
|    n_updates            | 12440       |
|    policy_gradient_loss | -0.00999    |
|    value_loss           | 0.685       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.27     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 269      |
|    time_elapsed    | 3941     |
|    total_timesteps | 550912   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 270        |
|    time_elapsed         | 3961       |
|    total_timesteps      | 552960     |
| train/                  |            |
|    approx_kl            | 0.06445266 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.177      |
|    n_updates            | 12450      |
|    policy_gradient_loss | -0.0236    |
|    value_loss           | 0.627      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.72       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 271        |
|    time_elapsed         | 3980       |
|    total_timesteps      | 555008     |
| train/                  |            |
|    approx_kl            | 0.09466275 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0808     |
|    n_updates            | 12460      |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.384      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 272        |
|    time_elapsed         | 3996       |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.04973633 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.203     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.168      |
|    n_updates            | 12470      |
|    policy_gradient_loss | -0.0211    |
|    value_loss           | 0.646      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 273         |
|    time_elapsed         | 4013        |
|    total_timesteps      | 559104      |
| train/                  |             |
|    approx_kl            | 0.031192716 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.613       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 12480       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.569       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=560000, episode_reward=1.82 +/- 8.37
Episode length: 26.67 +/- 16.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.7        |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.033275113 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.575       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0423      |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.406       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 274      |
|    time_elapsed    | 4029     |
|    total_timesteps | 561152   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 275         |
|    time_elapsed         | 4045        |
|    total_timesteps      | 563200      |
| train/                  |             |
|    approx_kl            | 0.064335555 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0515      |
|    n_updates            | 12500       |
|    policy_gradient_loss | -0.00394    |
|    value_loss           | 0.562       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 276         |
|    time_elapsed         | 4060        |
|    total_timesteps      | 565248      |
| train/                  |             |
|    approx_kl            | 0.039721332 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0466      |
|    n_updates            | 12510       |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.31        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 277         |
|    time_elapsed         | 4076        |
|    total_timesteps      | 567296      |
| train/                  |             |
|    approx_kl            | 0.033293515 |
|    clip_fraction        | 0.0864      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0719      |
|    n_updates            | 12520       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.411       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.36        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 278         |
|    time_elapsed         | 4091        |
|    total_timesteps      | 569344      |
| train/                  |             |
|    approx_kl            | 0.044877153 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.204       |
|    n_updates            | 12530       |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.51        |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=8.37 +/- 1.24
Episode length: 16.67 +/- 2.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | 8.37       |
| time/                   |            |
|    total_timesteps      | 570000     |
| train/                  |            |
|    approx_kl            | 0.09304587 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.366      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0991     |
|    n_updates            | 12540      |
|    policy_gradient_loss | -0.0278    |
|    value_loss           | 0.696      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 279      |
|    time_elapsed    | 4107     |
|    total_timesteps | 571392   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 280         |
|    time_elapsed         | 4122        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.057616666 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    n_updates            | 12550       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.586       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 281        |
|    time_elapsed         | 4139       |
|    total_timesteps      | 575488     |
| train/                  |            |
|    approx_kl            | 0.04305002 |
|    clip_fraction        | 0.0705     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0966    |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 12560      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.323      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.5       |
|    ep_rew_mean          | 7.91       |
| time/                   |            |
|    fps                  | 138        |
|    iterations           | 282        |
|    time_elapsed         | 4155       |
|    total_timesteps      | 577536     |
| train/                  |            |
|    approx_kl            | 0.07530896 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.599      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.083      |
|    n_updates            | 12570      |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.482      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 138         |
|    iterations           | 283         |
|    time_elapsed         | 4172        |
|    total_timesteps      | 579584      |
| train/                  |             |
|    approx_kl            | 0.038088553 |
|    clip_fraction        | 0.0651      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0761     |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.065       |
|    n_updates            | 12580       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.247       |
-----------------------------------------
Eval num_timesteps=580000, episode_reward=8.38 +/- 0.12
Episode length: 19.67 +/- 3.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 8.38        |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.045713507 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0485      |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.317       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.92     |
| time/              |          |
|    fps             | 138      |
|    iterations      | 284      |
|    time_elapsed    | 4187     |
|    total_timesteps | 581632   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 285         |
|    time_elapsed         | 4198        |
|    total_timesteps      | 583680      |
| train/                  |             |
|    approx_kl            | 0.042041622 |
|    clip_fraction        | 0.0848      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.148       |
|    n_updates            | 12600       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.355       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 286         |
|    time_elapsed         | 4208        |
|    total_timesteps      | 585728      |
| train/                  |             |
|    approx_kl            | 0.045314796 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0512      |
|    n_updates            | 12610       |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.264       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 287         |
|    time_elapsed         | 4218        |
|    total_timesteps      | 587776      |
| train/                  |             |
|    approx_kl            | 0.043085903 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.07        |
|    n_updates            | 12620       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.371       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.18        |
| time/                   |             |
|    fps                  | 139         |
|    iterations           | 288         |
|    time_elapsed         | 4230        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.033242412 |
|    clip_fraction        | 0.0743      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 12630       |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.315       |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=8.89 +/- 0.69
Episode length: 17.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.89        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.036817804 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 12640       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.32        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 139      |
|    iterations      | 289      |
|    time_elapsed    | 4242     |
|    total_timesteps | 591872   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 290        |
|    time_elapsed         | 4253       |
|    total_timesteps      | 593920     |
| train/                  |            |
|    approx_kl            | 0.04631587 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.588      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.156      |
|    n_updates            | 12650      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.519      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 291        |
|    time_elapsed         | 4264       |
|    total_timesteps      | 595968     |
| train/                  |            |
|    approx_kl            | 0.04633037 |
|    clip_fraction        | 0.0859     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0977    |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 12660      |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 0.31       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 8.21       |
| time/                   |            |
|    fps                  | 139        |
|    iterations           | 292        |
|    time_elapsed         | 4274       |
|    total_timesteps      | 598016     |
| train/                  |            |
|    approx_kl            | 0.46543676 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.212      |
|    n_updates            | 12670      |
|    policy_gradient_loss | -0.0291    |
|    value_loss           | 0.583      |
----------------------------------------
reached max steps=100
Eval num_timesteps=600000, episode_reward=9.64 +/- 0.18
Episode length: 20.00 +/- 0.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 20        |
|    mean_reward          | 9.64      |
| time/                   |           |
|    total_timesteps      | 600000    |
| train/                  |           |
|    approx_kl            | 0.0435369 |
|    clip_fraction        | 0.071     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0944   |
|    explained_variance   | 0.739     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.165     |
|    n_updates            | 12680     |
|    policy_gradient_loss | -0.0121   |
|    value_loss           | 0.352     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 139      |
|    iterations      | 293      |
|    time_elapsed    | 4286     |
|    total_timesteps | 600064   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 294        |
|    time_elapsed         | 4296       |
|    total_timesteps      | 602112     |
| train/                  |            |
|    approx_kl            | 0.05993095 |
|    clip_fraction        | 0.0764     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.097     |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0381     |
|    n_updates            | 12690      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.348      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.78        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 295         |
|    time_elapsed         | 4305        |
|    total_timesteps      | 604160      |
| train/                  |             |
|    approx_kl            | 0.032280404 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0881     |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.22        |
|    n_updates            | 12700       |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.287       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 140        |
|    iterations           | 296        |
|    time_elapsed         | 4315       |
|    total_timesteps      | 606208     |
| train/                  |            |
|    approx_kl            | 0.03889195 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.122     |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0441     |
|    n_updates            | 12710      |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 0.451      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.84        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 297         |
|    time_elapsed         | 4326        |
|    total_timesteps      | 608256      |
| train/                  |             |
|    approx_kl            | 0.049807437 |
|    clip_fraction        | 0.0795      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.102       |
|    n_updates            | 12720       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.396       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=610000, episode_reward=8.46 +/- 0.29
Episode length: 22.33 +/- 3.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 22.3      |
|    mean_reward          | 8.46      |
| time/                   |           |
|    total_timesteps      | 610000    |
| train/                  |           |
|    approx_kl            | 0.0784893 |
|    clip_fraction        | 0.112     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.156    |
|    explained_variance   | 0.691     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0878    |
|    n_updates            | 12730     |
|    policy_gradient_loss | -0.0207   |
|    value_loss           | 0.617     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 140      |
|    iterations      | 298      |
|    time_elapsed    | 4336     |
|    total_timesteps | 610304   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 299         |
|    time_elapsed         | 4347        |
|    total_timesteps      | 612352      |
| train/                  |             |
|    approx_kl            | 0.048375826 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0994      |
|    n_updates            | 12740       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.602       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.58        |
| time/                   |             |
|    fps                  | 140         |
|    iterations           | 300         |
|    time_elapsed         | 4357        |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.049775958 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.2         |
|    n_updates            | 12750       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.583       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 141         |
|    iterations           | 301         |
|    time_elapsed         | 4368        |
|    total_timesteps      | 616448      |
| train/                  |             |
|    approx_kl            | 0.047846276 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.39        |
|    n_updates            | 12760       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.644       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 141         |
|    iterations           | 302         |
|    time_elapsed         | 4380        |
|    total_timesteps      | 618496      |
| train/                  |             |
|    approx_kl            | 0.048737116 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0302      |
|    n_updates            | 12770       |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.459       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=620000, episode_reward=-1.16 +/- 6.84
Episode length: 38.67 +/- 16.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 38.7       |
|    mean_reward          | -1.16      |
| time/                   |            |
|    total_timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.03876206 |
|    clip_fraction        | 0.0956     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.257      |
|    n_updates            | 12780      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.433      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.84     |
| time/              |          |
|    fps             | 141      |
|    iterations      | 303      |
|    time_elapsed    | 4391     |
|    total_timesteps | 620544   |
---------------------------------
reached max steps=100
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20.3     |
|    ep_rew_mean          | 7.85     |
| time/                   |          |
|    fps                  | 141      |
|    iterations           | 304      |
|    time_elapsed         | 4401     |
|    total_timesteps      | 622592   |
| train/                  |          |
|    approx_kl            | 0.040031 |
|    clip_fraction        | 0.0901   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.147   |
|    explained_variance   | 0.733    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0433   |
|    n_updates            | 12790    |
|    policy_gradient_loss | 0.0111   |
|    value_loss           | 0.394    |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.91       |
| time/                   |            |
|    fps                  | 141        |
|    iterations           | 305        |
|    time_elapsed         | 4411       |
|    total_timesteps      | 624640     |
| train/                  |            |
|    approx_kl            | 0.02827785 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0833     |
|    n_updates            | 12800      |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.425      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.31        |
| time/                   |             |
|    fps                  | 141         |
|    iterations           | 306         |
|    time_elapsed         | 4421        |
|    total_timesteps      | 626688      |
| train/                  |             |
|    approx_kl            | 0.040115744 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.252       |
|    n_updates            | 12810       |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.467       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.65       |
| time/                   |            |
|    fps                  | 141        |
|    iterations           | 307        |
|    time_elapsed         | 4431       |
|    total_timesteps      | 628736     |
| train/                  |            |
|    approx_kl            | 0.06975524 |
|    clip_fraction        | 0.0878     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0972     |
|    n_updates            | 12820      |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 0.234      |
----------------------------------------
Eval num_timesteps=630000, episode_reward=7.85 +/- 0.96
Episode length: 16.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 7.85        |
| time/                   |             |
|    total_timesteps      | 630000      |
| train/                  |             |
|    approx_kl            | 0.101655774 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 12830       |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.535       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.73     |
| time/              |          |
|    fps             | 142      |
|    iterations      | 308      |
|    time_elapsed    | 4440     |
|    total_timesteps | 630784   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 142         |
|    iterations           | 309         |
|    time_elapsed         | 4450        |
|    total_timesteps      | 632832      |
| train/                  |             |
|    approx_kl            | 0.042041063 |
|    clip_fraction        | 0.096       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.128       |
|    n_updates            | 12840       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.358       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 142         |
|    iterations           | 310         |
|    time_elapsed         | 4460        |
|    total_timesteps      | 634880      |
| train/                  |             |
|    approx_kl            | 0.054817125 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.174       |
|    n_updates            | 12850       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.418       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 142        |
|    iterations           | 311        |
|    time_elapsed         | 4470       |
|    total_timesteps      | 636928     |
| train/                  |            |
|    approx_kl            | 0.06908466 |
|    clip_fraction        | 0.0933     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.126     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0295     |
|    n_updates            | 12860      |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 0.346      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.52        |
| time/                   |             |
|    fps                  | 142         |
|    iterations           | 312         |
|    time_elapsed         | 4480        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.044074386 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.627       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0974      |
|    n_updates            | 12870       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.579       |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=8.02 +/- 1.15
Episode length: 21.33 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.3       |
|    mean_reward          | 8.02       |
| time/                   |            |
|    total_timesteps      | 640000     |
| train/                  |            |
|    approx_kl            | 0.09872723 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.185     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.396      |
|    n_updates            | 12880      |
|    policy_gradient_loss | -0.00749   |
|    value_loss           | 0.684      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.13     |
| time/              |          |
|    fps             | 142      |
|    iterations      | 313      |
|    time_elapsed    | 4490     |
|    total_timesteps | 641024   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.87       |
| time/                   |            |
|    fps                  | 142        |
|    iterations           | 314        |
|    time_elapsed         | 4500       |
|    total_timesteps      | 643072     |
| train/                  |            |
|    approx_kl            | 0.03501847 |
|    clip_fraction        | 0.0713     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0991    |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0905     |
|    n_updates            | 12890      |
|    policy_gradient_loss | -0.0126    |
|    value_loss           | 0.285      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.81        |
| time/                   |             |
|    fps                  | 143         |
|    iterations           | 315         |
|    time_elapsed         | 4509        |
|    total_timesteps      | 645120      |
| train/                  |             |
|    approx_kl            | 0.036623813 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.102       |
|    n_updates            | 12900       |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.287       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 143        |
|    iterations           | 316        |
|    time_elapsed         | 4518       |
|    total_timesteps      | 647168     |
| train/                  |            |
|    approx_kl            | 0.04443857 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.571      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0474     |
|    n_updates            | 12910      |
|    policy_gradient_loss | -0.0283    |
|    value_loss           | 0.395      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 143        |
|    iterations           | 317        |
|    time_elapsed         | 4529       |
|    total_timesteps      | 649216     |
| train/                  |            |
|    approx_kl            | 0.08038684 |
|    clip_fraction        | 0.0784     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 12920      |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 0.324      |
----------------------------------------
Eval num_timesteps=650000, episode_reward=7.65 +/- 0.91
Episode length: 23.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23         |
|    mean_reward          | 7.65       |
| time/                   |            |
|    total_timesteps      | 650000     |
| train/                  |            |
|    approx_kl            | 0.03255867 |
|    clip_fraction        | 0.0832     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.129     |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.1        |
|    n_updates            | 12930      |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 0.386      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 143      |
|    iterations      | 318      |
|    time_elapsed    | 4539     |
|    total_timesteps | 651264   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.97       |
| time/                   |            |
|    fps                  | 143        |
|    iterations           | 319        |
|    time_elapsed         | 4549       |
|    total_timesteps      | 653312     |
| train/                  |            |
|    approx_kl            | 0.07927858 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.277      |
|    n_updates            | 12940      |
|    policy_gradient_loss | -0.0229    |
|    value_loss           | 0.766      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 8.05       |
| time/                   |            |
|    fps                  | 143        |
|    iterations           | 320        |
|    time_elapsed         | 4559       |
|    total_timesteps      | 655360     |
| train/                  |            |
|    approx_kl            | 0.03272086 |
|    clip_fraction        | 0.0956     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.144     |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0823     |
|    n_updates            | 12950      |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 0.29       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 143         |
|    iterations           | 321         |
|    time_elapsed         | 4569        |
|    total_timesteps      | 657408      |
| train/                  |             |
|    approx_kl            | 0.042811155 |
|    clip_fraction        | 0.0957      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0834      |
|    n_updates            | 12960       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.363       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 8.19        |
| time/                   |             |
|    fps                  | 144         |
|    iterations           | 322         |
|    time_elapsed         | 4579        |
|    total_timesteps      | 659456      |
| train/                  |             |
|    approx_kl            | 0.036082238 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.258       |
|    n_updates            | 12970       |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.325       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=660000, episode_reward=4.64 +/- 6.18
Episode length: 29.00 +/- 14.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29         |
|    mean_reward          | 4.64       |
| time/                   |            |
|    total_timesteps      | 660000     |
| train/                  |            |
|    approx_kl            | 0.03943543 |
|    clip_fraction        | 0.0834     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.105     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0502     |
|    n_updates            | 12980      |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.274      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 8.1      |
| time/              |          |
|    fps             | 144      |
|    iterations      | 323      |
|    time_elapsed    | 4589     |
|    total_timesteps | 661504   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 8.14        |
| time/                   |             |
|    fps                  | 144         |
|    iterations           | 324         |
|    time_elapsed         | 4599        |
|    total_timesteps      | 663552      |
| train/                  |             |
|    approx_kl            | 0.030862149 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0398      |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.288       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.17        |
| time/                   |             |
|    fps                  | 144         |
|    iterations           | 325         |
|    time_elapsed         | 4609        |
|    total_timesteps      | 665600      |
| train/                  |             |
|    approx_kl            | 0.042863645 |
|    clip_fraction        | 0.0886      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.201       |
|    n_updates            | 13000       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.459       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 144         |
|    iterations           | 326         |
|    time_elapsed         | 4618        |
|    total_timesteps      | 667648      |
| train/                  |             |
|    approx_kl            | 0.035999093 |
|    clip_fraction        | 0.0847      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 13010       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.338       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.71       |
| time/                   |            |
|    fps                  | 144        |
|    iterations           | 327        |
|    time_elapsed         | 4628       |
|    total_timesteps      | 669696     |
| train/                  |            |
|    approx_kl            | 0.04566388 |
|    clip_fraction        | 0.0911     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0936     |
|    n_updates            | 13020      |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.48       |
----------------------------------------
Eval num_timesteps=670000, episode_reward=8.52 +/- 0.91
Episode length: 19.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 8.52        |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.032203645 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.175       |
|    n_updates            | 13030       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.511       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.76     |
| time/              |          |
|    fps             | 144      |
|    iterations      | 328      |
|    time_elapsed    | 4638     |
|    total_timesteps | 671744   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 144         |
|    iterations           | 329         |
|    time_elapsed         | 4648        |
|    total_timesteps      | 673792      |
| train/                  |             |
|    approx_kl            | 0.051776983 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0951     |
|    explained_variance   | 0.654       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.358       |
|    n_updates            | 13040       |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.43        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 8.11       |
| time/                   |            |
|    fps                  | 145        |
|    iterations           | 330        |
|    time_elapsed         | 4658       |
|    total_timesteps      | 675840     |
| train/                  |            |
|    approx_kl            | 0.36571044 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0357     |
|    n_updates            | 13050      |
|    policy_gradient_loss | -0.0263    |
|    value_loss           | 0.365      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.73        |
| time/                   |             |
|    fps                  | 145         |
|    iterations           | 331         |
|    time_elapsed         | 4668        |
|    total_timesteps      | 677888      |
| train/                  |             |
|    approx_kl            | 0.030430913 |
|    clip_fraction        | 0.0726      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0838     |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0471      |
|    n_updates            | 13060       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.263       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 145         |
|    iterations           | 332         |
|    time_elapsed         | 4678        |
|    total_timesteps      | 679936      |
| train/                  |             |
|    approx_kl            | 0.050926574 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0498      |
|    n_updates            | 13070       |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.384       |
-----------------------------------------
Eval num_timesteps=680000, episode_reward=8.38 +/- 0.74
Episode length: 19.67 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.7       |
|    mean_reward          | 8.38       |
| time/                   |            |
|    total_timesteps      | 680000     |
| train/                  |            |
|    approx_kl            | 0.03444008 |
|    clip_fraction        | 0.069      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0931    |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.167      |
|    n_updates            | 13080      |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.37       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 145      |
|    iterations      | 333      |
|    time_elapsed    | 4688     |
|    total_timesteps | 681984   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.91       |
| time/                   |            |
|    fps                  | 145        |
|    iterations           | 334        |
|    time_elapsed         | 4697       |
|    total_timesteps      | 684032     |
| train/                  |            |
|    approx_kl            | 0.06251616 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.655      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.151      |
|    n_updates            | 13090      |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.399      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.7        |
|    ep_rew_mean          | 7.7         |
| time/                   |             |
|    fps                  | 145         |
|    iterations           | 335         |
|    time_elapsed         | 4710        |
|    total_timesteps      | 686080      |
| train/                  |             |
|    approx_kl            | 0.052780844 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0803      |
|    n_updates            | 13100       |
|    policy_gradient_loss | -0.028      |
|    value_loss           | 0.499       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 145         |
|    iterations           | 336         |
|    time_elapsed         | 4720        |
|    total_timesteps      | 688128      |
| train/                  |             |
|    approx_kl            | 0.047713853 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0994      |
|    n_updates            | 13110       |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.472       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=690000, episode_reward=8.22 +/- 1.47
Episode length: 17.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.22        |
| time/                   |             |
|    total_timesteps      | 690000      |
| train/                  |             |
|    approx_kl            | 0.033227548 |
|    clip_fraction        | 0.0928      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.15        |
|    n_updates            | 13120       |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.436       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 145      |
|    iterations      | 337      |
|    time_elapsed    | 4729     |
|    total_timesteps | 690176   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.96        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 338         |
|    time_elapsed         | 4740        |
|    total_timesteps      | 692224      |
| train/                  |             |
|    approx_kl            | 0.041096233 |
|    clip_fraction        | 0.0949      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 13130       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.607       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.89       |
| time/                   |            |
|    fps                  | 146        |
|    iterations           | 339        |
|    time_elapsed         | 4751       |
|    total_timesteps      | 694272     |
| train/                  |            |
|    approx_kl            | 0.03573868 |
|    clip_fraction        | 0.0906     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.148      |
|    n_updates            | 13140      |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.456      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 146        |
|    iterations           | 340        |
|    time_elapsed         | 4761       |
|    total_timesteps      | 696320     |
| train/                  |            |
|    approx_kl            | 0.10221687 |
|    clip_fraction        | 0.0982     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.149      |
|    n_updates            | 13150      |
|    policy_gradient_loss | -0.0253    |
|    value_loss           | 0.442      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 341         |
|    time_elapsed         | 4773        |
|    total_timesteps      | 698368      |
| train/                  |             |
|    approx_kl            | 0.024296723 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 13160       |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.668       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=700000, episode_reward=7.48 +/- 0.56
Episode length: 17.67 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 7.48        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.042816244 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 13170       |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.521       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.78     |
| time/              |          |
|    fps             | 146      |
|    iterations      | 342      |
|    time_elapsed    | 4784     |
|    total_timesteps | 700416   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.23        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 343         |
|    time_elapsed         | 4795        |
|    total_timesteps      | 702464      |
| train/                  |             |
|    approx_kl            | 0.038647376 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.155       |
|    n_updates            | 13180       |
|    policy_gradient_loss | -0.00888    |
|    value_loss           | 0.497       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 344         |
|    time_elapsed         | 4805        |
|    total_timesteps      | 704512      |
| train/                  |             |
|    approx_kl            | 0.040733665 |
|    clip_fraction        | 0.0679      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.096      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0782      |
|    n_updates            | 13190       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.272       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 345         |
|    time_elapsed         | 4815        |
|    total_timesteps      | 706560      |
| train/                  |             |
|    approx_kl            | 0.045336418 |
|    clip_fraction        | 0.0897      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 13200       |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.576       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.21        |
| time/                   |             |
|    fps                  | 146         |
|    iterations           | 346         |
|    time_elapsed         | 4824        |
|    total_timesteps      | 708608      |
| train/                  |             |
|    approx_kl            | 0.054936286 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.405       |
|    n_updates            | 13210       |
|    policy_gradient_loss | -0.0309     |
|    value_loss           | 0.59        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=710000, episode_reward=3.01 +/- 7.78
Episode length: 27.33 +/- 16.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27.3       |
|    mean_reward          | 3.01       |
| time/                   |            |
|    total_timesteps      | 710000     |
| train/                  |            |
|    approx_kl            | 0.03404352 |
|    clip_fraction        | 0.0729     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0941    |
|    explained_variance   | 0.705      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0657     |
|    n_updates            | 13220      |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 0.325      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.18     |
| time/              |          |
|    fps             | 146      |
|    iterations      | 347      |
|    time_elapsed    | 4834     |
|    total_timesteps | 710656   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.98       |
| time/                   |            |
|    fps                  | 147        |
|    iterations           | 348        |
|    time_elapsed         | 4844       |
|    total_timesteps      | 712704     |
| train/                  |            |
|    approx_kl            | 0.04361495 |
|    clip_fraction        | 0.0741     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0837    |
|    explained_variance   | 0.792      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0864     |
|    n_updates            | 13230      |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 0.263      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.47        |
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 349         |
|    time_elapsed         | 4854        |
|    total_timesteps      | 714752      |
| train/                  |             |
|    approx_kl            | 0.036499325 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.481       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.077       |
|    n_updates            | 13240       |
|    policy_gradient_loss | -0.00219    |
|    value_loss           | 0.495       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 350         |
|    time_elapsed         | 4865        |
|    total_timesteps      | 716800      |
| train/                  |             |
|    approx_kl            | 0.085540526 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.637       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 13250       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.554       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 351         |
|    time_elapsed         | 4876        |
|    total_timesteps      | 718848      |
| train/                  |             |
|    approx_kl            | 0.045695845 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 13260       |
|    policy_gradient_loss | -0.00936    |
|    value_loss           | 0.6         |
-----------------------------------------
reached max steps=100
Eval num_timesteps=720000, episode_reward=5.98 +/- 5.64
Episode length: 29.00 +/- 14.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 5.98        |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.039725937 |
|    clip_fraction        | 0.086       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0997     |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0857      |
|    n_updates            | 13270       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.382       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 7.83     |
| time/              |          |
|    fps             | 147      |
|    iterations      | 352      |
|    time_elapsed    | 4885     |
|    total_timesteps | 720896   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.8       |
|    ep_rew_mean          | 6.74       |
| time/                   |            |
|    fps                  | 147        |
|    iterations           | 353        |
|    time_elapsed         | 4897       |
|    total_timesteps      | 722944     |
| train/                  |            |
|    approx_kl            | 0.05812431 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.111      |
|    n_updates            | 13280      |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.373      |
----------------------------------------
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20.3     |
|    ep_rew_mean          | 8.12     |
| time/                   |          |
|    fps                  | 147      |
|    iterations           | 354      |
|    time_elapsed         | 4909     |
|    total_timesteps      | 724992   |
| train/                  |          |
|    approx_kl            | 0.367501 |
|    clip_fraction        | 0.273    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.21    |
|    explained_variance   | 0.542    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.405    |
|    n_updates            | 13290    |
|    policy_gradient_loss | -0.0293  |
|    value_loss           | 1.12     |
--------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 355         |
|    time_elapsed         | 4920        |
|    total_timesteps      | 727040      |
| train/                  |             |
|    approx_kl            | 0.049708527 |
|    clip_fraction        | 0.0777      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0931     |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 13300       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.397       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.05        |
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 356         |
|    time_elapsed         | 4930        |
|    total_timesteps      | 729088      |
| train/                  |             |
|    approx_kl            | 0.044139072 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.461       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 13310       |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.65        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=730000, episode_reward=7.28 +/- 0.45
Episode length: 21.67 +/- 2.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.7       |
|    mean_reward          | 7.28       |
| time/                   |            |
|    total_timesteps      | 730000     |
| train/                  |            |
|    approx_kl            | 0.03706831 |
|    clip_fraction        | 0.0713     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0763    |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.123      |
|    n_updates            | 13320      |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 0.339      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.3     |
|    ep_rew_mean     | 7.43     |
| time/              |          |
|    fps             | 147      |
|    iterations      | 357      |
|    time_elapsed    | 4941     |
|    total_timesteps | 731136   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 8.03       |
| time/                   |            |
|    fps                  | 148        |
|    iterations           | 358        |
|    time_elapsed         | 4952       |
|    total_timesteps      | 733184     |
| train/                  |            |
|    approx_kl            | 0.08067024 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.147     |
|    explained_variance   | 0.533      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.232      |
|    n_updates            | 13330      |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.749      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.02        |
| time/                   |             |
|    fps                  | 148         |
|    iterations           | 359         |
|    time_elapsed         | 4962        |
|    total_timesteps      | 735232      |
| train/                  |             |
|    approx_kl            | 0.041118823 |
|    clip_fraction        | 0.0912      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.64        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0935      |
|    n_updates            | 13340       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.424       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.88       |
| time/                   |            |
|    fps                  | 148        |
|    iterations           | 360        |
|    time_elapsed         | 4973       |
|    total_timesteps      | 737280     |
| train/                  |            |
|    approx_kl            | 0.07033961 |
|    clip_fraction        | 0.0862     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0868     |
|    n_updates            | 13350      |
|    policy_gradient_loss | -0.0187    |
|    value_loss           | 0.416      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.96       |
| time/                   |            |
|    fps                  | 148        |
|    iterations           | 361        |
|    time_elapsed         | 4984       |
|    total_timesteps      | 739328     |
| train/                  |            |
|    approx_kl            | 0.04687851 |
|    clip_fraction        | 0.0845     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.231      |
|    n_updates            | 13360      |
|    policy_gradient_loss | -0.00837   |
|    value_loss           | 0.443      |
----------------------------------------
reached max steps=100
Eval num_timesteps=740000, episode_reward=4.21 +/- 7.25
Episode length: 31.00 +/- 13.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 31         |
|    mean_reward          | 4.21       |
| time/                   |            |
|    total_timesteps      | 740000     |
| train/                  |            |
|    approx_kl            | 0.03242269 |
|    clip_fraction        | 0.089      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.22       |
|    n_updates            | 13370      |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.499      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 148      |
|    iterations      | 362      |
|    time_elapsed    | 4995     |
|    total_timesteps | 741376   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 148        |
|    iterations           | 363        |
|    time_elapsed         | 5007       |
|    total_timesteps      | 743424     |
| train/                  |            |
|    approx_kl            | 0.18924588 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0674     |
|    n_updates            | 13380      |
|    policy_gradient_loss | 0.00114    |
|    value_loss           | 0.506      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.3         |
| time/                   |             |
|    fps                  | 148         |
|    iterations           | 364         |
|    time_elapsed         | 5018        |
|    total_timesteps      | 745472      |
| train/                  |             |
|    approx_kl            | 0.028681096 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.351       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.2      |
|    ep_rew_mean          | 7.32      |
| time/                   |           |
|    fps                  | 148       |
|    iterations           | 365       |
|    time_elapsed         | 5029      |
|    total_timesteps      | 747520    |
| train/                  |           |
|    approx_kl            | 0.0741222 |
|    clip_fraction        | 0.17      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.213    |
|    explained_variance   | 0.505     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.19      |
|    n_updates            | 13400     |
|    policy_gradient_loss | -0.0324   |
|    value_loss           | 0.576     |
---------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.5      |
|    ep_rew_mean          | 8.17      |
| time/                   |           |
|    fps                  | 148       |
|    iterations           | 366       |
|    time_elapsed         | 5039      |
|    total_timesteps      | 749568    |
| train/                  |           |
|    approx_kl            | 0.0400697 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.174    |
|    explained_variance   | 0.647     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0633    |
|    n_updates            | 13410     |
|    policy_gradient_loss | -0.0126   |
|    value_loss           | 0.701     |
---------------------------------------
reached max steps=100
Eval num_timesteps=750000, episode_reward=6.04 +/- 1.36
Episode length: 27.33 +/- 6.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.3        |
|    mean_reward          | 6.04        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.038348235 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0983     |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.0099     |
|    value_loss           | 0.345       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.93     |
| time/              |          |
|    fps             | 148      |
|    iterations      | 367      |
|    time_elapsed    | 5051     |
|    total_timesteps | 751616   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.12        |
| time/                   |             |
|    fps                  | 148         |
|    iterations           | 368         |
|    time_elapsed         | 5062        |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.035594776 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0825      |
|    n_updates            | 13430       |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.37        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 148         |
|    iterations           | 369         |
|    time_elapsed         | 5073        |
|    total_timesteps      | 755712      |
| train/                  |             |
|    approx_kl            | 0.044060163 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0978     |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.067       |
|    n_updates            | 13440       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.262       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 149         |
|    iterations           | 370         |
|    time_elapsed         | 5084        |
|    total_timesteps      | 757760      |
| train/                  |             |
|    approx_kl            | 0.048658628 |
|    clip_fraction        | 0.0715      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0828     |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0131      |
|    n_updates            | 13450       |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.213       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.84        |
| time/                   |             |
|    fps                  | 149         |
|    iterations           | 371         |
|    time_elapsed         | 5095        |
|    total_timesteps      | 759808      |
| train/                  |             |
|    approx_kl            | 0.036741495 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0985      |
|    n_updates            | 13460       |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.258       |
-----------------------------------------
Eval num_timesteps=760000, episode_reward=7.29 +/- 0.57
Episode length: 17.00 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 7.29        |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.036621504 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.041       |
|    n_updates            | 13470       |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.391       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 8.12     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 372      |
|    time_elapsed    | 5106     |
|    total_timesteps | 761856   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 149         |
|    iterations           | 373         |
|    time_elapsed         | 5116        |
|    total_timesteps      | 763904      |
| train/                  |             |
|    approx_kl            | 0.033662908 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0426      |
|    n_updates            | 13480       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.252       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.8      |
|    ep_rew_mean          | 7.97      |
| time/                   |           |
|    fps                  | 149       |
|    iterations           | 374       |
|    time_elapsed         | 5127      |
|    total_timesteps      | 765952    |
| train/                  |           |
|    approx_kl            | 0.1263996 |
|    clip_fraction        | 0.132     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.155    |
|    explained_variance   | 0.644     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0847    |
|    n_updates            | 13490     |
|    policy_gradient_loss | -0.0225   |
|    value_loss           | 0.579     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 149         |
|    iterations           | 375         |
|    time_elapsed         | 5136        |
|    total_timesteps      | 768000      |
| train/                  |             |
|    approx_kl            | 0.041538533 |
|    clip_fraction        | 0.0985      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0904      |
|    n_updates            | 13500       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.398       |
-----------------------------------------
Eval num_timesteps=770000, episode_reward=9.26 +/- 0.69
Episode length: 18.67 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.7       |
|    mean_reward          | 9.26       |
| time/                   |            |
|    total_timesteps      | 770000     |
| train/                  |            |
|    approx_kl            | 0.03537322 |
|    clip_fraction        | 0.078      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.118     |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0802     |
|    n_updates            | 13510      |
|    policy_gradient_loss | -0.0122    |
|    value_loss           | 0.435      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    fps             | 149      |
|    iterations      | 376      |
|    time_elapsed    | 5146     |
|    total_timesteps | 770048   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 149         |
|    iterations           | 377         |
|    time_elapsed         | 5155        |
|    total_timesteps      | 772096      |
| train/                  |             |
|    approx_kl            | 0.085976705 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.684       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0926      |
|    n_updates            | 13520       |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.293       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.77       |
| time/                   |            |
|    fps                  | 149        |
|    iterations           | 378        |
|    time_elapsed         | 5165       |
|    total_timesteps      | 774144     |
| train/                  |            |
|    approx_kl            | 0.04759986 |
|    clip_fraction        | 0.072      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0961    |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 13530      |
|    policy_gradient_loss | -0.0073    |
|    value_loss           | 0.479      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.88       |
| time/                   |            |
|    fps                  | 150        |
|    iterations           | 379        |
|    time_elapsed         | 5174       |
|    total_timesteps      | 776192     |
| train/                  |            |
|    approx_kl            | 0.03623487 |
|    clip_fraction        | 0.0883     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 13540      |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.402      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 150         |
|    iterations           | 380         |
|    time_elapsed         | 5185        |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.041828945 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.141       |
|    n_updates            | 13550       |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.306       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=780000, episode_reward=8.52 +/- 1.36
Episode length: 19.00 +/- 2.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 8.52        |
| time/                   |             |
|    total_timesteps      | 780000      |
| train/                  |             |
|    approx_kl            | 0.039181624 |
|    clip_fraction        | 0.0752      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0625      |
|    n_updates            | 13560       |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.378       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 381      |
|    time_elapsed    | 5195     |
|    total_timesteps | 780288   |
---------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.1      |
|    ep_rew_mean          | 8.04      |
| time/                   |           |
|    fps                  | 150       |
|    iterations           | 382       |
|    time_elapsed         | 5204      |
|    total_timesteps      | 782336    |
| train/                  |           |
|    approx_kl            | 0.0493149 |
|    clip_fraction        | 0.0861    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.122    |
|    explained_variance   | 0.628     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.216     |
|    n_updates            | 13570     |
|    policy_gradient_loss | -0.00958  |
|    value_loss           | 0.491     |
---------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 150        |
|    iterations           | 383        |
|    time_elapsed         | 5215       |
|    total_timesteps      | 784384     |
| train/                  |            |
|    approx_kl            | 0.03257587 |
|    clip_fraction        | 0.0937     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.153     |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0566     |
|    n_updates            | 13580      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.415      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 150         |
|    iterations           | 384         |
|    time_elapsed         | 5226        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.067770794 |
|    clip_fraction        | 0.0967      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.203       |
|    n_updates            | 13590       |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.487       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.6      |
|    ep_rew_mean          | 7.55      |
| time/                   |           |
|    fps                  | 150       |
|    iterations           | 385       |
|    time_elapsed         | 5236      |
|    total_timesteps      | 788480    |
| train/                  |           |
|    approx_kl            | 0.0359044 |
|    clip_fraction        | 0.093     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.136    |
|    explained_variance   | 0.719     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0874    |
|    n_updates            | 13600     |
|    policy_gradient_loss | -0.0157   |
|    value_loss           | 0.405     |
---------------------------------------
Eval num_timesteps=790000, episode_reward=6.62 +/- 1.34
Episode length: 24.67 +/- 4.92
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 24.7       |
|    mean_reward          | 6.62       |
| time/                   |            |
|    total_timesteps      | 790000     |
| train/                  |            |
|    approx_kl            | 0.04131632 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.147     |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.153      |
|    n_updates            | 13610      |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.398      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.89     |
| time/              |          |
|    fps             | 150      |
|    iterations      | 386      |
|    time_elapsed    | 5247     |
|    total_timesteps | 790528   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 150         |
|    iterations           | 387         |
|    time_elapsed         | 5257        |
|    total_timesteps      | 792576      |
| train/                  |             |
|    approx_kl            | 0.041451193 |
|    clip_fraction        | 0.087       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0512      |
|    n_updates            | 13620       |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.34        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 150         |
|    iterations           | 388         |
|    time_elapsed         | 5267        |
|    total_timesteps      | 794624      |
| train/                  |             |
|    approx_kl            | 0.037110925 |
|    clip_fraction        | 0.0782      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0728      |
|    n_updates            | 13630       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.324       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 150        |
|    iterations           | 389        |
|    time_elapsed         | 5277       |
|    total_timesteps      | 796672     |
| train/                  |            |
|    approx_kl            | 0.03732855 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.328      |
|    n_updates            | 13640      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.597      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 390         |
|    time_elapsed         | 5286        |
|    total_timesteps      | 798720      |
| train/                  |             |
|    approx_kl            | 0.053073198 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0627      |
|    n_updates            | 13650       |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.248       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=800000, episode_reward=8.52 +/- 0.79
Episode length: 19.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 8.52        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.049070105 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.226       |
|    n_updates            | 13660       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.416       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.96     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 391      |
|    time_elapsed    | 5296     |
|    total_timesteps | 800768   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.38       |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 392        |
|    time_elapsed         | 5305       |
|    total_timesteps      | 802816     |
| train/                  |            |
|    approx_kl            | 0.08120408 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0676     |
|    n_updates            | 13670      |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.401      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.71        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 393         |
|    time_elapsed         | 5315        |
|    total_timesteps      | 804864      |
| train/                  |             |
|    approx_kl            | 0.032455985 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.087      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0901      |
|    n_updates            | 13680       |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.269       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 394         |
|    time_elapsed         | 5325        |
|    total_timesteps      | 806912      |
| train/                  |             |
|    approx_kl            | 0.040004708 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.717       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0193      |
|    n_updates            | 13690       |
|    policy_gradient_loss | -0.0256     |
|    value_loss           | 0.407       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 395         |
|    time_elapsed         | 5336        |
|    total_timesteps      | 808960      |
| train/                  |             |
|    approx_kl            | 0.027837902 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.097       |
|    n_updates            | 13700       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.54        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=810000, episode_reward=9.41 +/- 1.10
Episode length: 18.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 9.41        |
| time/                   |             |
|    total_timesteps      | 810000      |
| train/                  |             |
|    approx_kl            | 0.040135283 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.2         |
|    n_updates            | 13710       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.696       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 7.39     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 396      |
|    time_elapsed    | 5347     |
|    total_timesteps | 811008   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 397         |
|    time_elapsed         | 5359        |
|    total_timesteps      | 813056      |
| train/                  |             |
|    approx_kl            | 0.043626584 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.205      |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.193       |
|    n_updates            | 13720       |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.789       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 398         |
|    time_elapsed         | 5370        |
|    total_timesteps      | 815104      |
| train/                  |             |
|    approx_kl            | 0.057183735 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.629       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0605      |
|    n_updates            | 13730       |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.519       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 399         |
|    time_elapsed         | 5381        |
|    total_timesteps      | 817152      |
| train/                  |             |
|    approx_kl            | 0.027200457 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0867     |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.128       |
|    n_updates            | 13740       |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 0.292       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 400         |
|    time_elapsed         | 5393        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.037049636 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 13750       |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.517       |
-----------------------------------------
Eval num_timesteps=820000, episode_reward=8.29 +/- 0.00
Episode length: 17.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17         |
|    mean_reward          | 8.29       |
| time/                   |            |
|    total_timesteps      | 820000     |
| train/                  |            |
|    approx_kl            | 0.03231509 |
|    clip_fraction        | 0.0808     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0924     |
|    n_updates            | 13760      |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.378      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.94     |
| time/              |          |
|    fps             | 151      |
|    iterations      | 401      |
|    time_elapsed    | 5405     |
|    total_timesteps | 821248   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 402         |
|    time_elapsed         | 5416        |
|    total_timesteps      | 823296      |
| train/                  |             |
|    approx_kl            | 0.042313382 |
|    clip_fraction        | 0.0944      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0512      |
|    n_updates            | 13770       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.402       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.1      |
|    ep_rew_mean          | 7.93      |
| time/                   |           |
|    fps                  | 152       |
|    iterations           | 403       |
|    time_elapsed         | 5427      |
|    total_timesteps      | 825344    |
| train/                  |           |
|    approx_kl            | 0.0527417 |
|    clip_fraction        | 0.0897    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.127    |
|    explained_variance   | 0.713     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.104     |
|    n_updates            | 13780     |
|    policy_gradient_loss | -0.0132   |
|    value_loss           | 0.468     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 404         |
|    time_elapsed         | 5437        |
|    total_timesteps      | 827392      |
| train/                  |             |
|    approx_kl            | 0.045533136 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0703      |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.436       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 152        |
|    iterations           | 405        |
|    time_elapsed         | 5446       |
|    total_timesteps      | 829440     |
| train/                  |            |
|    approx_kl            | 0.03224139 |
|    clip_fraction        | 0.0651     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0977    |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0831     |
|    n_updates            | 13800      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.284      |
----------------------------------------
Eval num_timesteps=830000, episode_reward=8.16 +/- 1.39
Episode length: 20.67 +/- 3.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.7        |
|    mean_reward          | 8.16        |
| time/                   |             |
|    total_timesteps      | 830000      |
| train/                  |             |
|    approx_kl            | 0.036596365 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.118       |
|    n_updates            | 13810       |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.388       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 152      |
|    iterations      | 406      |
|    time_elapsed    | 5456     |
|    total_timesteps | 831488   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.2         |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 407         |
|    time_elapsed         | 5465        |
|    total_timesteps      | 833536      |
| train/                  |             |
|    approx_kl            | 0.058730915 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.33        |
|    n_updates            | 13820       |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.623       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.06        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 408         |
|    time_elapsed         | 5475        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.040864535 |
|    clip_fraction        | 0.0868      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.137       |
|    n_updates            | 13830       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.262       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 409         |
|    time_elapsed         | 5493        |
|    total_timesteps      | 837632      |
| train/                  |             |
|    approx_kl            | 0.028949743 |
|    clip_fraction        | 0.0755      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.142       |
|    n_updates            | 13840       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.321       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 410         |
|    time_elapsed         | 5507        |
|    total_timesteps      | 839680      |
| train/                  |             |
|    approx_kl            | 0.061790384 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0832     |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.048       |
|    n_updates            | 13850       |
|    policy_gradient_loss | -0.0248     |
|    value_loss           | 0.257       |
-----------------------------------------
Eval num_timesteps=840000, episode_reward=7.55 +/- 0.56
Episode length: 17.33 +/- 2.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 7.55       |
| time/                   |            |
|    total_timesteps      | 840000     |
| train/                  |            |
|    approx_kl            | 0.05110723 |
|    clip_fraction        | 0.0778     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0946    |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0514     |
|    n_updates            | 13860      |
|    policy_gradient_loss | -0.00634   |
|    value_loss           | 0.268      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 411      |
|    time_elapsed    | 5523     |
|    total_timesteps | 841728   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 412         |
|    time_elapsed         | 5537        |
|    total_timesteps      | 843776      |
| train/                  |             |
|    approx_kl            | 0.061335117 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0991      |
|    n_updates            | 13870       |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.48        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 413         |
|    time_elapsed         | 5550        |
|    total_timesteps      | 845824      |
| train/                  |             |
|    approx_kl            | 0.033368506 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.175       |
|    n_updates            | 13880       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.331       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 8.11       |
| time/                   |            |
|    fps                  | 152        |
|    iterations           | 414        |
|    time_elapsed         | 5565       |
|    total_timesteps      | 847872     |
| train/                  |            |
|    approx_kl            | 0.03474927 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.634      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.177      |
|    n_updates            | 13890      |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.442      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.1      |
|    ep_rew_mean          | 7.99      |
| time/                   |           |
|    fps                  | 152       |
|    iterations           | 415       |
|    time_elapsed         | 5577      |
|    total_timesteps      | 849920    |
| train/                  |           |
|    approx_kl            | 0.0455191 |
|    clip_fraction        | 0.0784    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.105    |
|    explained_variance   | 0.764     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0608    |
|    n_updates            | 13900     |
|    policy_gradient_loss | -0.0158   |
|    value_loss           | 0.254     |
---------------------------------------
Eval num_timesteps=850000, episode_reward=9.12 +/- 0.73
Episode length: 19.33 +/- 2.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.3       |
|    mean_reward          | 9.12       |
| time/                   |            |
|    total_timesteps      | 850000     |
| train/                  |            |
|    approx_kl            | 0.06625129 |
|    clip_fraction        | 0.0817     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0986    |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.142      |
|    n_updates            | 13910      |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 0.323      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.24     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 416      |
|    time_elapsed    | 5589     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 8.23        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 417         |
|    time_elapsed         | 5600        |
|    total_timesteps      | 854016      |
| train/                  |             |
|    approx_kl            | 0.028523082 |
|    clip_fraction        | 0.0676      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0952     |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0961      |
|    n_updates            | 13920       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.313       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 152        |
|    iterations           | 418        |
|    time_elapsed         | 5612       |
|    total_timesteps      | 856064     |
| train/                  |            |
|    approx_kl            | 0.03420821 |
|    clip_fraction        | 0.0792     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0679     |
|    n_updates            | 13930      |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.259      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.34        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 419         |
|    time_elapsed         | 5622        |
|    total_timesteps      | 858112      |
| train/                  |             |
|    approx_kl            | 0.034881353 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0848      |
|    n_updates            | 13940       |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.397       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=860000, episode_reward=8.29 +/- 1.20
Episode length: 17.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17         |
|    mean_reward          | 8.29       |
| time/                   |            |
|    total_timesteps      | 860000     |
| train/                  |            |
|    approx_kl            | 0.07604906 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.6        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.13       |
|    n_updates            | 13950      |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 0.48       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 420      |
|    time_elapsed    | 5630     |
|    total_timesteps | 860160   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.53        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 421         |
|    time_elapsed         | 5638        |
|    total_timesteps      | 862208      |
| train/                  |             |
|    approx_kl            | 0.045686573 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.191       |
|    n_updates            | 13960       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.59        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 422        |
|    time_elapsed         | 5647       |
|    total_timesteps      | 864256     |
| train/                  |            |
|    approx_kl            | 0.05045238 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.193     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.329      |
|    n_updates            | 13970      |
|    policy_gradient_loss | -0.0273    |
|    value_loss           | 0.755      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.55        |
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 423         |
|    time_elapsed         | 5655        |
|    total_timesteps      | 866304      |
| train/                  |             |
|    approx_kl            | 0.043682344 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 13980       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.636       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.69       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 424        |
|    time_elapsed         | 5662       |
|    total_timesteps      | 868352     |
| train/                  |            |
|    approx_kl            | 0.07668858 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.169     |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.208      |
|    n_updates            | 13990      |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.789      |
----------------------------------------
reached max steps=100
Eval num_timesteps=870000, episode_reward=7.86 +/- 1.49
Episode length: 19.00 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 7.86        |
| time/                   |             |
|    total_timesteps      | 870000      |
| train/                  |             |
|    approx_kl            | 0.048595693 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.184       |
|    n_updates            | 14000       |
|    policy_gradient_loss | 0.00247     |
|    value_loss           | 0.704       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.96     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 425      |
|    time_elapsed    | 5671     |
|    total_timesteps | 870400   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 426         |
|    time_elapsed         | 5683        |
|    total_timesteps      | 872448      |
| train/                  |             |
|    approx_kl            | 0.033987693 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.105       |
|    n_updates            | 14010       |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.526       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.99       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 427        |
|    time_elapsed         | 5694       |
|    total_timesteps      | 874496     |
| train/                  |            |
|    approx_kl            | 0.08564831 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.179     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.361      |
|    n_updates            | 14020      |
|    policy_gradient_loss | -0.0196    |
|    value_loss           | 0.873      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.4       |
|    ep_rew_mean          | 8.23       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 428        |
|    time_elapsed         | 5706       |
|    total_timesteps      | 876544     |
| train/                  |            |
|    approx_kl            | 0.03594354 |
|    clip_fraction        | 0.0749     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0868     |
|    n_updates            | 14030      |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.366      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 429        |
|    time_elapsed         | 5719       |
|    total_timesteps      | 878592     |
| train/                  |            |
|    approx_kl            | 0.03929781 |
|    clip_fraction        | 0.0787     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0255     |
|    n_updates            | 14040      |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.315      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=880000, episode_reward=8.45 +/- 0.46
Episode length: 19.33 +/- 2.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 8.45        |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.054830596 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0696      |
|    n_updates            | 14050       |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.414       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 8        |
| time/              |          |
|    fps             | 153      |
|    iterations      | 430      |
|    time_elapsed    | 5730     |
|    total_timesteps | 880640   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.18       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 431        |
|    time_elapsed         | 5743       |
|    total_timesteps      | 882688     |
| train/                  |            |
|    approx_kl            | 0.07506767 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0986     |
|    n_updates            | 14060      |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 0.384      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 432        |
|    time_elapsed         | 5754       |
|    total_timesteps      | 884736     |
| train/                  |            |
|    approx_kl            | 0.05052292 |
|    clip_fraction        | 0.0726     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0849    |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.111      |
|    n_updates            | 14070      |
|    policy_gradient_loss | -0.00615   |
|    value_loss           | 0.346      |
----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.7      |
|    ep_rew_mean          | 7.63      |
| time/                   |           |
|    fps                  | 153       |
|    iterations           | 433       |
|    time_elapsed         | 5762      |
|    total_timesteps      | 886784    |
| train/                  |           |
|    approx_kl            | 0.0492083 |
|    clip_fraction        | 0.099     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.123    |
|    explained_variance   | 0.684     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.135     |
|    n_updates            | 14080     |
|    policy_gradient_loss | -0.0116   |
|    value_loss           | 0.427     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.96        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 434         |
|    time_elapsed         | 5770        |
|    total_timesteps      | 888832      |
| train/                  |             |
|    approx_kl            | 0.043986443 |
|    clip_fraction        | 0.098       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0903      |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.0293     |
|    value_loss           | 0.35        |
-----------------------------------------
Eval num_timesteps=890000, episode_reward=7.92 +/- 0.84
Episode length: 15.67 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 7.92       |
| time/                   |            |
|    total_timesteps      | 890000     |
| train/                  |            |
|    approx_kl            | 0.08523483 |
|    clip_fraction        | 0.0975     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0981    |
|    explained_variance   | 0.517      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.126      |
|    n_updates            | 14100      |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.562      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 8.04     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 435      |
|    time_elapsed    | 5779     |
|    total_timesteps | 890880   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.84        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 436         |
|    time_elapsed         | 5792        |
|    total_timesteps      | 892928      |
| train/                  |             |
|    approx_kl            | 0.042100515 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0814     |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0513      |
|    n_updates            | 14110       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.331       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 437        |
|    time_elapsed         | 5803       |
|    total_timesteps      | 894976     |
| train/                  |            |
|    approx_kl            | 0.04317116 |
|    clip_fraction        | 0.0882     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.433      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 14120      |
|    policy_gradient_loss | -0.0227    |
|    value_loss           | 0.396      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 438         |
|    time_elapsed         | 5814        |
|    total_timesteps      | 897024      |
| train/                  |             |
|    approx_kl            | 0.036734797 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.634       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 14130       |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.356       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 439         |
|    time_elapsed         | 5826        |
|    total_timesteps      | 899072      |
| train/                  |             |
|    approx_kl            | 0.027359378 |
|    clip_fraction        | 0.0796      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0615      |
|    n_updates            | 14140       |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.354       |
-----------------------------------------
Eval num_timesteps=900000, episode_reward=8.15 +/- 1.29
Episode length: 17.67 +/- 1.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.15       |
| time/                   |            |
|    total_timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.22853984 |
|    clip_fraction        | 0.0946     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | 0.563      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.237      |
|    n_updates            | 14150      |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.468      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 8.15     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 440      |
|    time_elapsed    | 5840     |
|    total_timesteps | 901120   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.22        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 441         |
|    time_elapsed         | 5850        |
|    total_timesteps      | 903168      |
| train/                  |             |
|    approx_kl            | 0.028220467 |
|    clip_fraction        | 0.0448      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0481     |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0629      |
|    n_updates            | 14160       |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.208       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 442         |
|    time_elapsed         | 5858        |
|    total_timesteps      | 905216      |
| train/                  |             |
|    approx_kl            | 0.050513916 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.089      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0531      |
|    n_updates            | 14170       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.294       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 443         |
|    time_elapsed         | 5866        |
|    total_timesteps      | 907264      |
| train/                  |             |
|    approx_kl            | 0.046709724 |
|    clip_fraction        | 0.0823      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.118       |
|    n_updates            | 14180       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.421       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 444         |
|    time_elapsed         | 5873        |
|    total_timesteps      | 909312      |
| train/                  |             |
|    approx_kl            | 0.043341767 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00467     |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.361       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=910000, episode_reward=3.31 +/- 8.03
Episode length: 29.00 +/- 14.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 3.31        |
| time/                   |             |
|    total_timesteps      | 910000      |
| train/                  |             |
|    approx_kl            | 0.061272733 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.125       |
|    n_updates            | 14200       |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.461       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.78     |
| time/              |          |
|    fps             | 154      |
|    iterations      | 445      |
|    time_elapsed    | 5881     |
|    total_timesteps | 911360   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 8.03       |
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 446        |
|    time_elapsed         | 5889       |
|    total_timesteps      | 913408     |
| train/                  |            |
|    approx_kl            | 0.04235419 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0338     |
|    n_updates            | 14210      |
|    policy_gradient_loss | -0.024     |
|    value_loss           | 0.369      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.87       |
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 447        |
|    time_elapsed         | 5897       |
|    total_timesteps      | 915456     |
| train/                  |            |
|    approx_kl            | 0.04259056 |
|    clip_fraction        | 0.0722     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0858    |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0267     |
|    n_updates            | 14220      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.279      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 7.75       |
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 448        |
|    time_elapsed         | 5905       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.07801518 |
|    clip_fraction        | 0.0947     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0913     |
|    n_updates            | 14230      |
|    policy_gradient_loss | 0.0383     |
|    value_loss           | 0.473      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.24       |
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 449        |
|    time_elapsed         | 5916       |
|    total_timesteps      | 919552     |
| train/                  |            |
|    approx_kl            | 0.04956317 |
|    clip_fraction        | 0.0784     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0817    |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0454     |
|    n_updates            | 14240      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.303      |
----------------------------------------
Eval num_timesteps=920000, episode_reward=7.86 +/- 0.64
Episode length: 19.00 +/- 5.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 7.86        |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.030286724 |
|    clip_fraction        | 0.0638      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0799     |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.101       |
|    n_updates            | 14250       |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.392       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 7.85     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 450      |
|    time_elapsed    | 5926     |
|    total_timesteps | 921600   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 451         |
|    time_elapsed         | 5937        |
|    total_timesteps      | 923648      |
| train/                  |             |
|    approx_kl            | 0.031125814 |
|    clip_fraction        | 0.0612      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0705     |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0396      |
|    n_updates            | 14260       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.207       |
-----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.8      |
|    ep_rew_mean          | 8.07      |
| time/                   |           |
|    fps                  | 155       |
|    iterations           | 452       |
|    time_elapsed         | 5947      |
|    total_timesteps      | 925696    |
| train/                  |           |
|    approx_kl            | 0.0459117 |
|    clip_fraction        | 0.0739    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0854   |
|    explained_variance   | 0.73      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.382     |
|    n_updates            | 14270     |
|    policy_gradient_loss | -0.00414  |
|    value_loss           | 0.449     |
---------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 453         |
|    time_elapsed         | 5957        |
|    total_timesteps      | 927744      |
| train/                  |             |
|    approx_kl            | 0.031095922 |
|    clip_fraction        | 0.0752      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    n_updates            | 14280       |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.379       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 454        |
|    time_elapsed         | 5964       |
|    total_timesteps      | 929792     |
| train/                  |            |
|    approx_kl            | 0.10382727 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0994    |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.169      |
|    n_updates            | 14290      |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.49       |
----------------------------------------
reached max steps=100
Eval num_timesteps=930000, episode_reward=3.02 +/- 9.21
Episode length: 30.33 +/- 13.96
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.3       |
|    mean_reward          | 3.02       |
| time/                   |            |
|    total_timesteps      | 930000     |
| train/                  |            |
|    approx_kl            | 0.13999121 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.148     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0961     |
|    n_updates            | 14300      |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.519      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 8.01     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 455      |
|    time_elapsed    | 5972     |
|    total_timesteps | 931840   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.52        |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 456         |
|    time_elapsed         | 5979        |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.050189793 |
|    clip_fraction        | 0.0859      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.462       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 14310       |
|    policy_gradient_loss | -0.00209    |
|    value_loss           | 0.413       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 457         |
|    time_elapsed         | 5987        |
|    total_timesteps      | 935936      |
| train/                  |             |
|    approx_kl            | 0.066037506 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 14320       |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.484       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.7      |
|    ep_rew_mean          | 7.56      |
| time/                   |           |
|    fps                  | 156       |
|    iterations           | 458       |
|    time_elapsed         | 5994      |
|    total_timesteps      | 937984    |
| train/                  |           |
|    approx_kl            | 0.0392225 |
|    clip_fraction        | 0.096     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.121    |
|    explained_variance   | 0.603     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.108     |
|    n_updates            | 14330     |
|    policy_gradient_loss | -0.0196   |
|    value_loss           | 0.433     |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=940000, episode_reward=-2.09 +/- 4.79
Episode length: 38.33 +/- 16.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.3        |
|    mean_reward          | -2.09       |
| time/                   |             |
|    total_timesteps      | 940000      |
| train/                  |             |
|    approx_kl            | 0.096372694 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0796      |
|    n_updates            | 14340       |
|    policy_gradient_loss | -0.0351     |
|    value_loss           | 0.52        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 8.11     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 459      |
|    time_elapsed    | 6002     |
|    total_timesteps | 940032   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 460         |
|    time_elapsed         | 6010        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.029170435 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0736      |
|    n_updates            | 14350       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.404       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.99       |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 461        |
|    time_elapsed         | 6020       |
|    total_timesteps      | 944128     |
| train/                  |            |
|    approx_kl            | 0.06613867 |
|    clip_fraction        | 0.0773     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0878    |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0971     |
|    n_updates            | 14360      |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 0.257      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 462        |
|    time_elapsed         | 6031       |
|    total_timesteps      | 946176     |
| train/                  |            |
|    approx_kl            | 0.07744039 |
|    clip_fraction        | 0.0771     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.091     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0645     |
|    n_updates            | 14370      |
|    policy_gradient_loss | -0.0224    |
|    value_loss           | 0.326      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 463         |
|    time_elapsed         | 6041        |
|    total_timesteps      | 948224      |
| train/                  |             |
|    approx_kl            | 0.055551857 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0224      |
|    n_updates            | 14380       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.346       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=950000, episode_reward=2.57 +/- 7.48
Episode length: 29.33 +/- 14.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 2.57        |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.025376877 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 0.329       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 156      |
|    iterations      | 464      |
|    time_elapsed    | 6053     |
|    total_timesteps | 950272   |
---------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.2      |
|    ep_rew_mean          | 7.76      |
| time/                   |           |
|    fps                  | 157       |
|    iterations           | 465       |
|    time_elapsed         | 6065      |
|    total_timesteps      | 952320    |
| train/                  |           |
|    approx_kl            | 0.0925036 |
|    clip_fraction        | 0.118     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.159    |
|    explained_variance   | 0.635     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.232     |
|    n_updates            | 14400     |
|    policy_gradient_loss | 0.323     |
|    value_loss           | 0.543     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.47        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 466         |
|    time_elapsed         | 6077        |
|    total_timesteps      | 954368      |
| train/                  |             |
|    approx_kl            | 0.043958314 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.173       |
|    n_updates            | 14410       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.565       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.8       |
|    ep_rew_mean          | 8.27       |
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 467        |
|    time_elapsed         | 6089       |
|    total_timesteps      | 956416     |
| train/                  |            |
|    approx_kl            | 0.04062824 |
|    clip_fraction        | 0.091      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.179      |
|    n_updates            | 14420      |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.496      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 468         |
|    time_elapsed         | 6097        |
|    total_timesteps      | 958464      |
| train/                  |             |
|    approx_kl            | 0.061338224 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0819     |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0717      |
|    n_updates            | 14430       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.231       |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=6.38 +/- 2.32
Episode length: 19.67 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.7       |
|    mean_reward          | 6.38       |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.07055644 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.131     |
|    explained_variance   | 0.695      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.129      |
|    n_updates            | 14440      |
|    policy_gradient_loss | -0.0278    |
|    value_loss           | 0.532      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 8.03     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 469      |
|    time_elapsed    | 6105     |
|    total_timesteps | 960512   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.92       |
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 470        |
|    time_elapsed         | 6113       |
|    total_timesteps      | 962560     |
| train/                  |            |
|    approx_kl            | 0.04376319 |
|    clip_fraction        | 0.0896     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0584     |
|    n_updates            | 14450      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.387      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 471        |
|    time_elapsed         | 6121       |
|    total_timesteps      | 964608     |
| train/                  |            |
|    approx_kl            | 0.05827033 |
|    clip_fraction        | 0.078      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0805     |
|    n_updates            | 14460      |
|    policy_gradient_loss | -0.0172    |
|    value_loss           | 0.254      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.35        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 472         |
|    time_elapsed         | 6131        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.044183344 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.035       |
|    n_updates            | 14470       |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.334       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.02        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 473         |
|    time_elapsed         | 6142        |
|    total_timesteps      | 968704      |
| train/                  |             |
|    approx_kl            | 0.030904423 |
|    clip_fraction        | 0.0542      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0785     |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0781      |
|    n_updates            | 14480       |
|    policy_gradient_loss | -0.0094     |
|    value_loss           | 0.258       |
-----------------------------------------
Eval num_timesteps=970000, episode_reward=8.97 +/- 1.25
Episode length: 20.00 +/- 1.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20         |
|    mean_reward          | 8.97       |
| time/                   |            |
|    total_timesteps      | 970000     |
| train/                  |            |
|    approx_kl            | 0.03324926 |
|    clip_fraction        | 0.0739     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0967    |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0645     |
|    n_updates            | 14490      |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.279      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 157      |
|    iterations      | 474      |
|    time_elapsed    | 6153     |
|    total_timesteps | 970752   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | 8.23       |
| time/                   |            |
|    fps                  | 157        |
|    iterations           | 475        |
|    time_elapsed         | 6164       |
|    total_timesteps      | 972800     |
| train/                  |            |
|    approx_kl            | 0.07238142 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.128     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.407      |
|    n_updates            | 14500      |
|    policy_gradient_loss | -0.0248    |
|    value_loss           | 0.667      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.35        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 476         |
|    time_elapsed         | 6175        |
|    total_timesteps      | 974848      |
| train/                  |             |
|    approx_kl            | 0.052038796 |
|    clip_fraction        | 0.058       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0871     |
|    explained_variance   | 0.827       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0323      |
|    n_updates            | 14510       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.204       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.18        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 477         |
|    time_elapsed         | 6183        |
|    total_timesteps      | 976896      |
| train/                  |             |
|    approx_kl            | 0.039793044 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0864     |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0401      |
|    n_updates            | 14520       |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.217       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 478         |
|    time_elapsed         | 6190        |
|    total_timesteps      | 978944      |
| train/                  |             |
|    approx_kl            | 0.037987866 |
|    clip_fraction        | 0.0722      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0448      |
|    n_updates            | 14530       |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.277       |
-----------------------------------------
Eval num_timesteps=980000, episode_reward=7.45 +/- 2.61
Episode length: 19.33 +/- 2.62
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.3       |
|    mean_reward          | 7.45       |
| time/                   |            |
|    total_timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.04881643 |
|    clip_fraction        | 0.0768     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0982    |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0526     |
|    n_updates            | 14540      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.259      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 8.12     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 479      |
|    time_elapsed    | 6198     |
|    total_timesteps | 980992   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 480         |
|    time_elapsed         | 6206        |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.055929523 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0927     |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0734      |
|    n_updates            | 14550       |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.284       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.19        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 481         |
|    time_elapsed         | 6214        |
|    total_timesteps      | 985088      |
| train/                  |             |
|    approx_kl            | 0.030511703 |
|    clip_fraction        | 0.0913      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.2         |
|    n_updates            | 14560       |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.454       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 482         |
|    time_elapsed         | 6222        |
|    total_timesteps      | 987136      |
| train/                  |             |
|    approx_kl            | 0.042422947 |
|    clip_fraction        | 0.0691      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0826     |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0245      |
|    n_updates            | 14570       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.239       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 483         |
|    time_elapsed         | 6229        |
|    total_timesteps      | 989184      |
| train/                  |             |
|    approx_kl            | 0.065253116 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 14580       |
|    policy_gradient_loss | -0.0217     |
|    value_loss           | 0.319       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=990000, episode_reward=2.20 +/- 8.75
Episode length: 28.00 +/- 15.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28         |
|    mean_reward          | 2.2        |
| time/                   |            |
|    total_timesteps      | 990000     |
| train/                  |            |
|    approx_kl            | 0.03409831 |
|    clip_fraction        | 0.0862     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.118     |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0329     |
|    n_updates            | 14590      |
|    policy_gradient_loss | -0.022     |
|    value_loss           | 0.366      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.9      |
| time/              |          |
|    fps             | 158      |
|    iterations      | 484      |
|    time_elapsed    | 6238     |
|    total_timesteps | 991232   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.96        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 485         |
|    time_elapsed         | 6249        |
|    total_timesteps      | 993280      |
| train/                  |             |
|    approx_kl            | 0.037661154 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0687      |
|    n_updates            | 14600       |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.587       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 486         |
|    time_elapsed         | 6260        |
|    total_timesteps      | 995328      |
| train/                  |             |
|    approx_kl            | 0.028446427 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0966      |
|    n_updates            | 14610       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.394       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 487        |
|    time_elapsed         | 6272       |
|    total_timesteps      | 997376     |
| train/                  |            |
|    approx_kl            | 0.05022434 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.179      |
|    n_updates            | 14620      |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.487      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 488        |
|    time_elapsed         | 6283       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.05232372 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0651     |
|    n_updates            | 14630      |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.496      |
----------------------------------------
reached max steps=100
Eval num_timesteps=1000000, episode_reward=2.27 +/- 8.78
Episode length: 27.67 +/- 15.84
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27.7       |
|    mean_reward          | 2.27       |
| time/                   |            |
|    total_timesteps      | 1000000    |
| train/                  |            |
|    approx_kl            | 0.03393946 |
|    clip_fraction        | 0.0973     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.23       |
|    n_updates            | 14640      |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.54       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 159      |
|    iterations      | 489      |
|    time_elapsed    | 6292     |
|    total_timesteps | 1001472  |
---------------------------------
