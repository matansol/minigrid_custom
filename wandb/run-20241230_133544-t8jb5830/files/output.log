Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Loaded model from models\orig_easy8_20241111\iter_1000000_steps. Continuing training.
Logging to ./logs/ppo/minigrid_custom_tensorboard/20241230_2
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x000002779DA84370> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000027797535000>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 7.15     |
| time/              |          |
|    fps             | 96       |
|    iterations      | 1        |
|    time_elapsed    | 21       |
|    total_timesteps | 2048     |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 25.9       |
|    ep_rew_mean          | 6.29       |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 2          |
|    time_elapsed         | 41         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.07814032 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.223     |
|    explained_variance   | 0.187      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.51       |
|    n_updates            | 9770       |
|    policy_gradient_loss | -0.00902   |
|    value_loss           | 1.24       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.3        |
|    ep_rew_mean          | 6.2         |
| time/                   |             |
|    fps                  | 97          |
|    iterations           | 3           |
|    time_elapsed         | 62          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.071696386 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.342      |
|    explained_variance   | 0.445       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.442       |
|    n_updates            | 9780        |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 1.72        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 26.2        |
|    ep_rew_mean          | 6.13        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 4           |
|    time_elapsed         | 82          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.058119103 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.798       |
|    n_updates            | 9790        |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 1.49        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=10000, episode_reward=7.65 +/- 1.09
Episode length: 23.00 +/- 4.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23          |
|    mean_reward          | 7.65        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.058815956 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.28       |
|    explained_variance   | 0.509       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.432       |
|    n_updates            | 9800        |
|    policy_gradient_loss | 0.000485    |
|    value_loss           | 1.66        |
-----------------------------------------
New best mean reward!
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 5.71     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 5        |
|    time_elapsed    | 103      |
|    total_timesteps | 10240    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.1        |
|    ep_rew_mean          | 6.67        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 6           |
|    time_elapsed         | 123         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.104927756 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.351      |
|    explained_variance   | 0.432       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.76        |
|    n_updates            | 9810        |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 1.59        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.7       |
|    ep_rew_mean          | 6.87       |
| time/                   |            |
|    fps                  | 99         |
|    iterations           | 7          |
|    time_elapsed         | 143        |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07788292 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.224     |
|    explained_variance   | 0.399      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.65       |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.017     |
|    value_loss           | 1.24       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.6       |
|    ep_rew_mean          | 6.8        |
| time/                   |            |
|    fps                  | 96         |
|    iterations           | 8          |
|    time_elapsed         | 169        |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.05886477 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.222     |
|    explained_variance   | 0.468      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.618      |
|    n_updates            | 9830       |
|    policy_gradient_loss | -0.0217    |
|    value_loss           | 1.08       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.4      |
|    ep_rew_mean          | 7.02      |
| time/                   |           |
|    fps                  | 95        |
|    iterations           | 9         |
|    time_elapsed         | 193       |
|    total_timesteps      | 18432     |
| train/                  |           |
|    approx_kl            | 0.0348946 |
|    clip_fraction        | 0.121     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.228    |
|    explained_variance   | 0.549     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.36      |
|    n_updates            | 9840      |
|    policy_gradient_loss | -0.0134   |
|    value_loss           | 1.09      |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=20000, episode_reward=2.58 +/- 8.91
Episode length: 32.33 +/- 12.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.3        |
|    mean_reward          | 2.58        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.060912192 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.53        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.18        |
|    n_updates            | 9850        |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 0.716       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.3     |
|    ep_rew_mean     | 6.93     |
| time/              |          |
|    fps             | 93       |
|    iterations      | 10       |
|    time_elapsed    | 219      |
|    total_timesteps | 20480    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 91         |
|    iterations           | 11         |
|    time_elapsed         | 247        |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.05552564 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.216     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.373      |
|    n_updates            | 9860       |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.796      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.25        |
| time/                   |             |
|    fps                  | 89          |
|    iterations           | 12          |
|    time_elapsed         | 274         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.029412854 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.335       |
|    n_updates            | 9870        |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.752       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.3        |
|    ep_rew_mean          | 6.86        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 13          |
|    time_elapsed         | 302         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.042853516 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.175       |
|    n_updates            | 9880        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.703       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 14          |
|    time_elapsed         | 327         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.064130515 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.423       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.676       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=30000, episode_reward=-1.90 +/- 8.62
Episode length: 39.00 +/- 15.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39         |
|    mean_reward          | -1.9       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.05172202 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.174     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.423      |
|    n_updates            | 9900       |
|    policy_gradient_loss | -0.0207    |
|    value_loss           | 0.614      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 6.9      |
| time/              |          |
|    fps             | 86       |
|    iterations      | 15       |
|    time_elapsed    | 353      |
|    total_timesteps | 30720    |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.24        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 16          |
|    time_elapsed         | 379         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.054625288 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | 0.513       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.22        |
|    n_updates            | 9910        |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.935       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 17         |
|    time_elapsed         | 404        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.04960318 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.138     |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.275      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.609      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.6        |
|    ep_rew_mean          | 6.97        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 18          |
|    time_elapsed         | 430         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.064056054 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.482       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.19        |
|    n_updates            | 9930        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.44        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.34        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 19          |
|    time_elapsed         | 456         |
|    total_timesteps      | 38912       |
| train/                  |             |
|    approx_kl            | 0.060204234 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.472       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.349       |
|    n_updates            | 9940        |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.787       |
-----------------------------------------
Eval num_timesteps=40000, episode_reward=8.38 +/- 1.35
Episode length: 19.67 +/- 3.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 8.38        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.077654175 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.234       |
|    n_updates            | 9950        |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.659       |
-----------------------------------------
New best mean reward!
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 84       |
|    iterations      | 20       |
|    time_elapsed    | 482      |
|    total_timesteps | 40960    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.4        |
|    ep_rew_mean          | 6.85        |
| time/                   |             |
|    fps                  | 84          |
|    iterations           | 21          |
|    time_elapsed         | 508         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.043395076 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.204       |
|    n_updates            | 9960        |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.537       |
-----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.2      |
|    ep_rew_mean          | 7.16      |
| time/                   |           |
|    fps                  | 83        |
|    iterations           | 22        |
|    time_elapsed         | 536       |
|    total_timesteps      | 45056     |
| train/                  |           |
|    approx_kl            | 0.0557874 |
|    clip_fraction        | 0.154     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.204    |
|    explained_variance   | 0.56      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.311     |
|    n_updates            | 9970      |
|    policy_gradient_loss | -0.0262   |
|    value_loss           | 0.988     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.9        |
|    ep_rew_mean          | 7.23        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 23          |
|    time_elapsed         | 561         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.038818594 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.136       |
|    n_updates            | 9980        |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.653       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | 7.69       |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 24         |
|    time_elapsed         | 585        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.04395497 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 9990       |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.496      |
----------------------------------------
Eval num_timesteps=50000, episode_reward=7.52 +/- 1.06
Episode length: 19.00 +/- 2.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 7.52        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.041094363 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0983      |
|    n_updates            | 10000       |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.509       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 7.08     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 25       |
|    time_elapsed    | 611      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 26          |
|    time_elapsed         | 637         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.041208763 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.415       |
|    n_updates            | 10010       |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.898       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23          |
|    ep_rew_mean          | 6.81        |
| time/                   |             |
|    fps                  | 83          |
|    iterations           | 27          |
|    time_elapsed         | 662         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.062121764 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.241       |
|    n_updates            | 10020       |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.61        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.7      |
|    ep_rew_mean          | 6.5       |
| time/                   |           |
|    fps                  | 83        |
|    iterations           | 28        |
|    time_elapsed         | 687       |
|    total_timesteps      | 57344     |
| train/                  |           |
|    approx_kl            | 0.6033865 |
|    clip_fraction        | 0.213     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.156    |
|    explained_variance   | 0.487     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.35      |
|    n_updates            | 10030     |
|    policy_gradient_loss | -0.00916  |
|    value_loss           | 0.925     |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 7.15       |
| time/                   |            |
|    fps                  | 83         |
|    iterations           | 29         |
|    time_elapsed         | 712        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.13465068 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.202     |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.338      |
|    n_updates            | 10040      |
|    policy_gradient_loss | -0.0311    |
|    value_loss           | 1.12       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=60000, episode_reward=8.67 +/- 0.22
Episode length: 18.33 +/- 3.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 8.67       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.07188596 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.172     |
|    explained_variance   | 0.468      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.269      |
|    n_updates            | 10050      |
|    policy_gradient_loss | -0.00329   |
|    value_loss           | 1.12       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 6.85     |
| time/              |          |
|    fps             | 83       |
|    iterations      | 30       |
|    time_elapsed    | 733      |
|    total_timesteps | 61440    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.7       |
|    ep_rew_mean          | 7.16       |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 31         |
|    time_elapsed         | 754        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.05805927 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.184     |
|    explained_variance   | 0.461      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.336      |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.0253    |
|    value_loss           | 0.824      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.3        |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 32         |
|    time_elapsed         | 777        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.06965524 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.517      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.752      |
|    n_updates            | 10070      |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 1.07       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 84         |
|    iterations           | 33         |
|    time_elapsed         | 797        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.06485163 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.238      |
|    n_updates            | 10080      |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.61       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.4        |
|    ep_rew_mean          | 7.12        |
| time/                   |             |
|    fps                  | 85          |
|    iterations           | 34          |
|    time_elapsed         | 816         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.047629938 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 10090       |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.529       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=70000, episode_reward=2.06 +/- 8.55
Episode length: 31.67 +/- 14.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 31.7       |
|    mean_reward          | 2.06       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.07441148 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.242      |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.954      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 7.07     |
| time/              |          |
|    fps             | 85       |
|    iterations      | 35       |
|    time_elapsed    | 838      |
|    total_timesteps | 71680    |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.64       |
| time/                   |            |
|    fps                  | 85         |
|    iterations           | 36         |
|    time_elapsed         | 859        |
|    total_timesteps      | 73728      |
| train/                  |            |
|    approx_kl            | 0.07720324 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.164     |
|    explained_variance   | 0.387      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.171      |
|    n_updates            | 10110      |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 0.81       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 86          |
|    iterations           | 37          |
|    time_elapsed         | 881         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.047633465 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.283       |
|    n_updates            | 10120       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.714       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 38         |
|    time_elapsed         | 901        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.04957454 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.126     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0852     |
|    n_updates            | 10130      |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 0.555      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 86         |
|    iterations           | 39         |
|    time_elapsed         | 920        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.08133504 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.221      |
|    n_updates            | 10140      |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 0.576      |
----------------------------------------
Eval num_timesteps=80000, episode_reward=7.80 +/- 0.58
Episode length: 22.33 +/- 3.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.3        |
|    mean_reward          | 7.8         |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.052650895 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.605       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 10150       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.624       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 86       |
|    iterations      | 40       |
|    time_elapsed    | 941      |
|    total_timesteps | 81920    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.8        |
|    ep_rew_mean          | 7.21        |
| time/                   |             |
|    fps                  | 87          |
|    iterations           | 41          |
|    time_elapsed         | 962         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.041515045 |
|    clip_fraction        | 0.0922      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.237       |
|    n_updates            | 10160       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.635       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 6.94       |
| time/                   |            |
|    fps                  | 87         |
|    iterations           | 42         |
|    time_elapsed         | 977        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.07494422 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.176     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.402      |
|    n_updates            | 10170      |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.827      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22         |
|    ep_rew_mean          | 6.84       |
| time/                   |            |
|    fps                  | 88         |
|    iterations           | 43         |
|    time_elapsed         | 991        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.06587911 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.543      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.259      |
|    n_updates            | 10180      |
|    policy_gradient_loss | -0.00581   |
|    value_loss           | 0.69       |
----------------------------------------
reached max steps=100
Eval num_timesteps=90000, episode_reward=8.67 +/- 0.82
Episode length: 18.33 +/- 2.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 8.67        |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.047149733 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.291       |
|    n_updates            | 10190       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 1.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 89       |
|    iterations      | 44       |
|    time_elapsed    | 1005     |
|    total_timesteps | 90112    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.48        |
| time/                   |             |
|    fps                  | 90          |
|    iterations           | 45          |
|    time_elapsed         | 1019        |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.062463894 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.496       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 10200       |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.711       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.36        |
| time/                   |             |
|    fps                  | 90          |
|    iterations           | 46          |
|    time_elapsed         | 1035        |
|    total_timesteps      | 94208       |
| train/                  |             |
|    approx_kl            | 0.067147315 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.205       |
|    n_updates            | 10210       |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.441       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 91         |
|    iterations           | 47         |
|    time_elapsed         | 1049       |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.06038653 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.138     |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.195      |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.0195    |
|    value_loss           | 0.642      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.05        |
| time/                   |             |
|    fps                  | 92          |
|    iterations           | 48          |
|    time_elapsed         | 1064        |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.037289694 |
|    clip_fraction        | 0.0706      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.099      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.074       |
|    n_updates            | 10230       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.487       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=100000, episode_reward=7.63 +/- 0.66
Episode length: 17.00 +/- 1.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17        |
|    mean_reward          | 7.63      |
| time/                   |           |
|    total_timesteps      | 100000    |
| train/                  |           |
|    approx_kl            | 0.0542877 |
|    clip_fraction        | 0.129     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.147    |
|    explained_variance   | 0.583     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.103     |
|    n_updates            | 10240     |
|    policy_gradient_loss | -0.0186   |
|    value_loss           | 0.768     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 6.72     |
| time/              |          |
|    fps             | 92       |
|    iterations      | 49       |
|    time_elapsed    | 1079     |
|    total_timesteps | 100352   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 6.82        |
| time/                   |             |
|    fps                  | 93          |
|    iterations           | 50          |
|    time_elapsed         | 1093        |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.070453204 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 10250       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.655       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.81        |
| time/                   |             |
|    fps                  | 94          |
|    iterations           | 51          |
|    time_elapsed         | 1108        |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.069857344 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.515       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.273       |
|    n_updates            | 10260       |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.859       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 94         |
|    iterations           | 52         |
|    time_elapsed         | 1124       |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.04881757 |
|    clip_fraction        | 0.087      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0904    |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0711     |
|    n_updates            | 10270      |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 0.366      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.83        |
| time/                   |             |
|    fps                  | 95          |
|    iterations           | 53          |
|    time_elapsed         | 1140        |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.040137142 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.438       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0844      |
|    n_updates            | 10280       |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.474       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=110000, episode_reward=8.17 +/- 1.22
Episode length: 23.67 +/- 5.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.7        |
|    mean_reward          | 8.17        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.047810785 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0554      |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.565       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 95       |
|    iterations      | 54       |
|    time_elapsed    | 1155     |
|    total_timesteps | 110592   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 96          |
|    iterations           | 55          |
|    time_elapsed         | 1170        |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.054464906 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.114       |
|    n_updates            | 10300       |
|    policy_gradient_loss | -0.0219     |
|    value_loss           | 0.446       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.2       |
|    ep_rew_mean          | 7.23       |
| time/                   |            |
|    fps                  | 96         |
|    iterations           | 56         |
|    time_elapsed         | 1184       |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.18334061 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.131     |
|    explained_variance   | 0.566      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.136      |
|    n_updates            | 10310      |
|    policy_gradient_loss | -0.0292    |
|    value_loss           | 0.566      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.3       |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 97         |
|    iterations           | 57         |
|    time_elapsed         | 1198       |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.05312012 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.462      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.264      |
|    n_updates            | 10320      |
|    policy_gradient_loss | -0.0229    |
|    value_loss           | 0.721      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.1      |
|    ep_rew_mean          | 7.37      |
| time/                   |           |
|    fps                  | 97        |
|    iterations           | 58        |
|    time_elapsed         | 1214      |
|    total_timesteps      | 118784    |
| train/                  |           |
|    approx_kl            | 0.0365565 |
|    clip_fraction        | 0.126     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.16     |
|    explained_variance   | 0.646     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.229     |
|    n_updates            | 10330     |
|    policy_gradient_loss | -0.0201   |
|    value_loss           | 0.732     |
---------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=120000, episode_reward=1.93 +/- 8.44
Episode length: 35.33 +/- 10.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 35.3        |
|    mean_reward          | 1.93        |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.045205228 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.499       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.433       |
|    n_updates            | 10340       |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.963       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22       |
|    ep_rew_mean     | 7.45     |
| time/              |          |
|    fps             | 98       |
|    iterations      | 59       |
|    time_elapsed    | 1230     |
|    total_timesteps | 120832   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.2       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 60         |
|    time_elapsed         | 1248       |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.03891988 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 10350      |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.687      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23         |
|    ep_rew_mean          | 7.2        |
| time/                   |            |
|    fps                  | 98         |
|    iterations           | 61         |
|    time_elapsed         | 1265       |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.06733538 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.144      |
|    n_updates            | 10360      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.585      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22        |
|    ep_rew_mean          | 7.43      |
| time/                   |           |
|    fps                  | 99        |
|    iterations           | 62        |
|    time_elapsed         | 1280      |
|    total_timesteps      | 126976    |
| train/                  |           |
|    approx_kl            | 0.1442854 |
|    clip_fraction        | 0.163     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.206    |
|    explained_variance   | 0.454     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.168     |
|    n_updates            | 10370     |
|    policy_gradient_loss | -0.0273   |
|    value_loss           | 0.78      |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 7.06        |
| time/                   |             |
|    fps                  | 99          |
|    iterations           | 63          |
|    time_elapsed         | 1294        |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.059786655 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.223       |
|    n_updates            | 10380       |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.678       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=130000, episode_reward=7.67 +/- 1.90
Episode length: 18.33 +/- 2.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 7.67       |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.07362284 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.166     |
|    explained_variance   | 0.458      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.208      |
|    n_updates            | 10390      |
|    policy_gradient_loss | -0.0326    |
|    value_loss           | 0.803      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.4     |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 100      |
|    iterations      | 64       |
|    time_elapsed    | 1308     |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 100         |
|    iterations           | 65          |
|    time_elapsed         | 1321        |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.044606313 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.526       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.373       |
|    n_updates            | 10400       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.701       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 7.27        |
| time/                   |             |
|    fps                  | 101         |
|    iterations           | 66          |
|    time_elapsed         | 1335        |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.035005458 |
|    clip_fraction        | 0.0972      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.253       |
|    n_updates            | 10410       |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.404       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 101        |
|    iterations           | 67         |
|    time_elapsed         | 1349       |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.07006663 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.603      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.231      |
|    n_updates            | 10420      |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.666      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.39       |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 68         |
|    time_elapsed         | 1364       |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.08621044 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0455     |
|    n_updates            | 10430      |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 0.44       |
----------------------------------------
reached max steps=100
Eval num_timesteps=140000, episode_reward=2.72 +/- 9.03
Episode length: 28.67 +/- 15.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.7        |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.048031427 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.584       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0937      |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.528       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 7.48     |
| time/              |          |
|    fps             | 102      |
|    iterations      | 69       |
|    time_elapsed    | 1378     |
|    total_timesteps | 141312   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 102        |
|    iterations           | 70         |
|    time_elapsed         | 1392       |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.05674527 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.177      |
|    n_updates            | 10450      |
|    policy_gradient_loss | -0.0186    |
|    value_loss           | 0.503      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.1       |
|    ep_rew_mean          | 7.02       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 71         |
|    time_elapsed         | 1406       |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.07336503 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.229     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.223      |
|    n_updates            | 10460      |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.842      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 103        |
|    iterations           | 72         |
|    time_elapsed         | 1421       |
|    total_timesteps      | 147456     |
| train/                  |            |
|    approx_kl            | 0.07255543 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.198     |
|    explained_variance   | 0.534      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.303      |
|    n_updates            | 10470      |
|    policy_gradient_loss | -0.00883   |
|    value_loss           | 0.934      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.52       |
| time/                   |            |
|    fps                  | 104        |
|    iterations           | 73         |
|    time_elapsed         | 1435       |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.04674967 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0818     |
|    n_updates            | 10480      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.444      |
----------------------------------------
reached max steps=100
Eval num_timesteps=150000, episode_reward=3.31 +/- 5.25
Episode length: 29.00 +/- 14.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29         |
|    mean_reward          | 3.31       |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.05912835 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.103      |
|    n_updates            | 10490      |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.637      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 104      |
|    iterations      | 74       |
|    time_elapsed    | 1449     |
|    total_timesteps | 151552   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.28        |
| time/                   |             |
|    fps                  | 104         |
|    iterations           | 75          |
|    time_elapsed         | 1464        |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.036481455 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 10500       |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.604       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.9      |
|    ep_rew_mean          | 7.22      |
| time/                   |           |
|    fps                  | 105       |
|    iterations           | 76        |
|    time_elapsed         | 1478      |
|    total_timesteps      | 155648    |
| train/                  |           |
|    approx_kl            | 0.0716417 |
|    clip_fraction        | 0.144     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.169    |
|    explained_variance   | 0.508     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.137     |
|    n_updates            | 10510     |
|    policy_gradient_loss | -0.0296   |
|    value_loss           | 0.91      |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.9        |
|    ep_rew_mean          | 7.07        |
| time/                   |             |
|    fps                  | 105         |
|    iterations           | 77          |
|    time_elapsed         | 1492        |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.038251158 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.471       |
|    n_updates            | 10520       |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.788       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 6.88       |
| time/                   |            |
|    fps                  | 106        |
|    iterations           | 78         |
|    time_elapsed         | 1506       |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.05496774 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.218     |
|    explained_variance   | 0.503      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.317      |
|    n_updates            | 10530      |
|    policy_gradient_loss | -0.0205    |
|    value_loss           | 1.37       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=160000, episode_reward=-0.27 +/- 7.13
Episode length: 40.67 +/- 13.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40.7       |
|    mean_reward          | -0.265     |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.05176758 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.213      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.275      |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.0222    |
|    value_loss           | 0.818      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 6.99     |
| time/              |          |
|    fps             | 106      |
|    iterations      | 79       |
|    time_elapsed    | 1521     |
|    total_timesteps | 161792   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.4      |
|    ep_rew_mean          | 6.8       |
| time/                   |           |
|    fps                  | 106       |
|    iterations           | 80        |
|    time_elapsed         | 1536      |
|    total_timesteps      | 163840    |
| train/                  |           |
|    approx_kl            | 0.0701212 |
|    clip_fraction        | 0.137     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.19     |
|    explained_variance   | 0.485     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.281     |
|    n_updates            | 10550     |
|    policy_gradient_loss | -0.0261   |
|    value_loss           | 0.943     |
---------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 7.25        |
| time/                   |             |
|    fps                  | 106         |
|    iterations           | 81          |
|    time_elapsed         | 1551        |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.071376115 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.256       |
|    n_updates            | 10560       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.945       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 107         |
|    iterations           | 82          |
|    time_elapsed         | 1566        |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.040109243 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.43        |
|    n_updates            | 10570       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.797       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.51        |
| time/                   |             |
|    fps                  | 107         |
|    iterations           | 83          |
|    time_elapsed         | 1580        |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.037480507 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.65        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.168       |
|    n_updates            | 10580       |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.626       |
-----------------------------------------
Eval num_timesteps=170000, episode_reward=7.86 +/- 0.67
Episode length: 19.00 +/- 4.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 7.86        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.032644127 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.55        |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.872       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 7.09     |
| time/              |          |
|    fps             | 107      |
|    iterations      | 84       |
|    time_elapsed    | 1594     |
|    total_timesteps | 172032   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.7        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 85          |
|    time_elapsed         | 1608        |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.037511893 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.177       |
|    n_updates            | 10600       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.951       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.07       |
| time/                   |            |
|    fps                  | 108        |
|    iterations           | 86         |
|    time_elapsed         | 1621       |
|    total_timesteps      | 176128     |
| train/                  |            |
|    approx_kl            | 0.03913531 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.566      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.212      |
|    n_updates            | 10610      |
|    policy_gradient_loss | -0.0216    |
|    value_loss           | 0.726      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.24        |
| time/                   |             |
|    fps                  | 108         |
|    iterations           | 87          |
|    time_elapsed         | 1634        |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.047481827 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.306       |
|    n_updates            | 10620       |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.889       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=180000, episode_reward=7.63 +/- 0.63
Episode length: 17.00 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 7.63        |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.059103325 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.591       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.233       |
|    n_updates            | 10630       |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.86        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.2     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 109      |
|    iterations      | 88       |
|    time_elapsed    | 1647     |
|    total_timesteps | 180224   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | 7.17       |
| time/                   |            |
|    fps                  | 109        |
|    iterations           | 89         |
|    time_elapsed         | 1660       |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.03452442 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.182     |
|    explained_variance   | 0.587      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.332      |
|    n_updates            | 10640      |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 0.763      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.44        |
| time/                   |             |
|    fps                  | 109         |
|    iterations           | 90          |
|    time_elapsed         | 1675        |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.032582015 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.572       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 10650       |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.674       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 7.13       |
| time/                   |            |
|    fps                  | 110        |
|    iterations           | 91         |
|    time_elapsed         | 1691       |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.05763694 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.644      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 10660      |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.61       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23         |
|    ep_rew_mean          | 6.94       |
| time/                   |            |
|    fps                  | 110        |
|    iterations           | 92         |
|    time_elapsed         | 1707       |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.07839568 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.574      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.143      |
|    n_updates            | 10670      |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 0.716      |
----------------------------------------
Eval num_timesteps=190000, episode_reward=8.44 +/- 1.29
Episode length: 16.33 +/- 1.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 8.44       |
| time/                   |            |
|    total_timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.09307076 |
|    clip_fraction        | 0.201      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.188     |
|    explained_variance   | 0.106      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.388      |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.0401    |
|    value_loss           | 1.26       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.41     |
| time/              |          |
|    fps             | 110      |
|    iterations      | 93       |
|    time_elapsed    | 1722     |
|    total_timesteps | 190464   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.7         |
| time/                   |             |
|    fps                  | 110         |
|    iterations           | 94          |
|    time_elapsed         | 1736        |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.040863246 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.573       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 111         |
|    iterations           | 95          |
|    time_elapsed         | 1751        |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.050378926 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.154       |
|    n_updates            | 10700       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.437       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.1         |
| time/                   |             |
|    fps                  | 111         |
|    iterations           | 96          |
|    time_elapsed         | 1764        |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.040024333 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 10710       |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.571       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 111        |
|    iterations           | 97         |
|    time_elapsed         | 1777       |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.06750099 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.382      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.377      |
|    n_updates            | 10720      |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 1.12       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=200000, episode_reward=-1.19 +/- 4.29
Episode length: 40.33 +/- 13.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.3        |
|    mean_reward          | -1.19       |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.041581288 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.571       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 10730       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.515       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 112      |
|    iterations      | 98       |
|    time_elapsed    | 1791     |
|    total_timesteps | 200704   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.44       |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 99         |
|    time_elapsed         | 1805       |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.04451897 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.178     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.107      |
|    n_updates            | 10740      |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.6        |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 100        |
|    time_elapsed         | 1819       |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.08179406 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.527      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0525     |
|    n_updates            | 10750      |
|    policy_gradient_loss | -0.0222    |
|    value_loss           | 0.536      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.46       |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 101        |
|    time_elapsed         | 1835       |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.03296998 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.188     |
|    explained_variance   | 0.671      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.141      |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.0209    |
|    value_loss           | 0.643      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | 7.22       |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 102        |
|    time_elapsed         | 1854       |
|    total_timesteps      | 208896     |
| train/                  |            |
|    approx_kl            | 0.03271022 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.152     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.308      |
|    n_updates            | 10770      |
|    policy_gradient_loss | -0.00896   |
|    value_loss           | 0.584      |
----------------------------------------
reached max steps=100
Eval num_timesteps=210000, episode_reward=1.31 +/- 8.01
Episode length: 29.00 +/- 14.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 1.31        |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.060401414 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 10780       |
|    policy_gradient_loss | -0.0295     |
|    value_loss           | 0.804       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.13     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 103      |
|    time_elapsed    | 1869     |
|    total_timesteps | 210944   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.83        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 104         |
|    time_elapsed         | 1886        |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.025515217 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 10790       |
|    policy_gradient_loss | -0.0218     |
|    value_loss           | 0.629       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 105         |
|    time_elapsed         | 1901        |
|    total_timesteps      | 215040      |
| train/                  |             |
|    approx_kl            | 0.031646363 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.345       |
|    n_updates            | 10800       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.49        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 106         |
|    time_elapsed         | 1921        |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.034992676 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0827      |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.472       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.74        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 107         |
|    time_elapsed         | 1941        |
|    total_timesteps      | 219136      |
| train/                  |             |
|    approx_kl            | 0.039692562 |
|    clip_fraction        | 0.0991      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.129       |
|    n_updates            | 10820       |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.622       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=220000, episode_reward=-1.68 +/- 6.92
Episode length: 38.00 +/- 16.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38          |
|    mean_reward          | -1.68       |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.033185173 |
|    clip_fraction        | 0.0987      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0693      |
|    n_updates            | 10830       |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.445       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 108      |
|    time_elapsed    | 1963     |
|    total_timesteps | 221184   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.4      |
|    ep_rew_mean          | 7.8       |
| time/                   |           |
|    fps                  | 112       |
|    iterations           | 109       |
|    time_elapsed         | 1985      |
|    total_timesteps      | 223232    |
| train/                  |           |
|    approx_kl            | 0.0400651 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.206    |
|    explained_variance   | 0.677     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0916    |
|    n_updates            | 10840     |
|    policy_gradient_loss | -0.0217   |
|    value_loss           | 0.431     |
---------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.15       |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 110        |
|    time_elapsed         | 2005       |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.04676474 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.169      |
|    n_updates            | 10850      |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.416      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.66       |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 111        |
|    time_elapsed         | 2024       |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.03416965 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.172     |
|    explained_variance   | 0.639      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.634      |
|    n_updates            | 10860      |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 0.656      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 112         |
|    time_elapsed         | 2041        |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.038687073 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 10870       |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.448       |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=8.54 +/- 0.65
Episode length: 22.00 +/- 4.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 22        |
|    mean_reward          | 8.54      |
| time/                   |           |
|    total_timesteps      | 230000    |
| train/                  |           |
|    approx_kl            | 0.0334813 |
|    clip_fraction        | 0.0854    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.138    |
|    explained_variance   | 0.709     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.175     |
|    n_updates            | 10880     |
|    policy_gradient_loss | -0.0133   |
|    value_loss           | 0.378     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 113      |
|    time_elapsed    | 2058     |
|    total_timesteps | 231424   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.72        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 114         |
|    time_elapsed         | 2075        |
|    total_timesteps      | 233472      |
| train/                  |             |
|    approx_kl            | 0.026540417 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0623      |
|    n_updates            | 10890       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.315       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 115         |
|    time_elapsed         | 2092        |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.051633112 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 10900       |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.533       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.33        |
| time/                   |             |
|    fps                  | 112         |
|    iterations           | 116         |
|    time_elapsed         | 2108        |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.042598695 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 10910       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.374       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 117        |
|    time_elapsed         | 2124       |
|    total_timesteps      | 239616     |
| train/                  |            |
|    approx_kl            | 0.04772872 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.523      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.337      |
|    n_updates            | 10920      |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.652      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=240000, episode_reward=2.72 +/- 9.00
Episode length: 28.67 +/- 15.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.7        |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 240000      |
| train/                  |             |
|    approx_kl            | 0.046971828 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.237       |
|    n_updates            | 10930       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.631       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 112      |
|    iterations      | 118      |
|    time_elapsed    | 2141     |
|    total_timesteps | 241664   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 112        |
|    iterations           | 119        |
|    time_elapsed         | 2157       |
|    total_timesteps      | 243712     |
| train/                  |            |
|    approx_kl            | 0.07268132 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.553      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.103      |
|    n_updates            | 10940      |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.515      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 120        |
|    time_elapsed         | 2174       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.08619428 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0847     |
|    n_updates            | 10950      |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.537      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.7         |
| time/                   |             |
|    fps                  | 113         |
|    iterations           | 121         |
|    time_elapsed         | 2191        |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.057501182 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0786      |
|    n_updates            | 10960       |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.33        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 113        |
|    iterations           | 122        |
|    time_elapsed         | 2209       |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.06387065 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.715      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0882     |
|    n_updates            | 10970      |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 0.342      |
----------------------------------------
Eval num_timesteps=250000, episode_reward=8.39 +/- 0.74
Episode length: 22.67 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.7        |
|    mean_reward          | 8.39        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.049066357 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.398       |
-----------------------------------------
Traceback (most recent call last):
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 359, in <module>
    main()
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 315, in main
    model.learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 224, in collect_rollouts
    if not callback.on_step():
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 223, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\integration\sb3\sb3.py", line 136, in _on_step
    self.save_model()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\integration\sb3\sb3.py", line 145, in save_model
    wandb.save(self.path, base_path=self.model_save_path)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 392, in wrapper_fn
    return func(self, *args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 382, in wrapper
    return func(self, *args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 1963, in save
    return self._save(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 2022, in _save
    target_path.symlink_to(source_path)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\pathlib.py", line 1255, in symlink_to
    self._accessor.symlink(target, self, target_is_directory)
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\Users\\matan\\master_thesis\\minigrid_custom\\models\\wandb_models\\(2, 2, 2, -3, 0.2)\\model.zip' -> 'C:\\Users\\matan\\master_thesis\\minigrid_custom\\wandb\\run-20241230_133544-t8jb5830\\files\\model.zip'
