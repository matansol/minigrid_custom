Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Loaded model from models\orig_easy8_20241111\iter_1000000_steps. Continuing training.
Logging to ./logs/ppo/minigrid_custom_tensorboard/20241230_3
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x00000208F9420520> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x00000208F9169180>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.8     |
|    ep_rew_mean     | 7.16     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.7       |
|    ep_rew_mean          | 5.52       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 2          |
|    time_elapsed         | 15         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.14241144 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.228     |
|    explained_variance   | 0.0742     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.293      |
|    n_updates            | 9770       |
|    policy_gradient_loss | -0.00366   |
|    value_loss           | 1.16       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.9       |
|    ep_rew_mean          | 7.29       |
| time/                   |            |
|    fps                  | 268        |
|    iterations           | 3          |
|    time_elapsed         | 22         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.07168353 |
|    clip_fraction        | 0.218      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.344     |
|    explained_variance   | 0.483      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.625      |
|    n_updates            | 9780       |
|    policy_gradient_loss | -0.00854   |
|    value_loss           | 2.23       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 25.8      |
|    ep_rew_mean          | 6.2       |
| time/                   |           |
|    fps                  | 275       |
|    iterations           | 4         |
|    time_elapsed         | 29        |
|    total_timesteps      | 8192      |
| train/                  |           |
|    approx_kl            | 0.0654351 |
|    clip_fraction        | 0.142     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.213    |
|    explained_variance   | 0.409     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.229     |
|    n_updates            | 9790      |
|    policy_gradient_loss | 0.000895  |
|    value_loss           | 0.944     |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=10000, episode_reward=1.64 +/- 6.82
Episode length: 36.67 +/- 9.98
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 36.7        |
|    mean_reward          | 1.64        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.074564576 |
|    clip_fraction        | 0.205       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.283      |
|    explained_variance   | 0.473       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.308       |
|    n_updates            | 9800        |
|    policy_gradient_loss | -0.0275     |
|    value_loss           | 1.25        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 26.2     |
|    ep_rew_mean     | 5.94     |
| time/              |          |
|    fps             | 273      |
|    iterations      | 5        |
|    time_elapsed    | 37       |
|    total_timesteps | 10240    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.9        |
|    ep_rew_mean          | 6.89        |
| time/                   |             |
|    fps                  | 262         |
|    iterations           | 6           |
|    time_elapsed         | 46          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.034828536 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.56        |
|    n_updates            | 9810        |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 1.48        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.9       |
|    ep_rew_mean          | 7.14       |
| time/                   |            |
|    fps                  | 259        |
|    iterations           | 7          |
|    time_elapsed         | 55         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.07379967 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.201     |
|    explained_variance   | 0.522      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.375      |
|    n_updates            | 9820       |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 1.29       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.4        |
|    ep_rew_mean          | 6.95        |
| time/                   |             |
|    fps                  | 258         |
|    iterations           | 8           |
|    time_elapsed         | 63          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.048733104 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.275       |
|    n_updates            | 9830        |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.775       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23          |
|    ep_rew_mean          | 6.61        |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 9           |
|    time_elapsed         | 72          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.058868114 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.535       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.282       |
|    n_updates            | 9840        |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.768       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=20000, episode_reward=7.77 +/- 1.32
Episode length: 16.33 +/- 2.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 7.77        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.055519804 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.199      |
|    explained_variance   | 0.484       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.119       |
|    n_updates            | 9850        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.958       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 7.37     |
| time/              |          |
|    fps             | 255      |
|    iterations      | 10       |
|    time_elapsed    | 80       |
|    total_timesteps | 20480    |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.16       |
| time/                   |            |
|    fps                  | 257        |
|    iterations           | 11         |
|    time_elapsed         | 87         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.05402726 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.183     |
|    explained_variance   | 0.549      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.199      |
|    n_updates            | 9860       |
|    policy_gradient_loss | -0.0162    |
|    value_loss           | 0.841      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.14        |
| time/                   |             |
|    fps                  | 256         |
|    iterations           | 12          |
|    time_elapsed         | 95          |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.054965004 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.554       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.282       |
|    n_updates            | 9870        |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.723       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.55        |
| time/                   |             |
|    fps                  | 255         |
|    iterations           | 13          |
|    time_elapsed         | 104         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.066034876 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.314       |
|    n_updates            | 9880        |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.698       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.8        |
|    ep_rew_mean          | 7.06        |
| time/                   |             |
|    fps                  | 254         |
|    iterations           | 14          |
|    time_elapsed         | 112         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.090241164 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.59        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.48        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=30000, episode_reward=1.83 +/- 8.45
Episode length: 29.67 +/- 14.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 1.83        |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.084917836 |
|    clip_fraction        | 0.16        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.525       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.212       |
|    n_updates            | 9900        |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.964       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.5     |
|    ep_rew_mean     | 6.9      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 15       |
|    time_elapsed    | 124      |
|    total_timesteps | 30720    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.1        |
|    ep_rew_mean          | 7.31        |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 16          |
|    time_elapsed         | 136         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.033350393 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.203      |
|    explained_variance   | 0.587       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.382       |
|    n_updates            | 9910        |
|    policy_gradient_loss | -0.00917    |
|    value_loss           | 1.01        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.9       |
|    ep_rew_mean          | 7.24       |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 17         |
|    time_elapsed         | 145        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.05835857 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.493      |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 0.856      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 18         |
|    time_elapsed         | 158        |
|    total_timesteps      | 36864      |
| train/                  |            |
|    approx_kl            | 0.02450942 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.189     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.343      |
|    n_updates            | 9930       |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 1.02       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.8       |
|    ep_rew_mean          | 7.12       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 19         |
|    time_elapsed         | 166        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.07197697 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.274      |
|    n_updates            | 9940       |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.789      |
----------------------------------------
Eval num_timesteps=40000, episode_reward=8.60 +/- 0.12
Episode length: 18.67 +/- 3.77
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.7       |
|    mean_reward          | 8.6        |
| time/                   |            |
|    total_timesteps      | 40000      |
| train/                  |            |
|    approx_kl            | 0.05901436 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.385      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.391      |
|    n_updates            | 9950       |
|    policy_gradient_loss | -0.0272    |
|    value_loss           | 0.958      |
----------------------------------------
New best mean reward!
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 234      |
|    iterations      | 20       |
|    time_elapsed    | 175      |
|    total_timesteps | 40960    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.58        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 21          |
|    time_elapsed         | 183         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.036784805 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.312       |
|    n_updates            | 9960        |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.89        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.2       |
|    ep_rew_mean          | 6.98       |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 22         |
|    time_elapsed         | 190        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.03671515 |
|    clip_fraction        | 0.0996     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.176      |
|    n_updates            | 9970       |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 0.556      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.3        |
|    ep_rew_mean          | 6.55        |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 23          |
|    time_elapsed         | 198         |
|    total_timesteps      | 47104       |
| train/                  |             |
|    approx_kl            | 0.049637906 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.235       |
|    n_updates            | 9980        |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 1.02        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 7.1        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 24         |
|    time_elapsed         | 205        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.04097296 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.24      |
|    explained_variance   | 0.508      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.277      |
|    n_updates            | 9990       |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 1.37       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=50000, episode_reward=7.02 +/- 0.62
Episode length: 21.33 +/- 4.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.3        |
|    mean_reward          | 7.02        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.046025753 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.598       |
|    n_updates            | 10000       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.848       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 238      |
|    iterations      | 25       |
|    time_elapsed    | 214      |
|    total_timesteps | 51200    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.3        |
|    ep_rew_mean          | 7.13        |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 26          |
|    time_elapsed         | 223         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.052858077 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.215      |
|    explained_variance   | 0.581       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.349       |
|    n_updates            | 10010       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.809       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.7        |
|    ep_rew_mean          | 6.18        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 27          |
|    time_elapsed         | 236         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.042250723 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.209       |
|    n_updates            | 10020       |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.813       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.9        |
|    ep_rew_mean          | 7.16        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 28          |
|    time_elapsed         | 246         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.047102638 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.392       |
|    n_updates            | 10030       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 1.34        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.6       |
|    ep_rew_mean          | 6.96       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 29         |
|    time_elapsed         | 254        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.07775351 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.211     |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.324      |
|    n_updates            | 10040      |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 0.902      |
----------------------------------------
Eval num_timesteps=60000, episode_reward=7.23 +/- 1.26
Episode length: 28.00 +/- 4.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | 7.23        |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.030800521 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.631       |
|    n_updates            | 10050       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 1.13        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.1      |
| time/              |          |
|    fps             | 234      |
|    iterations      | 30       |
|    time_elapsed    | 261      |
|    total_timesteps | 61440    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.6       |
|    ep_rew_mean          | 7.07       |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 31         |
|    time_elapsed         | 269        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.03592882 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.175     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.396      |
|    n_updates            | 10060      |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 0.816      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.5        |
|    ep_rew_mean          | 6.72        |
| time/                   |             |
|    fps                  | 237         |
|    iterations           | 32          |
|    time_elapsed         | 276         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.050375223 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.378       |
|    n_updates            | 10070       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.779       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22         |
|    ep_rew_mean          | 6.91       |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 33         |
|    time_elapsed         | 283        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.06720275 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.207     |
|    explained_variance   | 0.55       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.341      |
|    n_updates            | 10080      |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 0.896      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.31       |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 34         |
|    time_elapsed         | 290        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.16031751 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.183     |
|    explained_variance   | 0.528      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.186      |
|    n_updates            | 10090      |
|    policy_gradient_loss | -0.0291    |
|    value_loss           | 0.725      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=70000, episode_reward=5.73 +/- 2.08
Episode length: 15.00 +/- 1.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15         |
|    mean_reward          | 5.73       |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.04804179 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.13      |
|    explained_variance   | 0.572      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.495      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 7.25     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 35       |
|    time_elapsed    | 297      |
|    total_timesteps | 71680    |
---------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22.4      |
|    ep_rew_mean          | 6.99      |
| time/                   |           |
|    fps                  | 241       |
|    iterations           | 36        |
|    time_elapsed         | 305       |
|    total_timesteps      | 73728     |
| train/                  |           |
|    approx_kl            | 0.0969563 |
|    clip_fraction        | 0.132     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.149    |
|    explained_variance   | 0.578     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.471     |
|    n_updates            | 10110     |
|    policy_gradient_loss | -0.017    |
|    value_loss           | 0.799     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.9       |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 37         |
|    time_elapsed         | 313        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.04517858 |
|    clip_fraction        | 0.0989     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.197      |
|    n_updates            | 10120      |
|    policy_gradient_loss | -0.0121    |
|    value_loss           | 0.674      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.3       |
|    ep_rew_mean          | 7.35       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 38         |
|    time_elapsed         | 320        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.05821018 |
|    clip_fraction        | 0.0996     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 10130      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.413      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.37        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 39          |
|    time_elapsed         | 328         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.069194406 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 10140       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.659       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=80000, episode_reward=1.17 +/- 7.90
Episode length: 29.67 +/- 14.52
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29.7       |
|    mean_reward          | 1.17       |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.05584601 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.464      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.174      |
|    n_updates            | 10150      |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 0.668      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 40       |
|    time_elapsed    | 336      |
|    total_timesteps | 81920    |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.2         |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 41          |
|    time_elapsed         | 345         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.060353063 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 10160       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.546       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.2       |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 42         |
|    time_elapsed         | 352        |
|    total_timesteps      | 86016      |
| train/                  |            |
|    approx_kl            | 0.05237682 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.19      |
|    explained_variance   | 0.63       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.219      |
|    n_updates            | 10170      |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.561      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 7.19        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 43          |
|    time_elapsed         | 362         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.058121003 |
|    clip_fraction        | 0.166       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.169      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0927      |
|    n_updates            | 10180       |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.407       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=90000, episode_reward=3.02 +/- 9.22
Episode length: 30.33 +/- 14.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 30.3     |
|    mean_reward          | 3.02     |
| time/                   |          |
|    total_timesteps      | 90000    |
| train/                  |          |
|    approx_kl            | 0.081223 |
|    clip_fraction        | 0.132    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.189   |
|    explained_variance   | 0.592    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.27     |
|    n_updates            | 10190    |
|    policy_gradient_loss | -0.0134  |
|    value_loss           | 0.792    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 7.02     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 44       |
|    time_elapsed    | 370      |
|    total_timesteps | 90112    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.19       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 45         |
|    time_elapsed         | 379        |
|    total_timesteps      | 92160      |
| train/                  |            |
|    approx_kl            | 0.08034055 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.185     |
|    explained_variance   | 0.62       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.345      |
|    n_updates            | 10200      |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.755      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.5       |
|    ep_rew_mean          | 6.99       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 46         |
|    time_elapsed         | 387        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.04282126 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.169     |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.14       |
|    n_updates            | 10210      |
|    policy_gradient_loss | 0.0377     |
|    value_loss           | 0.712      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.1       |
|    ep_rew_mean          | 6.97       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 47         |
|    time_elapsed         | 395        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.04607705 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.204     |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 10220      |
|    policy_gradient_loss | -0.00658   |
|    value_loss           | 0.905      |
----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.8      |
|    ep_rew_mean          | 7.49      |
| time/                   |           |
|    fps                  | 244       |
|    iterations           | 48        |
|    time_elapsed         | 402       |
|    total_timesteps      | 98304     |
| train/                  |           |
|    approx_kl            | 0.0755083 |
|    clip_fraction        | 0.133     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.181    |
|    explained_variance   | 0.538     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.338     |
|    n_updates            | 10230     |
|    policy_gradient_loss | 0.0043    |
|    value_loss           | 0.682     |
---------------------------------------
reached max steps=100
Eval num_timesteps=100000, episode_reward=3.83 +/- 6.97
Episode length: 29.67 +/- 14.43
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 3.83        |
| time/                   |             |
|    total_timesteps      | 100000      |
| train/                  |             |
|    approx_kl            | 0.048841488 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0897      |
|    n_updates            | 10240       |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.399       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 49       |
|    time_elapsed    | 410      |
|    total_timesteps | 100352   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.3        |
|    ep_rew_mean          | 7.34        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 50          |
|    time_elapsed         | 418         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.055484407 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.074       |
|    n_updates            | 10250       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.353       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.19       |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 51         |
|    time_elapsed         | 425        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.08828367 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.175     |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.198      |
|    n_updates            | 10260      |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.875      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 52         |
|    time_elapsed         | 433        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.06192638 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.176     |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 10270      |
|    policy_gradient_loss | -0.0222    |
|    value_loss           | 0.741      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.35        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 53          |
|    time_elapsed         | 441         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.045344815 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 10280       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.485       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=110000, episode_reward=6.73 +/- 2.73
Episode length: 15.00 +/- 2.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 6.73        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.053769067 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.16       |
|    explained_variance   | 0.593       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.703       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23       |
|    ep_rew_mean     | 6.56     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 54       |
|    time_elapsed    | 449      |
|    total_timesteps | 110592   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.4         |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 55          |
|    time_elapsed         | 457         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.051281482 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.208      |
|    explained_variance   | 0.563       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.34        |
|    n_updates            | 10300       |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 1.25        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.3       |
|    ep_rew_mean          | 6.65       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 56         |
|    time_elapsed         | 465        |
|    total_timesteps      | 114688     |
| train/                  |            |
|    approx_kl            | 0.04690971 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.356      |
|    n_updates            | 10310      |
|    policy_gradient_loss | -0.0058    |
|    value_loss           | 0.75       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 6.96       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 57         |
|    time_elapsed         | 472        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.09487429 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.212     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.241      |
|    n_updates            | 10320      |
|    policy_gradient_loss | -0.0234    |
|    value_loss           | 1.04       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 7.17        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 58          |
|    time_elapsed         | 480         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.048427742 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.283       |
|    n_updates            | 10330       |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.794       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=120000, episode_reward=4.34 +/- 4.53
Episode length: 27.33 +/- 16.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.3        |
|    mean_reward          | 4.34        |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.057143487 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.23        |
|    n_updates            | 10340       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.797       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 7.27     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 59       |
|    time_elapsed    | 488      |
|    total_timesteps | 120832   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 6.93        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 60          |
|    time_elapsed         | 496         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.033033315 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.17        |
|    n_updates            | 10350       |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.804       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.08       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 61         |
|    time_elapsed         | 504        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.05590468 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.2       |
|    explained_variance   | 0.484      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.389      |
|    n_updates            | 10360      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.99       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.14        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 62          |
|    time_elapsed         | 511         |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.036994394 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.177       |
|    n_updates            | 10370       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.628       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.2        |
|    ep_rew_mean          | 6.67        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 63          |
|    time_elapsed         | 519         |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.078106545 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.556       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.293       |
|    n_updates            | 10380       |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 0.673       |
-----------------------------------------
Eval num_timesteps=130000, episode_reward=7.55 +/- 0.90
Episode length: 17.33 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 7.55        |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.046511654 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.604       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.457       |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.936       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.13     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 64       |
|    time_elapsed    | 528      |
|    total_timesteps | 131072   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 65          |
|    time_elapsed         | 539         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.072348565 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.569       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 10400       |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.707       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.19        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 66          |
|    time_elapsed         | 550         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.038258046 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 10410       |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.744       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.2        |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 67         |
|    time_elapsed         | 558        |
|    total_timesteps      | 137216     |
| train/                  |            |
|    approx_kl            | 0.08072844 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.174     |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 10420      |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.628      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.42        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 68          |
|    time_elapsed         | 566         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.055189274 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.261       |
|    n_updates            | 10430       |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 1.05        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=140000, episode_reward=3.25 +/- 6.56
Episode length: 32.33 +/- 12.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 32.3      |
|    mean_reward          | 3.25      |
| time/                   |           |
|    total_timesteps      | 140000    |
| train/                  |           |
|    approx_kl            | 0.0436569 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.155    |
|    explained_variance   | 0.561     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.164     |
|    n_updates            | 10440     |
|    policy_gradient_loss | -0.0147   |
|    value_loss           | 0.527     |
---------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.16     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 69       |
|    time_elapsed    | 573      |
|    total_timesteps | 141312   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.02        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 70          |
|    time_elapsed         | 581         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.049855437 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 10450       |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.53        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 71         |
|    time_elapsed         | 589        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.05707863 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.188     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0685     |
|    n_updates            | 10460      |
|    policy_gradient_loss | -0.0221    |
|    value_loss           | 0.563      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.19        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 72          |
|    time_elapsed         | 596         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.041730933 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.517       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0898      |
|    n_updates            | 10470       |
|    policy_gradient_loss | -0.0288     |
|    value_loss           | 0.695       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.5        |
|    ep_rew_mean          | 7.53        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 73          |
|    time_elapsed         | 607         |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.032922804 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.57        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.257       |
|    n_updates            | 10480       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.771       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=150000, episode_reward=8.17 +/- 0.42
Episode length: 23.67 +/- 3.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23.7       |
|    mean_reward          | 8.17       |
| time/                   |            |
|    total_timesteps      | 150000     |
| train/                  |            |
|    approx_kl            | 0.05353471 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.341      |
|    n_updates            | 10490      |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.946      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 74       |
|    time_elapsed    | 616      |
|    total_timesteps | 151552   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.4       |
|    ep_rew_mean          | 6.68       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 75         |
|    time_elapsed         | 627        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.06856315 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.132      |
|    n_updates            | 10500      |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.374      |
----------------------------------------
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 21.1     |
|    ep_rew_mean          | 7.48     |
| time/                   |          |
|    fps                  | 243      |
|    iterations           | 76       |
|    time_elapsed         | 638      |
|    total_timesteps      | 155648   |
| train/                  |          |
|    approx_kl            | 1.053792 |
|    clip_fraction        | 0.279    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.151   |
|    explained_variance   | 0.45     |
|    learning_rate        | 0.0003   |
|    loss                 | 0.343    |
|    n_updates            | 10510    |
|    policy_gradient_loss | -0.0379  |
|    value_loss           | 1.4      |
--------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.6        |
|    ep_rew_mean          | 6.99        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 77          |
|    time_elapsed         | 645         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.046189874 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.54        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 10520       |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.631       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.3       |
|    ep_rew_mean          | 7.17       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 78         |
|    time_elapsed         | 655        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.15509118 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.392      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.237      |
|    n_updates            | 10530      |
|    policy_gradient_loss | -0.0304    |
|    value_loss           | 0.573      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=160000, episode_reward=-0.71 +/- 6.54
Episode length: 39.67 +/- 14.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39.7       |
|    mean_reward          | -0.714     |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.06010736 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.178     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.245      |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.0235    |
|    value_loss           | 0.653      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 7.32     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 79       |
|    time_elapsed    | 667      |
|    total_timesteps | 161792   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 80         |
|    time_elapsed         | 679        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.11143877 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.179     |
|    explained_variance   | 0.505      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.197      |
|    n_updates            | 10550      |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.794      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 81         |
|    time_elapsed         | 689        |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.03429877 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.128      |
|    n_updates            | 10560      |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 0.527      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 82          |
|    time_elapsed         | 700         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.050539233 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.167       |
|    n_updates            | 10570       |
|    policy_gradient_loss | -0.00871    |
|    value_loss           | 0.642       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.17        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 83          |
|    time_elapsed         | 710         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.060159158 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.154       |
|    n_updates            | 10580       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.439       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=170000, episode_reward=2.79 +/- 7.63
Episode length: 28.33 +/- 15.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.3       |
|    mean_reward          | 2.79       |
| time/                   |            |
|    total_timesteps      | 170000     |
| train/                  |            |
|    approx_kl            | 0.06512579 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.151      |
|    n_updates            | 10590      |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.729      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 238      |
|    iterations      | 84       |
|    time_elapsed    | 720      |
|    total_timesteps | 172032   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.04       |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 85         |
|    time_elapsed         | 732        |
|    total_timesteps      | 174080     |
| train/                  |            |
|    approx_kl            | 0.04679238 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.594      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.129      |
|    n_updates            | 10600      |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 0.632      |
----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.6      |
|    ep_rew_mean          | 7.91      |
| time/                   |           |
|    fps                  | 237       |
|    iterations           | 86        |
|    time_elapsed         | 742       |
|    total_timesteps      | 176128    |
| train/                  |           |
|    approx_kl            | 0.1456173 |
|    clip_fraction        | 0.167     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.178    |
|    explained_variance   | 0.303     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.234     |
|    n_updates            | 10610     |
|    policy_gradient_loss | -0.0297   |
|    value_loss           | 0.703     |
---------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.7      |
|    ep_rew_mean          | 7.88      |
| time/                   |           |
|    fps                  | 236       |
|    iterations           | 87        |
|    time_elapsed         | 751       |
|    total_timesteps      | 178176    |
| train/                  |           |
|    approx_kl            | 0.0404812 |
|    clip_fraction        | 0.0939    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.107    |
|    explained_variance   | 0.723     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.111     |
|    n_updates            | 10620     |
|    policy_gradient_loss | -0.0151   |
|    value_loss           | 0.326     |
---------------------------------------
Eval num_timesteps=180000, episode_reward=6.58 +/- 0.10
Episode length: 15.67 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 6.58       |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.05832515 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.122     |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.185      |
|    n_updates            | 10630      |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.481      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.82     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 88       |
|    time_elapsed    | 760      |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 89          |
|    time_elapsed         | 771         |
|    total_timesteps      | 182272      |
| train/                  |             |
|    approx_kl            | 0.038792424 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.088       |
|    n_updates            | 10640       |
|    policy_gradient_loss | -0.00902    |
|    value_loss           | 0.465       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 90          |
|    time_elapsed         | 779         |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.040622547 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 10650       |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.328       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 91         |
|    time_elapsed         | 788        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.09016214 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.148      |
|    n_updates            | 10660      |
|    policy_gradient_loss | -0.0212    |
|    value_loss           | 0.5        |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 7.73        |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 92          |
|    time_elapsed         | 797         |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.039418727 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.201       |
|    n_updates            | 10670       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.477       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=190000, episode_reward=8.54 +/- 1.57
Episode length: 22.00 +/- 2.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22          |
|    mean_reward          | 8.54        |
| time/                   |             |
|    total_timesteps      | 190000      |
| train/                  |             |
|    approx_kl            | 0.037385218 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0339      |
|    n_updates            | 10680       |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.408       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 93       |
|    time_elapsed    | 806      |
|    total_timesteps | 190464   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 94          |
|    time_elapsed         | 817         |
|    total_timesteps      | 192512      |
| train/                  |             |
|    approx_kl            | 0.034769822 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0838      |
|    n_updates            | 10690       |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.499       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 95          |
|    time_elapsed         | 826         |
|    total_timesteps      | 194560      |
| train/                  |             |
|    approx_kl            | 0.057254463 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.426       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0365      |
|    n_updates            | 10700       |
|    policy_gradient_loss | -0.0334     |
|    value_loss           | 0.388       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.65       |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 96         |
|    time_elapsed         | 834        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.05905898 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0358     |
|    n_updates            | 10710      |
|    policy_gradient_loss | -0.0199    |
|    value_loss           | 0.546      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 97         |
|    time_elapsed         | 842        |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.03733804 |
|    clip_fraction        | 0.0897     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.217      |
|    n_updates            | 10720      |
|    policy_gradient_loss | 0.00011    |
|    value_loss           | 0.721      |
----------------------------------------
reached max steps=100
Eval num_timesteps=200000, episode_reward=9.35 +/- 0.91
Episode length: 21.33 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.3        |
|    mean_reward          | 9.35        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.045173828 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 10730       |
|    policy_gradient_loss | -0.00848    |
|    value_loss           | 0.503       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.35     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 98       |
|    time_elapsed    | 851      |
|    total_timesteps | 200704   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.24       |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 99         |
|    time_elapsed         | 859        |
|    total_timesteps      | 202752     |
| train/                  |            |
|    approx_kl            | 0.07540385 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.248      |
|    n_updates            | 10740      |
|    policy_gradient_loss | -0.00844   |
|    value_loss           | 0.719      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 100         |
|    time_elapsed         | 872         |
|    total_timesteps      | 204800      |
| train/                  |             |
|    approx_kl            | 0.043782588 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0.611       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.308       |
|    n_updates            | 10750       |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.982       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.12       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 101        |
|    time_elapsed         | 881        |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.05018755 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0959     |
|    n_updates            | 10760      |
|    policy_gradient_loss | 0.00199    |
|    value_loss           | 0.611      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 7.2         |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 102         |
|    time_elapsed         | 891         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.045232214 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.514       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.636       |
|    n_updates            | 10770       |
|    policy_gradient_loss | -0.021      |
|    value_loss           | 0.985       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=210000, episode_reward=4.64 +/- 7.53
Episode length: 29.00 +/- 14.85
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29         |
|    mean_reward          | 4.64       |
| time/                   |            |
|    total_timesteps      | 210000     |
| train/                  |            |
|    approx_kl            | 0.05076277 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.482      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.403      |
|    n_updates            | 10780      |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 0.813      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 233      |
|    iterations      | 103      |
|    time_elapsed    | 904      |
|    total_timesteps | 210944   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 104        |
|    time_elapsed         | 915        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.08546391 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.517      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.182      |
|    n_updates            | 10790      |
|    policy_gradient_loss | -0.023     |
|    value_loss           | 0.557      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.48       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 105        |
|    time_elapsed         | 924        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.09688235 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.235      |
|    n_updates            | 10800      |
|    policy_gradient_loss | -0.0211    |
|    value_loss           | 0.527      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.34        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 106         |
|    time_elapsed         | 932         |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.061530624 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.157      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.552       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.8      |
|    ep_rew_mean          | 7.14      |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 107       |
|    time_elapsed         | 943       |
|    total_timesteps      | 219136    |
| train/                  |           |
|    approx_kl            | 0.0375886 |
|    clip_fraction        | 0.115     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.188    |
|    explained_variance   | 0.672     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.389     |
|    n_updates            | 10820     |
|    policy_gradient_loss | -0.0183   |
|    value_loss           | 0.815     |
---------------------------------------
reached max steps=100
Eval num_timesteps=220000, episode_reward=8.90 +/- 0.76
Episode length: 20.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.3        |
|    mean_reward          | 8.9         |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.038851507 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.598       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.329       |
|    n_updates            | 10830       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.758       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.25     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 108      |
|    time_elapsed    | 952      |
|    total_timesteps | 221184   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 109         |
|    time_elapsed         | 960         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.037825424 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.181       |
|    n_updates            | 10840       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.773       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 110        |
|    time_elapsed         | 967        |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.03985174 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.211      |
|    n_updates            | 10850      |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.415      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 111         |
|    time_elapsed         | 977         |
|    total_timesteps      | 227328      |
| train/                  |             |
|    approx_kl            | 0.064410105 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.557       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.231       |
|    n_updates            | 10860       |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.753       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.7       |
|    ep_rew_mean          | 7.08       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 112        |
|    time_elapsed         | 984        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.06828588 |
|    clip_fraction        | 0.0951     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0986    |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.16       |
|    n_updates            | 10870      |
|    policy_gradient_loss | -0.0101    |
|    value_loss           | 0.488      |
----------------------------------------
reached max steps=100
Eval num_timesteps=230000, episode_reward=8.22 +/- 0.27
Episode length: 17.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 8.22       |
| time/                   |            |
|    total_timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.23028216 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.352      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.115      |
|    n_updates            | 10880      |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.708      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 23.6     |
|    ep_rew_mean     | 6.86     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 113      |
|    time_elapsed    | 992      |
|    total_timesteps | 231424   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.2       |
|    ep_rew_mean          | 6.89       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 114        |
|    time_elapsed         | 1000       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.09363222 |
|    clip_fraction        | 0.19       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.21      |
|    explained_variance   | 0.521      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.357      |
|    n_updates            | 10890      |
|    policy_gradient_loss | -0.0256    |
|    value_loss           | 1.17       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 6.73        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 115         |
|    time_elapsed         | 1010        |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.061617088 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.368       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.48        |
|    n_updates            | 10900       |
|    policy_gradient_loss | 0.0446      |
|    value_loss           | 1.04        |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 22        |
|    ep_rew_mean          | 6.96      |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 116       |
|    time_elapsed         | 1019      |
|    total_timesteps      | 237568    |
| train/                  |           |
|    approx_kl            | 0.1015282 |
|    clip_fraction        | 0.172     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.203    |
|    explained_variance   | 0.352     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.133     |
|    n_updates            | 10910     |
|    policy_gradient_loss | -0.0212   |
|    value_loss           | 1.03      |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.1        |
|    ep_rew_mean          | 7.29        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 117         |
|    time_elapsed         | 1031        |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.055523537 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.181      |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 10920       |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.8         |
-----------------------------------------
Eval num_timesteps=240000, episode_reward=6.66 +/- 1.28
Episode length: 15.33 +/- 1.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.3       |
|    mean_reward          | 6.66       |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.06334477 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.152     |
|    explained_variance   | 0.617      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.237      |
|    n_updates            | 10930      |
|    policy_gradient_loss | -0.0213    |
|    value_loss           | 0.692      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.33     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 118      |
|    time_elapsed    | 1042     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 119         |
|    time_elapsed         | 1050        |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.049762648 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.251       |
|    n_updates            | 10940       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.602       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 120         |
|    time_elapsed         | 1058        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.047662616 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 10950       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.421       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.4      |
|    ep_rew_mean          | 7.23      |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 121       |
|    time_elapsed         | 1067      |
|    total_timesteps      | 247808    |
| train/                  |           |
|    approx_kl            | 0.0654455 |
|    clip_fraction        | 0.146     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.14     |
|    explained_variance   | 0.716     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0997    |
|    n_updates            | 10960     |
|    policy_gradient_loss | -0.0196   |
|    value_loss           | 0.36      |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 122        |
|    time_elapsed         | 1078       |
|    total_timesteps      | 249856     |
| train/                  |            |
|    approx_kl            | 0.10354508 |
|    clip_fraction        | 0.186      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.641      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.156      |
|    n_updates            | 10970      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.631      |
----------------------------------------
Eval num_timesteps=250000, episode_reward=8.22 +/- 0.10
Episode length: 17.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 8.22       |
| time/                   |            |
|    total_timesteps      | 250000     |
| train/                  |            |
|    approx_kl            | 0.06067265 |
|    clip_fraction        | 0.119      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.138      |
|    n_updates            | 10980      |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.435      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 123      |
|    time_elapsed    | 1088     |
|    total_timesteps | 251904   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 124         |
|    time_elapsed         | 1100        |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.042342447 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 10990       |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.506       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.55        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 125         |
|    time_elapsed         | 1110        |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.032012187 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 11000       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.538       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 126        |
|    time_elapsed         | 1121       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.03093166 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.668      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.166      |
|    n_updates            | 11010      |
|    policy_gradient_loss | -0.0187    |
|    value_loss           | 0.47       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=260000, episode_reward=2.72 +/- 9.03
Episode length: 28.67 +/- 15.08
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 28.7      |
|    mean_reward          | 2.72      |
| time/                   |           |
|    total_timesteps      | 260000    |
| train/                  |           |
|    approx_kl            | 0.0548506 |
|    clip_fraction        | 0.124     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.182    |
|    explained_variance   | 0.645     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0873    |
|    n_updates            | 11020     |
|    policy_gradient_loss | -0.0179   |
|    value_loss           | 0.556     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.1     |
|    ep_rew_mean     | 6.81     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 127      |
|    time_elapsed    | 1130     |
|    total_timesteps | 260096   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.11        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 128         |
|    time_elapsed         | 1140        |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.061913855 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.488       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 11030       |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 1.09        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.24       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 129        |
|    time_elapsed         | 1147       |
|    total_timesteps      | 264192     |
| train/                  |            |
|    approx_kl            | 0.09219834 |
|    clip_fraction        | 0.185      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.202     |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.384      |
|    n_updates            | 11040      |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.914      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.22       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 130        |
|    time_elapsed         | 1155       |
|    total_timesteps      | 266240     |
| train/                  |            |
|    approx_kl            | 0.04382832 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.173     |
|    explained_variance   | 0.619      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.176      |
|    n_updates            | 11050      |
|    policy_gradient_loss | -0.00936   |
|    value_loss           | 0.938      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22         |
|    ep_rew_mean          | 7.01       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 131        |
|    time_elapsed         | 1163       |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.05493279 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.178     |
|    explained_variance   | 0.581      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.147      |
|    n_updates            | 11060      |
|    policy_gradient_loss | -0.0232    |
|    value_loss           | 0.528      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=270000, episode_reward=4.64 +/- 4.71
Episode length: 29.00 +/- 14.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 4.64        |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.047747057 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.595       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 11070       |
|    policy_gradient_loss | -0.0262     |
|    value_loss           | 0.81        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 230      |
|    iterations      | 132      |
|    time_elapsed    | 1171     |
|    total_timesteps | 270336   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 133         |
|    time_elapsed         | 1178        |
|    total_timesteps      | 272384      |
| train/                  |             |
|    approx_kl            | 0.048199393 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.16       |
|    explained_variance   | 0.607       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.247       |
|    n_updates            | 11080       |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.633       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 6.86       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 134        |
|    time_elapsed         | 1187       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.03784185 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.65       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.131      |
|    n_updates            | 11090      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.534      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.02       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 135        |
|    time_elapsed         | 1194       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.05474084 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.184     |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.315      |
|    n_updates            | 11100      |
|    policy_gradient_loss | -0.024     |
|    value_loss           | 0.543      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.42        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 136         |
|    time_elapsed         | 1203        |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.038701482 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.251       |
|    n_updates            | 11110       |
|    policy_gradient_loss | -0.0296     |
|    value_loss           | 0.545       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=280000, episode_reward=8.16 +/- 0.10
Episode length: 20.67 +/- 4.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20.7       |
|    mean_reward          | 8.16       |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.04747293 |
|    clip_fraction        | 0.0955     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.194      |
|    n_updates            | 11120      |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.494      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 7.44     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 137      |
|    time_elapsed    | 1212     |
|    total_timesteps | 280576   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 138        |
|    time_elapsed         | 1219       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.07388735 |
|    clip_fraction        | 0.117      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.144      |
|    n_updates            | 11130      |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 0.525      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 139        |
|    time_elapsed         | 1227       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.06726863 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.18      |
|    explained_variance   | 0.652      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.199      |
|    n_updates            | 11140      |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 0.768      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.4      |
|    ep_rew_mean          | 7.42      |
| time/                   |           |
|    fps                  | 232       |
|    iterations           | 140       |
|    time_elapsed         | 1235      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.0404614 |
|    clip_fraction        | 0.118     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.154    |
|    explained_variance   | 0.68      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0676    |
|    n_updates            | 11150     |
|    policy_gradient_loss | -0.0154   |
|    value_loss           | 0.396     |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.45       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 141        |
|    time_elapsed         | 1242       |
|    total_timesteps      | 288768     |
| train/                  |            |
|    approx_kl            | 0.04621032 |
|    clip_fraction        | 0.129      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.198     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.1        |
|    n_updates            | 11160      |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.66       |
----------------------------------------
reached max steps=100
Eval num_timesteps=290000, episode_reward=8.00 +/- 0.41
Episode length: 18.33 +/- 1.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 8          |
| time/                   |            |
|    total_timesteps      | 290000     |
| train/                  |            |
|    approx_kl            | 0.03755484 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.176     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.137      |
|    n_updates            | 11170      |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.54       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.47     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 142      |
|    time_elapsed    | 1251     |
|    total_timesteps | 290816   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 143         |
|    time_elapsed         | 1258        |
|    total_timesteps      | 292864      |
| train/                  |             |
|    approx_kl            | 0.041192047 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.677       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.205       |
|    n_updates            | 11180       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.67        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.5         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 144         |
|    time_elapsed         | 1265        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.038929775 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.367       |
|    n_updates            | 11190       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.538       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.73        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 145         |
|    time_elapsed         | 1273        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.048266962 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.164      |
|    explained_variance   | 0.522       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0965      |
|    n_updates            | 11200       |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.53        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.49        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 146         |
|    time_elapsed         | 1282        |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.039176024 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.32        |
|    n_updates            | 11210       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.616       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=300000, episode_reward=8.60 +/- 0.90
Episode length: 18.67 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.7        |
|    mean_reward          | 8.6         |
| time/                   |             |
|    total_timesteps      | 300000      |
| train/                  |             |
|    approx_kl            | 0.035464473 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.189       |
|    n_updates            | 11220       |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.521       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.13     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 147      |
|    time_elapsed    | 1292     |
|    total_timesteps | 301056   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.2       |
|    ep_rew_mean          | 6.8        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 148        |
|    time_elapsed         | 1307       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.10545251 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.202     |
|    explained_variance   | 0.583      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.203      |
|    n_updates            | 11230      |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.831      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.51        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 149         |
|    time_elapsed         | 1319        |
|    total_timesteps      | 305152      |
| train/                  |             |
|    approx_kl            | 0.037075564 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.232      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.367       |
|    n_updates            | 11240       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.958       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.44        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 150         |
|    time_elapsed         | 1333        |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.048172217 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 11250       |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.692       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.01       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 151        |
|    time_elapsed         | 1341       |
|    total_timesteps      | 309248     |
| train/                  |            |
|    approx_kl            | 0.05825658 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.213     |
|    explained_variance   | 0.616      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.189      |
|    n_updates            | 11260      |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.624      |
----------------------------------------
Eval num_timesteps=310000, episode_reward=9.55 +/- 0.74
Episode length: 17.33 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 9.55       |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.04727906 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.215     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.411      |
|    n_updates            | 11270      |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.768      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.27     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 152      |
|    time_elapsed    | 1348     |
|    total_timesteps | 311296   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.23        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 153         |
|    time_elapsed         | 1356        |
|    total_timesteps      | 313344      |
| train/                  |             |
|    approx_kl            | 0.046408415 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.267       |
|    n_updates            | 11280       |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 0.609       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.64        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 154         |
|    time_elapsed         | 1363        |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.024598751 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.222       |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.0281     |
|    value_loss           | 0.629       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 155        |
|    time_elapsed         | 1370       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.03714542 |
|    clip_fraction        | 0.0946     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.693      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.115      |
|    n_updates            | 11300      |
|    policy_gradient_loss | -0.0186    |
|    value_loss           | 0.372      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 156         |
|    time_elapsed         | 1378        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.052269094 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0418      |
|    n_updates            | 11310       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.568       |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=8.52 +/- 0.79
Episode length: 19.00 +/- 0.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19         |
|    mean_reward          | 8.52       |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.07342376 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.197     |
|    explained_variance   | 0.455      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.24       |
|    n_updates            | 11320      |
|    policy_gradient_loss | -0.0285    |
|    value_loss           | 0.654      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.23     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 157      |
|    time_elapsed    | 1385     |
|    total_timesteps | 321536   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 158         |
|    time_elapsed         | 1392        |
|    total_timesteps      | 323584      |
| train/                  |             |
|    approx_kl            | 0.090591185 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.47        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 11330       |
|    policy_gradient_loss | -0.0255     |
|    value_loss           | 0.668       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 159         |
|    time_elapsed         | 1400        |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.031290498 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.201       |
|    n_updates            | 11340       |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.411       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.54        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 160         |
|    time_elapsed         | 1407        |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.054155804 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 11350       |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.389       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.57        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 161         |
|    time_elapsed         | 1414        |
|    total_timesteps      | 329728      |
| train/                  |             |
|    approx_kl            | 0.049358003 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.105       |
|    n_updates            | 11360       |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.349       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=330000, episode_reward=2.72 +/- 9.03
Episode length: 28.67 +/- 15.08
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.7       |
|    mean_reward          | 2.72       |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.03748793 |
|    clip_fraction        | 0.0982     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0958     |
|    n_updates            | 11370      |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.377      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.85     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 162      |
|    time_elapsed    | 1421     |
|    total_timesteps | 331776   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.3      |
|    ep_rew_mean          | 7.29      |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 163       |
|    time_elapsed         | 1431      |
|    total_timesteps      | 333824    |
| train/                  |           |
|    approx_kl            | 0.0354855 |
|    clip_fraction        | 0.106     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.15     |
|    explained_variance   | 0.652     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.175     |
|    n_updates            | 11380     |
|    policy_gradient_loss | -0.0191   |
|    value_loss           | 0.514     |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.69       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 164        |
|    time_elapsed         | 1442       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.03376646 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.24       |
|    n_updates            | 11390      |
|    policy_gradient_loss | -0.0243    |
|    value_loss           | 0.474      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.81        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 165         |
|    time_elapsed         | 1449        |
|    total_timesteps      | 337920      |
| train/                  |             |
|    approx_kl            | 0.049166802 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.29        |
|    n_updates            | 11400       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.547       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 166         |
|    time_elapsed         | 1457        |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.018985212 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.137       |
|    n_updates            | 11410       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.448       |
-----------------------------------------
Eval num_timesteps=340000, episode_reward=6.63 +/- 1.98
Episode length: 17.00 +/- 3.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 6.63        |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.035091184 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0989      |
|    n_updates            | 11420       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.348       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 167      |
|    time_elapsed    | 1467     |
|    total_timesteps | 342016   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.59        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 168         |
|    time_elapsed         | 1475        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.042343922 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.448       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.221       |
|    n_updates            | 11430       |
|    policy_gradient_loss | -0.0281     |
|    value_loss           | 0.424       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.59        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 169         |
|    time_elapsed         | 1483        |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.037670046 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 11440       |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.516       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 21.2     |
|    ep_rew_mean          | 7.2      |
| time/                   |          |
|    fps                  | 233      |
|    iterations           | 170      |
|    time_elapsed         | 1491     |
|    total_timesteps      | 348160   |
| train/                  |          |
|    approx_kl            | 0.039227 |
|    clip_fraction        | 0.1      |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.147   |
|    explained_variance   | 0.699    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.0435   |
|    n_updates            | 11450    |
|    policy_gradient_loss | -0.0157  |
|    value_loss           | 0.371    |
--------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=350000, episode_reward=7.64 +/- 1.14
Episode length: 20.00 +/- 2.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 20        |
|    mean_reward          | 7.64      |
| time/                   |           |
|    total_timesteps      | 350000    |
| train/                  |           |
|    approx_kl            | 0.0893911 |
|    clip_fraction        | 0.157     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.194    |
|    explained_variance   | 0.374     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.187     |
|    n_updates            | 11460     |
|    policy_gradient_loss | -0.0262   |
|    value_loss           | 0.711     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.34     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 171      |
|    time_elapsed    | 1499     |
|    total_timesteps | 350208   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 172         |
|    time_elapsed         | 1507        |
|    total_timesteps      | 352256      |
| train/                  |             |
|    approx_kl            | 0.042042445 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.487       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.243       |
|    n_updates            | 11470       |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.784       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.8      |
|    ep_rew_mean          | 7.49      |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 173       |
|    time_elapsed         | 1516      |
|    total_timesteps      | 354304    |
| train/                  |           |
|    approx_kl            | 0.0462178 |
|    clip_fraction        | 0.111     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.157    |
|    explained_variance   | 0.651     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.095     |
|    n_updates            | 11480     |
|    policy_gradient_loss | -0.00805  |
|    value_loss           | 0.336     |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 174        |
|    time_elapsed         | 1527       |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.07235109 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.181     |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.163      |
|    n_updates            | 11490      |
|    policy_gradient_loss | -0.0238    |
|    value_loss           | 0.493      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.48        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 175         |
|    time_elapsed         | 1534        |
|    total_timesteps      | 358400      |
| train/                  |             |
|    approx_kl            | 0.055563834 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.653       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.275       |
|    n_updates            | 11500       |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.52        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=360000, episode_reward=8.32 +/- 1.12
Episode length: 23.00 +/- 5.66
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23         |
|    mean_reward          | 8.32       |
| time/                   |            |
|    total_timesteps      | 360000     |
| train/                  |            |
|    approx_kl            | 0.04306875 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.207     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.174      |
|    n_updates            | 11510      |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.813      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 176      |
|    time_elapsed    | 1543     |
|    total_timesteps | 360448   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 177         |
|    time_elapsed         | 1553        |
|    total_timesteps      | 362496      |
| train/                  |             |
|    approx_kl            | 0.028556421 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 11520       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.728       |
-----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 7.81      |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 178       |
|    time_elapsed         | 1561      |
|    total_timesteps      | 364544    |
| train/                  |           |
|    approx_kl            | 0.0627574 |
|    clip_fraction        | 0.0949    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.121    |
|    explained_variance   | 0.737     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.102     |
|    n_updates            | 11530     |
|    policy_gradient_loss | -0.0189   |
|    value_loss           | 0.333     |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 179        |
|    time_elapsed         | 1570       |
|    total_timesteps      | 366592     |
| train/                  |            |
|    approx_kl            | 0.05709576 |
|    clip_fraction        | 0.0931     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.254      |
|    n_updates            | 11540      |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.427      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 180         |
|    time_elapsed         | 1579        |
|    total_timesteps      | 368640      |
| train/                  |             |
|    approx_kl            | 0.082738206 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.384       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.28        |
|    n_updates            | 11550       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.985       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=370000, episode_reward=5.38 +/- 5.24
Episode length: 28.67 +/- 15.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 28.7      |
|    mean_reward          | 5.38      |
| time/                   |           |
|    total_timesteps      | 370000    |
| train/                  |           |
|    approx_kl            | 0.1045623 |
|    clip_fraction        | 0.1       |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.105    |
|    explained_variance   | 0.414     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0635    |
|    n_updates            | 11560     |
|    policy_gradient_loss | -0.0197   |
|    value_loss           | 0.39      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 181      |
|    time_elapsed    | 1587     |
|    total_timesteps | 370688   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 182         |
|    time_elapsed         | 1597        |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.049168155 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.642       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 11570       |
|    policy_gradient_loss | -0.000821   |
|    value_loss           | 0.488       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 8.12        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 183         |
|    time_elapsed         | 1607        |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.046074882 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0714      |
|    n_updates            | 11580       |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.367       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 184         |
|    time_elapsed         | 1616        |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.046343584 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0765      |
|    n_updates            | 11590       |
|    policy_gradient_loss | -0.00696    |
|    value_loss           | 0.359       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 185        |
|    time_elapsed         | 1624       |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.04453598 |
|    clip_fraction        | 0.0765     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.122     |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.12       |
|    n_updates            | 11600      |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 0.323      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=380000, episode_reward=1.85 +/- 8.38
Episode length: 32.67 +/- 12.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 32.7       |
|    mean_reward          | 1.85       |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.10677506 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.8        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0549     |
|    n_updates            | 11610      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.317      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 186      |
|    time_elapsed    | 1632     |
|    total_timesteps | 380928   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.06        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 187         |
|    time_elapsed         | 1640        |
|    total_timesteps      | 382976      |
| train/                  |             |
|    approx_kl            | 0.079168424 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.321       |
|    n_updates            | 11620       |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.556       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.27        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 188         |
|    time_elapsed         | 1647        |
|    total_timesteps      | 385024      |
| train/                  |             |
|    approx_kl            | 0.043360133 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0507      |
|    n_updates            | 11630       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.27        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.91       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 189        |
|    time_elapsed         | 1656       |
|    total_timesteps      | 387072     |
| train/                  |            |
|    approx_kl            | 0.08729854 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.184     |
|    explained_variance   | 0.539      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0848     |
|    n_updates            | 11640      |
|    policy_gradient_loss | -0.0257    |
|    value_loss           | 0.578      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.3       |
|    ep_rew_mean          | 7.14       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 190        |
|    time_elapsed         | 1665       |
|    total_timesteps      | 389120     |
| train/                  |            |
|    approx_kl            | 0.04469307 |
|    clip_fraction        | 0.0932     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.13       |
|    n_updates            | 11650      |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.34       |
----------------------------------------
Eval num_timesteps=390000, episode_reward=8.37 +/- 1.28
Episode length: 16.67 +/- 1.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | 8.37       |
| time/                   |            |
|    total_timesteps      | 390000     |
| train/                  |            |
|    approx_kl            | 0.11454378 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0649     |
|    n_updates            | 11660      |
|    policy_gradient_loss | -0.0242    |
|    value_loss           | 0.497      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.31     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 191      |
|    time_elapsed    | 1675     |
|    total_timesteps | 391168   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.47       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 192        |
|    time_elapsed         | 1684       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.08113503 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.279      |
|    n_updates            | 11670      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.657      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 193        |
|    time_elapsed         | 1694       |
|    total_timesteps      | 395264     |
| train/                  |            |
|    approx_kl            | 0.04886636 |
|    clip_fraction        | 0.0983     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.147     |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.269      |
|    n_updates            | 11680      |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.578      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.52        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 194         |
|    time_elapsed         | 1703        |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.053355083 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.169       |
|    n_updates            | 11690       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.464       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 195        |
|    time_elapsed         | 1711       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.05127827 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.225      |
|    n_updates            | 11700      |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 0.408      |
----------------------------------------
Eval num_timesteps=400000, episode_reward=8.60 +/- 0.59
Episode length: 18.67 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.7        |
|    mean_reward          | 8.6         |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.055723093 |
|    clip_fraction        | 0.0597      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0749     |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0736      |
|    n_updates            | 11710       |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.248       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 233      |
|    iterations      | 196      |
|    time_elapsed    | 1720     |
|    total_timesteps | 401408   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.4       |
|    ep_rew_mean          | 7.64       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 197        |
|    time_elapsed         | 1729       |
|    total_timesteps      | 403456     |
| train/                  |            |
|    approx_kl            | 0.04921542 |
|    clip_fraction        | 0.087      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0917     |
|    n_updates            | 11720      |
|    policy_gradient_loss | -0.00494   |
|    value_loss           | 0.304      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 7.42        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 198         |
|    time_elapsed         | 1737        |
|    total_timesteps      | 405504      |
| train/                  |             |
|    approx_kl            | 0.028818265 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 11730       |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.364       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.63       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 199        |
|    time_elapsed         | 1744       |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.07395546 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.202     |
|    explained_variance   | 0.665      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0638     |
|    n_updates            | 11740      |
|    policy_gradient_loss | -0.0015    |
|    value_loss           | 0.46       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 200        |
|    time_elapsed         | 1752       |
|    total_timesteps      | 409600     |
| train/                  |            |
|    approx_kl            | 0.07500204 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.128     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.247      |
|    n_updates            | 11750      |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.566      |
----------------------------------------
Eval num_timesteps=410000, episode_reward=6.82 +/- 0.43
Episode length: 17.67 +/- 2.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.7      |
|    mean_reward          | 6.82      |
| time/                   |           |
|    total_timesteps      | 410000    |
| train/                  |           |
|    approx_kl            | 0.0625386 |
|    clip_fraction        | 0.128     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.154    |
|    explained_variance   | 0.492     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.516     |
|    n_updates            | 11760     |
|    policy_gradient_loss | -0.0287   |
|    value_loss           | 0.796     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 201      |
|    time_elapsed    | 1760     |
|    total_timesteps | 411648   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 202         |
|    time_elapsed         | 1768        |
|    total_timesteps      | 413696      |
| train/                  |             |
|    approx_kl            | 0.038673293 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.102       |
|    n_updates            | 11770       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.305       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.66       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 203        |
|    time_elapsed         | 1776       |
|    total_timesteps      | 415744     |
| train/                  |            |
|    approx_kl            | 0.04157144 |
|    clip_fraction        | 0.0794     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.783      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 11780      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.305      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 204        |
|    time_elapsed         | 1785       |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.04799805 |
|    clip_fraction        | 0.0896     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0372     |
|    n_updates            | 11790      |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.361      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.42        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 205         |
|    time_elapsed         | 1794        |
|    total_timesteps      | 419840      |
| train/                  |             |
|    approx_kl            | 0.030959701 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0613      |
|    n_updates            | 11800       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.314       |
-----------------------------------------
Eval num_timesteps=420000, episode_reward=6.34 +/- 1.98
Episode length: 18.33 +/- 4.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 6.34        |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.060948562 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0723      |
|    n_updates            | 11810       |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.352       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 7.93     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 206      |
|    time_elapsed    | 1803     |
|    total_timesteps | 421888   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 207         |
|    time_elapsed         | 1810        |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.039236173 |
|    clip_fraction        | 0.0969      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0922      |
|    n_updates            | 11820       |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 0.308       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 208        |
|    time_elapsed         | 1818       |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.03706204 |
|    clip_fraction        | 0.0764     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.129      |
|    n_updates            | 11830      |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 0.312      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 209        |
|    time_elapsed         | 1829       |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.04199315 |
|    clip_fraction        | 0.077      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0975    |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0464     |
|    n_updates            | 11840      |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.272      |
----------------------------------------
reached max steps=100
Eval num_timesteps=430000, episode_reward=9.64 +/- 0.31
Episode length: 20.00 +/- 1.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20         |
|    mean_reward          | 9.64       |
| time/                   |            |
|    total_timesteps      | 430000     |
| train/                  |            |
|    approx_kl            | 0.03692081 |
|    clip_fraction        | 0.0784     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0893    |
|    explained_variance   | 0.543      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.166      |
|    n_updates            | 11850      |
|    policy_gradient_loss | -0.0211    |
|    value_loss           | 0.382      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 7.92     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 210      |
|    time_elapsed    | 1837     |
|    total_timesteps | 430080   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 8.04       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 211        |
|    time_elapsed         | 1848       |
|    total_timesteps      | 432128     |
| train/                  |            |
|    approx_kl            | 0.03806203 |
|    clip_fraction        | 0.0784     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0968    |
|    explained_variance   | 0.664      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0593     |
|    n_updates            | 11860      |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.306      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.42        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 212         |
|    time_elapsed         | 1855        |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 0.029079106 |
|    clip_fraction        | 0.0587      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0744     |
|    explained_variance   | 0.822       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0273      |
|    n_updates            | 11870       |
|    policy_gradient_loss | -0.00849    |
|    value_loss           | 0.199       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.53        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 213         |
|    time_elapsed         | 1865        |
|    total_timesteps      | 436224      |
| train/                  |             |
|    approx_kl            | 0.055012774 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.508       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.273       |
|    n_updates            | 11880       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.624       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.8       |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 214        |
|    time_elapsed         | 1873       |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.06123512 |
|    clip_fraction        | 0.0963     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.118     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0678     |
|    n_updates            | 11890      |
|    policy_gradient_loss | -0.0234    |
|    value_loss           | 0.32       |
----------------------------------------
reached max steps=100
Eval num_timesteps=440000, episode_reward=6.58 +/- 1.46
Episode length: 15.67 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 6.58        |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.040780082 |
|    clip_fraction        | 0.0739      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0766      |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.447       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.7     |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 215      |
|    time_elapsed    | 1882     |
|    total_timesteps | 440320   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.5         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 216         |
|    time_elapsed         | 1891        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.040310126 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.703       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0306      |
|    n_updates            | 11910       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.364       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 7.74        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 217         |
|    time_elapsed         | 1899        |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.046917755 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.291       |
|    n_updates            | 11920       |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.52        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 7.88       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 218        |
|    time_elapsed         | 1908       |
|    total_timesteps      | 446464     |
| train/                  |            |
|    approx_kl            | 0.16537547 |
|    clip_fraction        | 0.0876     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0982    |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0461     |
|    n_updates            | 11930      |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.404      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.1      |
|    ep_rew_mean          | 7.69      |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 219       |
|    time_elapsed         | 1917      |
|    total_timesteps      | 448512    |
| train/                  |           |
|    approx_kl            | 0.0577142 |
|    clip_fraction        | 0.0802    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.104    |
|    explained_variance   | 0.698     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.335     |
|    n_updates            | 11940     |
|    policy_gradient_loss | -0.0201   |
|    value_loss           | 0.45      |
---------------------------------------
Eval num_timesteps=450000, episode_reward=7.11 +/- 1.42
Episode length: 16.33 +/- 2.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 7.11        |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.035970688 |
|    clip_fraction        | 0.0942      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.136       |
|    n_updates            | 11950       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.429       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 8        |
| time/              |          |
|    fps             | 233      |
|    iterations      | 220      |
|    time_elapsed    | 1926     |
|    total_timesteps | 450560   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 221        |
|    time_elapsed         | 1934       |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.03688964 |
|    clip_fraction        | 0.0946     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.118     |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.108      |
|    n_updates            | 11960      |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.346      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 222         |
|    time_elapsed         | 1941        |
|    total_timesteps      | 454656      |
| train/                  |             |
|    approx_kl            | 0.050460067 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0633      |
|    n_updates            | 11970       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.429       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 223        |
|    time_elapsed         | 1950       |
|    total_timesteps      | 456704     |
| train/                  |            |
|    approx_kl            | 0.05612514 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.144     |
|    explained_variance   | 0.576      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.199      |
|    n_updates            | 11980      |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 0.576      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.15        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 224         |
|    time_elapsed         | 1963        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.038464986 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0265      |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.45        |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=7.57 +/- 1.74
Episode length: 20.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.3        |
|    mean_reward          | 7.57        |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.044634193 |
|    clip_fraction        | 0.0767      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0981     |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0756      |
|    n_updates            | 12000       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.312       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 8.18     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 225      |
|    time_elapsed    | 1977     |
|    total_timesteps | 460800   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 226         |
|    time_elapsed         | 1993        |
|    total_timesteps      | 462848      |
| train/                  |             |
|    approx_kl            | 0.044039004 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0965     |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0844      |
|    n_updates            | 12010       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.357       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.23       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 227        |
|    time_elapsed         | 2006       |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.05379321 |
|    clip_fraction        | 0.0727     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0806    |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0702     |
|    n_updates            | 12020      |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 0.235      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 228         |
|    time_elapsed         | 2015        |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.031536825 |
|    clip_fraction        | 0.0734      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0643      |
|    n_updates            | 12030       |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.29        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 229        |
|    time_elapsed         | 2024       |
|    total_timesteps      | 468992     |
| train/                  |            |
|    approx_kl            | 0.03637602 |
|    clip_fraction        | 0.0912     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0247     |
|    n_updates            | 12040      |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.254      |
----------------------------------------
reached max steps=100
Eval num_timesteps=470000, episode_reward=6.44 +/- 2.18
Episode length: 16.33 +/- 3.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 6.44       |
| time/                   |            |
|    total_timesteps      | 470000     |
| train/                  |            |
|    approx_kl            | 0.04707765 |
|    clip_fraction        | 0.0866     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0359     |
|    n_updates            | 12050      |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.237      |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 230      |
|    time_elapsed    | 2034     |
|    total_timesteps | 471040   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 231        |
|    time_elapsed         | 2043       |
|    total_timesteps      | 473088     |
| train/                  |            |
|    approx_kl            | 0.04841712 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 12060      |
|    policy_gradient_loss | -0.0215    |
|    value_loss           | 0.55       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.07       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 232        |
|    time_elapsed         | 2051       |
|    total_timesteps      | 475136     |
| train/                  |            |
|    approx_kl            | 0.06278178 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.164     |
|    explained_variance   | 0.662      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.201      |
|    n_updates            | 12070      |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.685      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 233         |
|    time_elapsed         | 2061        |
|    total_timesteps      | 477184      |
| train/                  |             |
|    approx_kl            | 0.037902378 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.21        |
|    n_updates            | 12080       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.399       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 234         |
|    time_elapsed         | 2073        |
|    total_timesteps      | 479232      |
| train/                  |             |
|    approx_kl            | 0.036218997 |
|    clip_fraction        | 0.0849      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0826      |
|    n_updates            | 12090       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.36        |
-----------------------------------------
Eval num_timesteps=480000, episode_reward=7.86 +/- 0.47
Episode length: 19.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19         |
|    mean_reward          | 7.86       |
| time/                   |            |
|    total_timesteps      | 480000     |
| train/                  |            |
|    approx_kl            | 0.03480961 |
|    clip_fraction        | 0.0646     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0974    |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0599     |
|    n_updates            | 12100      |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.236      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 235      |
|    time_elapsed    | 2086     |
|    total_timesteps | 481280   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 236         |
|    time_elapsed         | 2093        |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.023144629 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 12110       |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.384       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 237        |
|    time_elapsed         | 2101       |
|    total_timesteps      | 485376     |
| train/                  |            |
|    approx_kl            | 0.03470652 |
|    clip_fraction        | 0.0845     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.048      |
|    n_updates            | 12120      |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 0.315      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 8.06       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 238        |
|    time_elapsed         | 2110       |
|    total_timesteps      | 487424     |
| train/                  |            |
|    approx_kl            | 0.02905006 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.146     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.332      |
|    n_updates            | 12130      |
|    policy_gradient_loss | -0.0235    |
|    value_loss           | 0.538      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 239         |
|    time_elapsed         | 2118        |
|    total_timesteps      | 489472      |
| train/                  |             |
|    approx_kl            | 0.044579525 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0949     |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0851      |
|    n_updates            | 12140       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.373       |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=7.26 +/- 0.56
Episode length: 18.67 +/- 2.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.7       |
|    mean_reward          | 7.26       |
| time/                   |            |
|    total_timesteps      | 490000     |
| train/                  |            |
|    approx_kl            | 0.04238023 |
|    clip_fraction        | 0.0778     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0845    |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.129      |
|    n_updates            | 12150      |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.288      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.13     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 240      |
|    time_elapsed    | 2126     |
|    total_timesteps | 491520   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 241         |
|    time_elapsed         | 2135        |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.030937634 |
|    clip_fraction        | 0.0687      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0175      |
|    n_updates            | 12160       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.35        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.8         |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 242         |
|    time_elapsed         | 2143        |
|    total_timesteps      | 495616      |
| train/                  |             |
|    approx_kl            | 0.061876014 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0966      |
|    n_updates            | 12170       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.378       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 243         |
|    time_elapsed         | 2151        |
|    total_timesteps      | 497664      |
| train/                  |             |
|    approx_kl            | 0.043494985 |
|    clip_fraction        | 0.0948      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.093       |
|    n_updates            | 12180       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.42        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.84        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 244         |
|    time_elapsed         | 2159        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.045107193 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0433      |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.357       |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=9.19 +/- 0.63
Episode length: 19.00 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 9.19        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.034017697 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.253       |
|    n_updates            | 12200       |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.346       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 245      |
|    time_elapsed    | 2167     |
|    total_timesteps | 501760   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.96        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 246         |
|    time_elapsed         | 2176        |
|    total_timesteps      | 503808      |
| train/                  |             |
|    approx_kl            | 0.061770216 |
|    clip_fraction        | 0.0997      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.566       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.077       |
|    n_updates            | 12210       |
|    policy_gradient_loss | -0.0252     |
|    value_loss           | 0.375       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 247         |
|    time_elapsed         | 2184        |
|    total_timesteps      | 505856      |
| train/                  |             |
|    approx_kl            | 0.046575237 |
|    clip_fraction        | 0.092       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0254      |
|    n_updates            | 12220       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.299       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 248         |
|    time_elapsed         | 2194        |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.034821004 |
|    clip_fraction        | 0.0869      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.673       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.44        |
|    n_updates            | 12230       |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.603       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.15        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 249         |
|    time_elapsed         | 2203        |
|    total_timesteps      | 509952      |
| train/                  |             |
|    approx_kl            | 0.032035515 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.165       |
|    n_updates            | 12240       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.582       |
-----------------------------------------
Eval num_timesteps=510000, episode_reward=8.16 +/- 0.27
Episode length: 20.67 +/- 4.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 20.7      |
|    mean_reward          | 8.16      |
| time/                   |           |
|    total_timesteps      | 510000    |
| train/                  |           |
|    approx_kl            | 0.1002831 |
|    clip_fraction        | 0.0871    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.116    |
|    explained_variance   | 0.714     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.11      |
|    n_updates            | 12250     |
|    policy_gradient_loss | -0.0131   |
|    value_loss           | 0.415     |
---------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 7.77     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 250      |
|    time_elapsed    | 2211     |
|    total_timesteps | 512000   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.74        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 251         |
|    time_elapsed         | 2220        |
|    total_timesteps      | 514048      |
| train/                  |             |
|    approx_kl            | 0.033715364 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.016       |
|    n_updates            | 12260       |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.34        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 7.92       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 252        |
|    time_elapsed         | 2229       |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.04512257 |
|    clip_fraction        | 0.0935     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.678      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.163      |
|    n_updates            | 12270      |
|    policy_gradient_loss | -0.0107    |
|    value_loss           | 0.469      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.72       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 253        |
|    time_elapsed         | 2240       |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.03672944 |
|    clip_fraction        | 0.0747     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0394     |
|    n_updates            | 12280      |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 0.254      |
----------------------------------------
reached max steps=100
Eval num_timesteps=520000, episode_reward=8.76 +/- 0.79
Episode length: 21.00 +/- 0.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 8.76       |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.06176593 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.661      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.198      |
|    n_updates            | 12290      |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.59       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 254      |
|    time_elapsed    | 2249     |
|    total_timesteps | 520192   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 255         |
|    time_elapsed         | 2258        |
|    total_timesteps      | 522240      |
| train/                  |             |
|    approx_kl            | 0.031425044 |
|    clip_fraction        | 0.0951      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.105       |
|    n_updates            | 12300       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.464       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.73       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 256        |
|    time_elapsed         | 2266       |
|    total_timesteps      | 524288     |
| train/                  |            |
|    approx_kl            | 0.03347128 |
|    clip_fraction        | 0.0956     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.122     |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0478     |
|    n_updates            | 12310      |
|    policy_gradient_loss | -0.0265    |
|    value_loss           | 0.37       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.78        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 257         |
|    time_elapsed         | 2276        |
|    total_timesteps      | 526336      |
| train/                  |             |
|    approx_kl            | 0.053106405 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0237      |
|    n_updates            | 12320       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.266       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 7.83        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 258         |
|    time_elapsed         | 2284        |
|    total_timesteps      | 528384      |
| train/                  |             |
|    approx_kl            | 0.029733034 |
|    clip_fraction        | 0.0865      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0382      |
|    n_updates            | 12330       |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.316       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=530000, episode_reward=7.92 +/- 0.84
Episode length: 15.67 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 15.7       |
|    mean_reward          | 7.92       |
| time/                   |            |
|    total_timesteps      | 530000     |
| train/                  |            |
|    approx_kl            | 0.03126961 |
|    clip_fraction        | 0.075      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.779      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0611     |
|    n_updates            | 12340      |
|    policy_gradient_loss | -0.0163    |
|    value_loss           | 0.318      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 231      |
|    iterations      | 259      |
|    time_elapsed    | 2295     |
|    total_timesteps | 530432   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.2      |
|    ep_rew_mean          | 7.67      |
| time/                   |           |
|    fps                  | 230       |
|    iterations           | 260       |
|    time_elapsed         | 2305      |
|    total_timesteps      | 532480    |
| train/                  |           |
|    approx_kl            | 0.0230196 |
|    clip_fraction        | 0.0757    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.112    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.159     |
|    n_updates            | 12350     |
|    policy_gradient_loss | -0.017    |
|    value_loss           | 0.424     |
---------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.64        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 261         |
|    time_elapsed         | 2313        |
|    total_timesteps      | 534528      |
| train/                  |             |
|    approx_kl            | 0.038838312 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0675      |
|    n_updates            | 12360       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.283       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 262        |
|    time_elapsed         | 2321       |
|    total_timesteps      | 536576     |
| train/                  |            |
|    approx_kl            | 0.06938308 |
|    clip_fraction        | 0.0796     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0979     |
|    n_updates            | 12370      |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 0.482      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 263         |
|    time_elapsed         | 2330        |
|    total_timesteps      | 538624      |
| train/                  |             |
|    approx_kl            | 0.037203074 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.038       |
|    n_updates            | 12380       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.331       |
-----------------------------------------
Eval num_timesteps=540000, episode_reward=6.76 +/- 0.85
Episode length: 21.00 +/- 5.10
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21         |
|    mean_reward          | 6.76       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.03361121 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.212      |
|    n_updates            | 12390      |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.491      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 264      |
|    time_elapsed    | 2339     |
|    total_timesteps | 540672   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 265         |
|    time_elapsed         | 2348        |
|    total_timesteps      | 542720      |
| train/                  |             |
|    approx_kl            | 0.021097206 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 12400       |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.301       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 266        |
|    time_elapsed         | 2355       |
|    total_timesteps      | 544768     |
| train/                  |            |
|    approx_kl            | 0.02784199 |
|    clip_fraction        | 0.0918     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0519     |
|    n_updates            | 12410      |
|    policy_gradient_loss | -0.0287    |
|    value_loss           | 0.321      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 7.81        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 267         |
|    time_elapsed         | 2363        |
|    total_timesteps      | 546816      |
| train/                  |             |
|    approx_kl            | 0.025931546 |
|    clip_fraction        | 0.0815      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0368      |
|    n_updates            | 12420       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.325       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 268         |
|    time_elapsed         | 2371        |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.042548034 |
|    clip_fraction        | 0.067       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0806     |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0442      |
|    n_updates            | 12430       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.228       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=550000, episode_reward=3.75 +/- 5.97
Episode length: 27.00 +/- 16.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27          |
|    mean_reward          | 3.75        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.037589733 |
|    clip_fraction        | 0.0816      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0766      |
|    n_updates            | 12440       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.269       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 269      |
|    time_elapsed    | 2381     |
|    total_timesteps | 550912   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 270         |
|    time_elapsed         | 2389        |
|    total_timesteps      | 552960      |
| train/                  |             |
|    approx_kl            | 0.032313157 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0396      |
|    n_updates            | 12450       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.39        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.31        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 271         |
|    time_elapsed         | 2397        |
|    total_timesteps      | 555008      |
| train/                  |             |
|    approx_kl            | 0.038695738 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0628      |
|    n_updates            | 12460       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.368       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 272         |
|    time_elapsed         | 2404        |
|    total_timesteps      | 557056      |
| train/                  |             |
|    approx_kl            | 0.032994002 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.089      |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0678      |
|    n_updates            | 12470       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.216       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.57        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 273         |
|    time_elapsed         | 2412        |
|    total_timesteps      | 559104      |
| train/                  |             |
|    approx_kl            | 0.037188433 |
|    clip_fraction        | 0.0968      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0713      |
|    n_updates            | 12480       |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.371       |
-----------------------------------------
Eval num_timesteps=560000, episode_reward=8.99 +/- 0.94
Episode length: 23.00 +/- 4.32
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23          |
|    mean_reward          | 8.99        |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.043604705 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0292      |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.0278     |
|    value_loss           | 0.451       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.63     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 274      |
|    time_elapsed    | 2421     |
|    total_timesteps | 561152   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 275         |
|    time_elapsed         | 2429        |
|    total_timesteps      | 563200      |
| train/                  |             |
|    approx_kl            | 0.046649806 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0649      |
|    n_updates            | 12500       |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.418       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 276         |
|    time_elapsed         | 2437        |
|    total_timesteps      | 565248      |
| train/                  |             |
|    approx_kl            | 0.024471048 |
|    clip_fraction        | 0.0855      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0997      |
|    n_updates            | 12510       |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.551       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8.04       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 277        |
|    time_elapsed         | 2445       |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.07095533 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.175     |
|    explained_variance   | 0.459      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.165      |
|    n_updates            | 12520      |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.498      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 278        |
|    time_elapsed         | 2454       |
|    total_timesteps      | 569344     |
| train/                  |            |
|    approx_kl            | 0.03643936 |
|    clip_fraction        | 0.0785     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.118     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0861     |
|    n_updates            | 12530      |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 0.365      |
----------------------------------------
Eval num_timesteps=570000, episode_reward=9.26 +/- 1.15
Episode length: 18.67 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.7        |
|    mean_reward          | 9.26        |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.046365596 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.505       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 12540       |
|    policy_gradient_loss | -0.0289     |
|    value_loss           | 0.62        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.7      |
| time/              |          |
|    fps             | 231      |
|    iterations      | 279      |
|    time_elapsed    | 2464     |
|    total_timesteps | 571392   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 280         |
|    time_elapsed         | 2472        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.035139777 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 12550       |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.512       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.1      |
|    ep_rew_mean          | 7.68      |
| time/                   |           |
|    fps                  | 231       |
|    iterations           | 281       |
|    time_elapsed         | 2481      |
|    total_timesteps      | 575488    |
| train/                  |           |
|    approx_kl            | 0.0627015 |
|    clip_fraction        | 0.0833    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.109    |
|    explained_variance   | 0.77      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.107     |
|    n_updates            | 12560     |
|    policy_gradient_loss | -0.0164   |
|    value_loss           | 0.303     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.59        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 282         |
|    time_elapsed         | 2492        |
|    total_timesteps      | 577536      |
| train/                  |             |
|    approx_kl            | 0.072623566 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.624       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.18        |
|    n_updates            | 12570       |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.728       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 8.08       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 283        |
|    time_elapsed         | 2502       |
|    total_timesteps      | 579584     |
| train/                  |            |
|    approx_kl            | 0.06933254 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0951     |
|    n_updates            | 12580      |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.56       |
----------------------------------------
Eval num_timesteps=580000, episode_reward=8.08 +/- 1.55
Episode length: 18.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 8.08        |
| time/                   |             |
|    total_timesteps      | 580000      |
| train/                  |             |
|    approx_kl            | 0.029615026 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0966     |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0532      |
|    n_updates            | 12590       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 284      |
|    time_elapsed    | 2514     |
|    total_timesteps | 581632   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 285        |
|    time_elapsed         | 2525       |
|    total_timesteps      | 583680     |
| train/                  |            |
|    approx_kl            | 0.03967681 |
|    clip_fraction        | 0.0748     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0532     |
|    n_updates            | 12600      |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 0.342      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 286         |
|    time_elapsed         | 2534        |
|    total_timesteps      | 585728      |
| train/                  |             |
|    approx_kl            | 0.031661082 |
|    clip_fraction        | 0.0839      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.099       |
|    n_updates            | 12610       |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.373       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 287        |
|    time_elapsed         | 2543       |
|    total_timesteps      | 587776     |
| train/                  |            |
|    approx_kl            | 0.04793229 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.164     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.342      |
|    n_updates            | 12620      |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.796      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.64        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 288         |
|    time_elapsed         | 2553        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.035586454 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.226       |
|    n_updates            | 12630       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.507       |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=8.38 +/- 0.10
Episode length: 19.67 +/- 4.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 8.38        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.049318418 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.267       |
|    n_updates            | 12640       |
|    policy_gradient_loss | -0.0314     |
|    value_loss           | 0.483       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 289      |
|    time_elapsed    | 2564     |
|    total_timesteps | 591872   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.44        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 290         |
|    time_elapsed         | 2572        |
|    total_timesteps      | 593920      |
| train/                  |             |
|    approx_kl            | 0.031968504 |
|    clip_fraction        | 0.072       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0766      |
|    n_updates            | 12650       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.361       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.5        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 291        |
|    time_elapsed         | 2579       |
|    total_timesteps      | 595968     |
| train/                  |            |
|    approx_kl            | 0.64788824 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.415      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0559     |
|    n_updates            | 12660      |
|    policy_gradient_loss | -0.0431    |
|    value_loss           | 0.555      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 292         |
|    time_elapsed         | 2588        |
|    total_timesteps      | 598016      |
| train/                  |             |
|    approx_kl            | 0.050804988 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 12670       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.566       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=600000, episode_reward=1.91 +/- 8.42
Episode length: 29.33 +/- 14.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 1.91        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.050953414 |
|    clip_fraction        | 0.0927      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.614       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 12680       |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.378       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 293      |
|    time_elapsed    | 2596     |
|    total_timesteps | 600064   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.87       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 294        |
|    time_elapsed         | 2604       |
|    total_timesteps      | 602112     |
| train/                  |            |
|    approx_kl            | 0.04642129 |
|    clip_fraction        | 0.0549     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.076     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0863     |
|    n_updates            | 12690      |
|    policy_gradient_loss | -0.0126    |
|    value_loss           | 0.266      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 295         |
|    time_elapsed         | 2611        |
|    total_timesteps      | 604160      |
| train/                  |             |
|    approx_kl            | 0.025748298 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.147       |
|    n_updates            | 12700       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.478       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 296         |
|    time_elapsed         | 2619        |
|    total_timesteps      | 606208      |
| train/                  |             |
|    approx_kl            | 0.041550238 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0925     |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 12710       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.309       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.24        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 297         |
|    time_elapsed         | 2628        |
|    total_timesteps      | 608256      |
| train/                  |             |
|    approx_kl            | 0.039859664 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.145       |
|    n_updates            | 12720       |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.595       |
-----------------------------------------
Eval num_timesteps=610000, episode_reward=7.55 +/- 0.90
Episode length: 17.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 7.55       |
| time/                   |            |
|    total_timesteps      | 610000     |
| train/                  |            |
|    approx_kl            | 0.07179671 |
|    clip_fraction        | 0.0841     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0888    |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.128      |
|    n_updates            | 12730      |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.281      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 298      |
|    time_elapsed    | 2641     |
|    total_timesteps | 610304   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 299         |
|    time_elapsed         | 2652        |
|    total_timesteps      | 612352      |
| train/                  |             |
|    approx_kl            | 0.028739512 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0814     |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0724      |
|    n_updates            | 12740       |
|    policy_gradient_loss | -0.00994    |
|    value_loss           | 0.335       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 300        |
|    time_elapsed         | 2665       |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.04801967 |
|    clip_fraction        | 0.0705     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0838    |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.136      |
|    n_updates            | 12750      |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.288      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 301         |
|    time_elapsed         | 2675        |
|    total_timesteps      | 616448      |
| train/                  |             |
|    approx_kl            | 0.045042764 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 12760       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.614       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.02        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 302         |
|    time_elapsed         | 2686        |
|    total_timesteps      | 618496      |
| train/                  |             |
|    approx_kl            | 0.032311372 |
|    clip_fraction        | 0.0794      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 12770       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.663       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=620000, episode_reward=7.77 +/- 0.76
Episode length: 16.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 7.77       |
| time/                   |            |
|    total_timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.03639397 |
|    clip_fraction        | 0.0614     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0698    |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.133      |
|    n_updates            | 12780      |
|    policy_gradient_loss | -0.00977   |
|    value_loss           | 0.339      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 303      |
|    time_elapsed    | 2696     |
|    total_timesteps | 620544   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 304         |
|    time_elapsed         | 2707        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.041606568 |
|    clip_fraction        | 0.064       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.081      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0617      |
|    n_updates            | 12790       |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.404       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 305         |
|    time_elapsed         | 2718        |
|    total_timesteps      | 624640      |
| train/                  |             |
|    approx_kl            | 0.028438035 |
|    clip_fraction        | 0.0526      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0639     |
|    explained_variance   | 0.82        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0632      |
|    n_updates            | 12800       |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.241       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.44       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 306        |
|    time_elapsed         | 2729       |
|    total_timesteps      | 626688     |
| train/                  |            |
|    approx_kl            | 0.05756575 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.585      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0319     |
|    n_updates            | 12810      |
|    policy_gradient_loss | -0.0297    |
|    value_loss           | 0.36       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 307         |
|    time_elapsed         | 2742        |
|    total_timesteps      | 628736      |
| train/                  |             |
|    approx_kl            | 0.081198476 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.612       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 12820       |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 0.504       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=630000, episode_reward=3.25 +/- 7.96
Episode length: 32.33 +/- 12.55
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.3        |
|    mean_reward          | 3.25        |
| time/                   |             |
|    total_timesteps      | 630000      |
| train/                  |             |
|    approx_kl            | 0.024878103 |
|    clip_fraction        | 0.0743      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0999     |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0969      |
|    n_updates            | 12830       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.467       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.21     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 308      |
|    time_elapsed    | 2754     |
|    total_timesteps | 630784   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 309         |
|    time_elapsed         | 2767        |
|    total_timesteps      | 632832      |
| train/                  |             |
|    approx_kl            | 0.027356617 |
|    clip_fraction        | 0.062       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0678     |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0757      |
|    n_updates            | 12840       |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.286       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.86       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 310        |
|    time_elapsed         | 2780       |
|    total_timesteps      | 634880     |
| train/                  |            |
|    approx_kl            | 0.03825275 |
|    clip_fraction        | 0.0683     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0837    |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.104      |
|    n_updates            | 12850      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.278      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 311         |
|    time_elapsed         | 2787        |
|    total_timesteps      | 636928      |
| train/                  |             |
|    approx_kl            | 0.029616052 |
|    clip_fraction        | 0.0747      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 12860       |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 0.322       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.6      |
|    ep_rew_mean          | 8         |
| time/                   |           |
|    fps                  | 228       |
|    iterations           | 312       |
|    time_elapsed         | 2796      |
|    total_timesteps      | 638976    |
| train/                  |           |
|    approx_kl            | 0.1638747 |
|    clip_fraction        | 0.137     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.108    |
|    explained_variance   | 0.601     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0779    |
|    n_updates            | 12870     |
|    policy_gradient_loss | -0.0224   |
|    value_loss           | 0.443     |
---------------------------------------
Eval num_timesteps=640000, episode_reward=7.93 +/- 1.10
Episode length: 18.67 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.7        |
|    mean_reward          | 7.93        |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.040331244 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.055       |
|    n_updates            | 12880       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.335       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.62     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 313      |
|    time_elapsed    | 2806     |
|    total_timesteps | 641024   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 314         |
|    time_elapsed         | 2817        |
|    total_timesteps      | 643072      |
| train/                  |             |
|    approx_kl            | 0.030483495 |
|    clip_fraction        | 0.0887      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0606      |
|    n_updates            | 12890       |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.43        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 8.36        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 315         |
|    time_elapsed         | 2829        |
|    total_timesteps      | 645120      |
| train/                  |             |
|    approx_kl            | 0.031312972 |
|    clip_fraction        | 0.0862      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.1         |
|    n_updates            | 12900       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.4         |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 316        |
|    time_elapsed         | 2842       |
|    total_timesteps      | 647168     |
| train/                  |            |
|    approx_kl            | 0.04186887 |
|    clip_fraction        | 0.0675     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0875    |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0288     |
|    n_updates            | 12910      |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 0.238      |
----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.2      |
|    ep_rew_mean          | 8.07      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 317       |
|    time_elapsed         | 2853      |
|    total_timesteps      | 649216    |
| train/                  |           |
|    approx_kl            | 0.0424991 |
|    clip_fraction        | 0.0879    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.122    |
|    explained_variance   | 0.728     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0586    |
|    n_updates            | 12920     |
|    policy_gradient_loss | -0.0226   |
|    value_loss           | 0.352     |
---------------------------------------
reached max steps=100
Eval num_timesteps=650000, episode_reward=8.15 +/- 0.27
Episode length: 17.67 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 8.15        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.034823198 |
|    clip_fraction        | 0.082       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.136       |
|    n_updates            | 12930       |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 0.356       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 318      |
|    time_elapsed    | 2864     |
|    total_timesteps | 651264   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.8       |
|    ep_rew_mean          | 7.96       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 319        |
|    time_elapsed         | 2876       |
|    total_timesteps      | 653312     |
| train/                  |            |
|    approx_kl            | 0.04808007 |
|    clip_fraction        | 0.09       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0771     |
|    n_updates            | 12940      |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 0.335      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 320         |
|    time_elapsed         | 2887        |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.041762605 |
|    clip_fraction        | 0.062       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0701     |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0602      |
|    n_updates            | 12950       |
|    policy_gradient_loss | -0.0099     |
|    value_loss           | 0.196       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.8         |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 321         |
|    time_elapsed         | 2898        |
|    total_timesteps      | 657408      |
| train/                  |             |
|    approx_kl            | 0.030099055 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0744     |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0474      |
|    n_updates            | 12960       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.259       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 322        |
|    time_elapsed         | 2910       |
|    total_timesteps      | 659456     |
| train/                  |            |
|    approx_kl            | 0.06268124 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.211      |
|    n_updates            | 12970      |
|    policy_gradient_loss | -0.0239    |
|    value_loss           | 0.43       |
----------------------------------------
reached max steps=100
Eval num_timesteps=660000, episode_reward=8.40 +/- 1.75
Episode length: 25.67 +/- 8.01
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.7       |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 660000     |
| train/                  |            |
|    approx_kl            | 0.03525147 |
|    clip_fraction        | 0.0681     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0814    |
|    explained_variance   | 0.818      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0647     |
|    n_updates            | 12980      |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.188      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 8.14     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 323      |
|    time_elapsed    | 2922     |
|    total_timesteps | 661504   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 324        |
|    time_elapsed         | 2933       |
|    total_timesteps      | 663552     |
| train/                  |            |
|    approx_kl            | 0.03327319 |
|    clip_fraction        | 0.0783     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0899    |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.225      |
|    n_updates            | 12990      |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 0.496      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.87       |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 325        |
|    time_elapsed         | 2945       |
|    total_timesteps      | 665600     |
| train/                  |            |
|    approx_kl            | 0.05601827 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.131     |
|    explained_variance   | 0.672      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.192      |
|    n_updates            | 13000      |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.628      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 326         |
|    time_elapsed         | 2956        |
|    total_timesteps      | 667648      |
| train/                  |             |
|    approx_kl            | 0.027086802 |
|    clip_fraction        | 0.0624      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0853     |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0771      |
|    n_updates            | 13010       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.271       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.91       |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 327        |
|    time_elapsed         | 2969       |
|    total_timesteps      | 669696     |
| train/                  |            |
|    approx_kl            | 0.03414853 |
|    clip_fraction        | 0.085      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0963    |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 13020      |
|    policy_gradient_loss | -0.021     |
|    value_loss           | 0.262      |
----------------------------------------
reached max steps=100
Eval num_timesteps=670000, episode_reward=2.72 +/- 9.03
Episode length: 28.67 +/- 15.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.7        |
|    mean_reward          | 2.72        |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.045050114 |
|    clip_fraction        | 0.0771      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0665      |
|    n_updates            | 13030       |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.281       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.2     |
|    ep_rew_mean     | 7.58     |
| time/              |          |
|    fps             | 225      |
|    iterations      | 328      |
|    time_elapsed    | 2980     |
|    total_timesteps | 671744   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.33        |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 329         |
|    time_elapsed         | 2992        |
|    total_timesteps      | 673792      |
| train/                  |             |
|    approx_kl            | 0.053109113 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.592       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.255       |
|    n_updates            | 13040       |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.649       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.4        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 330         |
|    time_elapsed         | 3004        |
|    total_timesteps      | 675840      |
| train/                  |             |
|    approx_kl            | 0.045419544 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.154      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 13050       |
|    policy_gradient_loss | -0.0267     |
|    value_loss           | 0.535       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 331         |
|    time_elapsed         | 3015        |
|    total_timesteps      | 677888      |
| train/                  |             |
|    approx_kl            | 0.040832426 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.147       |
|    n_updates            | 13060       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.544       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 332         |
|    time_elapsed         | 3026        |
|    total_timesteps      | 679936      |
| train/                  |             |
|    approx_kl            | 0.042150114 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 13070       |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.363       |
-----------------------------------------
Eval num_timesteps=680000, episode_reward=7.79 +/- 0.91
Episode length: 19.33 +/- 4.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 19.3      |
|    mean_reward          | 7.79      |
| time/                   |           |
|    total_timesteps      | 680000    |
| train/                  |           |
|    approx_kl            | 0.0162282 |
|    clip_fraction        | 0.0567    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0718   |
|    explained_variance   | 0.752     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0533    |
|    n_updates            | 13080     |
|    policy_gradient_loss | -0.0159   |
|    value_loss           | 0.279     |
---------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 8        |
| time/              |          |
|    fps             | 224      |
|    iterations      | 333      |
|    time_elapsed    | 3037     |
|    total_timesteps | 681984   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 334         |
|    time_elapsed         | 3050        |
|    total_timesteps      | 684032      |
| train/                  |             |
|    approx_kl            | 0.036016766 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.161       |
|    n_updates            | 13090       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.46        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 335         |
|    time_elapsed         | 3060        |
|    total_timesteps      | 686080      |
| train/                  |             |
|    approx_kl            | 0.038129367 |
|    clip_fraction        | 0.0904      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 13100       |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.418       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.11       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 336        |
|    time_elapsed         | 3072       |
|    total_timesteps      | 688128     |
| train/                  |            |
|    approx_kl            | 0.06991793 |
|    clip_fraction        | 0.0818     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.105     |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.18       |
|    n_updates            | 13110      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.389      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=690000, episode_reward=9.79 +/- 0.54
Episode length: 19.33 +/- 2.49
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 19.3      |
|    mean_reward          | 9.79      |
| time/                   |           |
|    total_timesteps      | 690000    |
| train/                  |           |
|    approx_kl            | 0.0375979 |
|    clip_fraction        | 0.074     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0824   |
|    explained_variance   | 0.725     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0525    |
|    n_updates            | 13120     |
|    policy_gradient_loss | -0.0157   |
|    value_loss           | 0.303     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.61     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 337      |
|    time_elapsed    | 3082     |
|    total_timesteps | 690176   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 338         |
|    time_elapsed         | 3095        |
|    total_timesteps      | 692224      |
| train/                  |             |
|    approx_kl            | 0.055686645 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.286       |
|    n_updates            | 13130       |
|    policy_gradient_loss | 0.0482      |
|    value_loss           | 0.608       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.5      |
|    ep_rew_mean          | 7.83      |
| time/                   |           |
|    fps                  | 223       |
|    iterations           | 339       |
|    time_elapsed         | 3108      |
|    total_timesteps      | 694272    |
| train/                  |           |
|    approx_kl            | 0.0677323 |
|    clip_fraction        | 0.0881    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.106    |
|    explained_variance   | 0.729     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.117     |
|    n_updates            | 13140     |
|    policy_gradient_loss | -0.0163   |
|    value_loss           | 0.405     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8.05       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 340        |
|    time_elapsed         | 3120       |
|    total_timesteps      | 696320     |
| train/                  |            |
|    approx_kl            | 0.09462573 |
|    clip_fraction        | 0.0807     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0931    |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0931     |
|    n_updates            | 13150      |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.374      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 341         |
|    time_elapsed         | 3133        |
|    total_timesteps      | 698368      |
| train/                  |             |
|    approx_kl            | 0.046345524 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0877     |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0853      |
|    n_updates            | 13160       |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.26        |
-----------------------------------------
Eval num_timesteps=700000, episode_reward=7.18 +/- 0.63
Episode length: 16.00 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 7.18        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.033841483 |
|    clip_fraction        | 0.0769      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0947     |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.255       |
|    n_updates            | 13170       |
|    policy_gradient_loss | 0.0271      |
|    value_loss           | 0.357       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.77     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 342      |
|    time_elapsed    | 3144     |
|    total_timesteps | 700416   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 343        |
|    time_elapsed         | 3156       |
|    total_timesteps      | 702464     |
| train/                  |            |
|    approx_kl            | 0.10852727 |
|    clip_fraction        | 0.0851     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0691     |
|    n_updates            | 13180      |
|    policy_gradient_loss | -0.0123    |
|    value_loss           | 0.386      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.89       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 344        |
|    time_elapsed         | 3168       |
|    total_timesteps      | 704512     |
| train/                  |            |
|    approx_kl            | 0.12022295 |
|    clip_fraction        | 0.0919     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0701     |
|    n_updates            | 13190      |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 0.473      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.06        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 345         |
|    time_elapsed         | 3180        |
|    total_timesteps      | 706560      |
| train/                  |             |
|    approx_kl            | 0.039201986 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0935     |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.128       |
|    n_updates            | 13200       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.298       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 346        |
|    time_elapsed         | 3194       |
|    total_timesteps      | 708608     |
| train/                  |            |
|    approx_kl            | 0.04875447 |
|    clip_fraction        | 0.071      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0959    |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0705     |
|    n_updates            | 13210      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.413      |
----------------------------------------
Eval num_timesteps=710000, episode_reward=7.77 +/- 0.90
Episode length: 16.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 7.77       |
| time/                   |            |
|    total_timesteps      | 710000     |
| train/                  |            |
|    approx_kl            | 0.07816841 |
|    clip_fraction        | 0.0772     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.181      |
|    n_updates            | 13220      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.354      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 8.06     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 347      |
|    time_elapsed    | 3206     |
|    total_timesteps | 710656   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 348         |
|    time_elapsed         | 3217        |
|    total_timesteps      | 712704      |
| train/                  |             |
|    approx_kl            | 0.041025862 |
|    clip_fraction        | 0.0735      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0869     |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0524      |
|    n_updates            | 13230       |
|    policy_gradient_loss | -0.00777    |
|    value_loss           | 0.3         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 349         |
|    time_elapsed         | 3229        |
|    total_timesteps      | 714752      |
| train/                  |             |
|    approx_kl            | 0.027667098 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0807     |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0644      |
|    n_updates            | 13240       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.238       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 350         |
|    time_elapsed         | 3241        |
|    total_timesteps      | 716800      |
| train/                  |             |
|    approx_kl            | 0.048858352 |
|    clip_fraction        | 0.0852      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 13250       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.349       |
-----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.9      |
|    ep_rew_mean          | 7.63      |
| time/                   |           |
|    fps                  | 220       |
|    iterations           | 351       |
|    time_elapsed         | 3253      |
|    total_timesteps      | 718848    |
| train/                  |           |
|    approx_kl            | 0.0520369 |
|    clip_fraction        | 0.0725    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0945   |
|    explained_variance   | 0.8       |
|    learning_rate        | 0.0003    |
|    loss                 | 0.157     |
|    n_updates            | 13260     |
|    policy_gradient_loss | -0.0172   |
|    value_loss           | 0.284     |
---------------------------------------
Eval num_timesteps=720000, episode_reward=8.29 +/- 1.28
Episode length: 17.00 +/- 1.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17         |
|    mean_reward          | 8.29       |
| time/                   |            |
|    total_timesteps      | 720000     |
| train/                  |            |
|    approx_kl            | 0.03675647 |
|    clip_fraction        | 0.0855     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0878     |
|    n_updates            | 13270      |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.398      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 352      |
|    time_elapsed    | 3265     |
|    total_timesteps | 720896   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 353         |
|    time_elapsed         | 3276        |
|    total_timesteps      | 722944      |
| train/                  |             |
|    approx_kl            | 0.036890604 |
|    clip_fraction        | 0.0804      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.298       |
|    n_updates            | 13280       |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 0.509       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 354         |
|    time_elapsed         | 3289        |
|    total_timesteps      | 724992      |
| train/                  |             |
|    approx_kl            | 0.045951873 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.0183     |
|    value_loss           | 0.351       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 355         |
|    time_elapsed         | 3302        |
|    total_timesteps      | 727040      |
| train/                  |             |
|    approx_kl            | 0.043561667 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.597       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.336       |
|    n_updates            | 13300       |
|    policy_gradient_loss | 0.00688     |
|    value_loss           | 0.459       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 356        |
|    time_elapsed         | 3312       |
|    total_timesteps      | 729088     |
| train/                  |            |
|    approx_kl            | 0.04289849 |
|    clip_fraction        | 0.0793     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0733     |
|    n_updates            | 13310      |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.303      |
----------------------------------------
Eval num_timesteps=730000, episode_reward=8.00 +/- 0.66
Episode length: 18.33 +/- 4.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 8           |
| time/                   |             |
|    total_timesteps      | 730000      |
| train/                  |             |
|    approx_kl            | 0.039566763 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0413      |
|    n_updates            | 13320       |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.353       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 357      |
|    time_elapsed    | 3324     |
|    total_timesteps | 731136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.26        |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 358         |
|    time_elapsed         | 3337        |
|    total_timesteps      | 733184      |
| train/                  |             |
|    approx_kl            | 0.057642728 |
|    clip_fraction        | 0.0714      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.082      |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0473      |
|    n_updates            | 13330       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.277       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 359        |
|    time_elapsed         | 3348       |
|    total_timesteps      | 735232     |
| train/                  |            |
|    approx_kl            | 0.02798482 |
|    clip_fraction        | 0.0544     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0662    |
|    explained_variance   | 0.832      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0301     |
|    n_updates            | 13340      |
|    policy_gradient_loss | -0.0123    |
|    value_loss           | 0.197      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 360         |
|    time_elapsed         | 3360        |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.075654306 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0354      |
|    n_updates            | 13350       |
|    policy_gradient_loss | -0.0331     |
|    value_loss           | 0.396       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.08        |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 361         |
|    time_elapsed         | 3370        |
|    total_timesteps      | 739328      |
| train/                  |             |
|    approx_kl            | 0.075724795 |
|    clip_fraction        | 0.0745      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0843     |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0486      |
|    n_updates            | 13360       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.24        |
-----------------------------------------
Eval num_timesteps=740000, episode_reward=8.02 +/- 0.28
Episode length: 21.33 +/- 4.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.3       |
|    mean_reward          | 8.02       |
| time/                   |            |
|    total_timesteps      | 740000     |
| train/                  |            |
|    approx_kl            | 0.03698684 |
|    clip_fraction        | 0.0872     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.802      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0317     |
|    n_updates            | 13370      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.23       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 362      |
|    time_elapsed    | 3383     |
|    total_timesteps | 741376   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 8.16       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 363        |
|    time_elapsed         | 3395       |
|    total_timesteps      | 743424     |
| train/                  |            |
|    approx_kl            | 0.05018798 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0756     |
|    n_updates            | 13380      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.278      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 8.09       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 364        |
|    time_elapsed         | 3406       |
|    total_timesteps      | 745472     |
| train/                  |            |
|    approx_kl            | 0.03579369 |
|    clip_fraction        | 0.074      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0684     |
|    n_updates            | 13390      |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 0.246      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 365         |
|    time_elapsed         | 3419        |
|    total_timesteps      | 747520      |
| train/                  |             |
|    approx_kl            | 0.035126798 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0694      |
|    n_updates            | 13400       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.341       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 366         |
|    time_elapsed         | 3431        |
|    total_timesteps      | 749568      |
| train/                  |             |
|    approx_kl            | 0.043416675 |
|    clip_fraction        | 0.0777      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0855      |
|    n_updates            | 13410       |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.337       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=750000, episode_reward=2.21 +/- 8.72
Episode length: 31.00 +/- 13.59
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 31          |
|    mean_reward          | 2.21        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.036693342 |
|    clip_fraction        | 0.0846      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.052       |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.266       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 367      |
|    time_elapsed    | 3442     |
|    total_timesteps | 751616   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 368         |
|    time_elapsed         | 3455        |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.037181348 |
|    clip_fraction        | 0.0707      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0884     |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.055       |
|    n_updates            | 13430       |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.218       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 369         |
|    time_elapsed         | 3468        |
|    total_timesteps      | 755712      |
| train/                  |             |
|    approx_kl            | 0.031330604 |
|    clip_fraction        | 0.0833      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 13440       |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.386       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 370         |
|    time_elapsed         | 3479        |
|    total_timesteps      | 757760      |
| train/                  |             |
|    approx_kl            | 0.041805178 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.63        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0372      |
|    n_updates            | 13450       |
|    policy_gradient_loss | -0.027      |
|    value_loss           | 0.43        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 371        |
|    time_elapsed         | 3490       |
|    total_timesteps      | 759808     |
| train/                  |            |
|    approx_kl            | 0.04029405 |
|    clip_fraction        | 0.0874     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.132     |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.188      |
|    n_updates            | 13460      |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.433      |
----------------------------------------
reached max steps=100
Eval num_timesteps=760000, episode_reward=4.20 +/- 6.02
Episode length: 28.00 +/- 15.56
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | 4.2         |
| time/                   |             |
|    total_timesteps      | 760000      |
| train/                  |             |
|    approx_kl            | 0.041371927 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 13470       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.603       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 372      |
|    time_elapsed    | 3502     |
|    total_timesteps | 761856   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 373        |
|    time_elapsed         | 3514       |
|    total_timesteps      | 763904     |
| train/                  |            |
|    approx_kl            | 0.04679973 |
|    clip_fraction        | 0.095      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0753     |
|    n_updates            | 13480      |
|    policy_gradient_loss | -0.0175    |
|    value_loss           | 0.682      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 8.06       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 374        |
|    time_elapsed         | 3525       |
|    total_timesteps      | 765952     |
| train/                  |            |
|    approx_kl            | 0.03482313 |
|    clip_fraction        | 0.0926     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.127     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 13490      |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.412      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 375         |
|    time_elapsed         | 3536        |
|    total_timesteps      | 768000      |
| train/                  |             |
|    approx_kl            | 0.111043155 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.162       |
|    n_updates            | 13500       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=770000, episode_reward=8.83 +/- 0.91
Episode length: 20.67 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.7        |
|    mean_reward          | 8.83        |
| time/                   |             |
|    total_timesteps      | 770000      |
| train/                  |             |
|    approx_kl            | 0.046220742 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0402      |
|    n_updates            | 13510       |
|    policy_gradient_loss | -0.0206     |
|    value_loss           | 0.283       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 376      |
|    time_elapsed    | 3549     |
|    total_timesteps | 770048   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.54        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 377         |
|    time_elapsed         | 3560        |
|    total_timesteps      | 772096      |
| train/                  |             |
|    approx_kl            | 0.034462247 |
|    clip_fraction        | 0.0853      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 13520       |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.393       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 378         |
|    time_elapsed         | 3573        |
|    total_timesteps      | 774144      |
| train/                  |             |
|    approx_kl            | 0.040694542 |
|    clip_fraction        | 0.0867      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 13530       |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.488       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.41        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 379         |
|    time_elapsed         | 3584        |
|    total_timesteps      | 776192      |
| train/                  |             |
|    approx_kl            | 0.034904808 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0979      |
|    n_updates            | 13540       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.569       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.4       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 380        |
|    time_elapsed         | 3596       |
|    total_timesteps      | 778240     |
| train/                  |            |
|    approx_kl            | 0.02974162 |
|    clip_fraction        | 0.0978     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.247      |
|    n_updates            | 13550      |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.594      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=780000, episode_reward=9.05 +/- 0.91
Episode length: 19.67 +/- 1.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.7       |
|    mean_reward          | 9.05       |
| time/                   |            |
|    total_timesteps      | 780000     |
| train/                  |            |
|    approx_kl            | 0.04095689 |
|    clip_fraction        | 0.0921     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.149     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0687     |
|    n_updates            | 13560      |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.455      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 381      |
|    time_elapsed    | 3606     |
|    total_timesteps | 780288   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.21        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 382         |
|    time_elapsed         | 3618        |
|    total_timesteps      | 782336      |
| train/                  |             |
|    approx_kl            | 0.034752853 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.148       |
|    n_updates            | 13570       |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.553       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 383         |
|    time_elapsed         | 3629        |
|    total_timesteps      | 784384      |
| train/                  |             |
|    approx_kl            | 0.037968047 |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.089      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0493      |
|    n_updates            | 13580       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.28        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 384         |
|    time_elapsed         | 3640        |
|    total_timesteps      | 786432      |
| train/                  |             |
|    approx_kl            | 0.046343498 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0954      |
|    n_updates            | 13590       |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.397       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 385         |
|    time_elapsed         | 3651        |
|    total_timesteps      | 788480      |
| train/                  |             |
|    approx_kl            | 0.041459948 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0415      |
|    n_updates            | 13600       |
|    policy_gradient_loss | -0.0201     |
|    value_loss           | 0.433       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=790000, episode_reward=1.29 +/- 6.12
Episode length: 39.67 +/- 14.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.7        |
|    mean_reward          | 1.29        |
| time/                   |             |
|    total_timesteps      | 790000      |
| train/                  |             |
|    approx_kl            | 0.034494728 |
|    clip_fraction        | 0.0874      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.094       |
|    n_updates            | 13610       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.5         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.05     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 386      |
|    time_elapsed    | 3664     |
|    total_timesteps | 790528   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 387         |
|    time_elapsed         | 3673        |
|    total_timesteps      | 792576      |
| train/                  |             |
|    approx_kl            | 0.042167198 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0601      |
|    n_updates            | 13620       |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.277       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 388        |
|    time_elapsed         | 3684       |
|    total_timesteps      | 794624     |
| train/                  |            |
|    approx_kl            | 0.40031105 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.526      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0803     |
|    n_updates            | 13630      |
|    policy_gradient_loss | -0.0263    |
|    value_loss           | 0.405      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.99       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 389        |
|    time_elapsed         | 3695       |
|    total_timesteps      | 796672     |
| train/                  |            |
|    approx_kl            | 0.06366275 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.725      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 13640      |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.422      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 390         |
|    time_elapsed         | 3706        |
|    total_timesteps      | 798720      |
| train/                  |             |
|    approx_kl            | 0.041056145 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.141       |
|    n_updates            | 13650       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.336       |
-----------------------------------------
Eval num_timesteps=800000, episode_reward=8.67 +/- 1.69
Episode length: 18.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 8.67       |
| time/                   |            |
|    total_timesteps      | 800000     |
| train/                  |            |
|    approx_kl            | 0.07331402 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.699      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0596     |
|    n_updates            | 13660      |
|    policy_gradient_loss | -0.0211    |
|    value_loss           | 0.431      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 391      |
|    time_elapsed    | 3718     |
|    total_timesteps | 800768   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 392        |
|    time_elapsed         | 3729       |
|    total_timesteps      | 802816     |
| train/                  |            |
|    approx_kl            | 0.03860277 |
|    clip_fraction        | 0.0969     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.15      |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.141      |
|    n_updates            | 13670      |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 0.338      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.44        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 393         |
|    time_elapsed         | 3740        |
|    total_timesteps      | 804864      |
| train/                  |             |
|    approx_kl            | 0.041470654 |
|    clip_fraction        | 0.0808      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0378      |
|    n_updates            | 13680       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.284       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.52       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 394        |
|    time_elapsed         | 3750       |
|    total_timesteps      | 806912     |
| train/                  |            |
|    approx_kl            | 0.09767853 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.226     |
|    explained_variance   | 0.452      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.273      |
|    n_updates            | 13690      |
|    policy_gradient_loss | -0.0285    |
|    value_loss           | 0.811      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 395         |
|    time_elapsed         | 3760        |
|    total_timesteps      | 808960      |
| train/                  |             |
|    approx_kl            | 0.039052103 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.216      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.241       |
|    n_updates            | 13700       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 0.712       |
-----------------------------------------
Eval num_timesteps=810000, episode_reward=7.45 +/- 1.09
Episode length: 19.33 +/- 3.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 7.45        |
| time/                   |             |
|    total_timesteps      | 810000      |
| train/                  |             |
|    approx_kl            | 0.058492377 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.182      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.118       |
|    n_updates            | 13710       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.442       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 396      |
|    time_elapsed    | 3770     |
|    total_timesteps | 811008   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.39        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 397         |
|    time_elapsed         | 3780        |
|    total_timesteps      | 813056      |
| train/                  |             |
|    approx_kl            | 0.052340433 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 13720       |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.48        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 398        |
|    time_elapsed         | 3790       |
|    total_timesteps      | 815104     |
| train/                  |            |
|    approx_kl            | 0.19059409 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.466      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.216      |
|    n_updates            | 13730      |
|    policy_gradient_loss | -0.023     |
|    value_loss           | 0.518      |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.2      |
|    ep_rew_mean          | 7.66      |
| time/                   |           |
|    fps                  | 214       |
|    iterations           | 399       |
|    time_elapsed         | 3801      |
|    total_timesteps      | 817152    |
| train/                  |           |
|    approx_kl            | 0.0448597 |
|    clip_fraction        | 0.0917    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.142    |
|    explained_variance   | 0.699     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0991    |
|    n_updates            | 13740     |
|    policy_gradient_loss | -0.0175   |
|    value_loss           | 0.475     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.3      |
|    ep_rew_mean          | 8.09      |
| time/                   |           |
|    fps                  | 214       |
|    iterations           | 400       |
|    time_elapsed         | 3812      |
|    total_timesteps      | 819200    |
| train/                  |           |
|    approx_kl            | 0.0215665 |
|    clip_fraction        | 0.0777    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.131    |
|    explained_variance   | 0.685     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.103     |
|    n_updates            | 13750     |
|    policy_gradient_loss | -0.0131   |
|    value_loss           | 0.395     |
---------------------------------------
reached max steps=100
Eval num_timesteps=820000, episode_reward=8.74 +/- 1.59
Episode length: 18.00 +/- 1.63
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18         |
|    mean_reward          | 8.74       |
| time/                   |            |
|    total_timesteps      | 820000     |
| train/                  |            |
|    approx_kl            | 0.03251139 |
|    clip_fraction        | 0.0702     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0907     |
|    n_updates            | 13760      |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 0.26       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 401      |
|    time_elapsed    | 3823     |
|    total_timesteps | 821248   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 402        |
|    time_elapsed         | 3833       |
|    total_timesteps      | 823296     |
| train/                  |            |
|    approx_kl            | 0.04720748 |
|    clip_fraction        | 0.0958     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.716      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.197      |
|    n_updates            | 13770      |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.457      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8.05       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 403        |
|    time_elapsed         | 3844       |
|    total_timesteps      | 825344     |
| train/                  |            |
|    approx_kl            | 0.04441162 |
|    clip_fraction        | 0.0927     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.24       |
|    n_updates            | 13780      |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 0.511      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 7.96       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 404        |
|    time_elapsed         | 3854       |
|    total_timesteps      | 827392     |
| train/                  |            |
|    approx_kl            | 0.16207626 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.122     |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.065      |
|    n_updates            | 13790      |
|    policy_gradient_loss | -0.0223    |
|    value_loss           | 0.361      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 405         |
|    time_elapsed         | 3864        |
|    total_timesteps      | 829440      |
| train/                  |             |
|    approx_kl            | 0.026516028 |
|    clip_fraction        | 0.0679      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0931     |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.106       |
|    n_updates            | 13800       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.287       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=830000, episode_reward=3.02 +/- 9.22
Episode length: 30.33 +/- 14.06
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.3        |
|    mean_reward          | 3.02        |
| time/                   |             |
|    total_timesteps      | 830000      |
| train/                  |             |
|    approx_kl            | 0.072265066 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0906     |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0543      |
|    n_updates            | 13810       |
|    policy_gradient_loss | -0.0261     |
|    value_loss           | 0.273       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 406      |
|    time_elapsed    | 3875     |
|    total_timesteps | 831488   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 407        |
|    time_elapsed         | 3886       |
|    total_timesteps      | 833536     |
| train/                  |            |
|    approx_kl            | 0.08134272 |
|    clip_fraction        | 0.0985     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.092     |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 13820      |
|    policy_gradient_loss | -0.0274    |
|    value_loss           | 0.411      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 408        |
|    time_elapsed         | 3896       |
|    total_timesteps      | 835584     |
| train/                  |            |
|    approx_kl            | 0.06974489 |
|    clip_fraction        | 0.0815     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0945    |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.102      |
|    n_updates            | 13830      |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 0.449      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.4       |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 409        |
|    time_elapsed         | 3906       |
|    total_timesteps      | 837632     |
| train/                  |            |
|    approx_kl            | 0.08548795 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.407      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0404     |
|    n_updates            | 13840      |
|    policy_gradient_loss | -0.0245    |
|    value_loss           | 0.446      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 410        |
|    time_elapsed         | 3916       |
|    total_timesteps      | 839680     |
| train/                  |            |
|    approx_kl            | 0.05411849 |
|    clip_fraction        | 0.0873     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.109     |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0322     |
|    n_updates            | 13850      |
|    policy_gradient_loss | -0.0237    |
|    value_loss           | 0.343      |
----------------------------------------
Eval num_timesteps=840000, episode_reward=8.38 +/- 1.10
Episode length: 19.67 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 8.38        |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.053022936 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0995     |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0396      |
|    n_updates            | 13860       |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.45        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 8.01     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 411      |
|    time_elapsed    | 3926     |
|    total_timesteps | 841728   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 412         |
|    time_elapsed         | 3937        |
|    total_timesteps      | 843776      |
| train/                  |             |
|    approx_kl            | 0.031677812 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.668       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0608      |
|    n_updates            | 13870       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.339       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.4       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 413        |
|    time_elapsed         | 3947       |
|    total_timesteps      | 845824     |
| train/                  |            |
|    approx_kl            | 0.09070568 |
|    clip_fraction        | 0.0929     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0906    |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0776     |
|    n_updates            | 13880      |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 0.478      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 414         |
|    time_elapsed         | 3958        |
|    total_timesteps      | 847872      |
| train/                  |             |
|    approx_kl            | 0.032544218 |
|    clip_fraction        | 0.0682      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0921     |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0846      |
|    n_updates            | 13890       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.333       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 415         |
|    time_elapsed         | 3969        |
|    total_timesteps      | 849920      |
| train/                  |             |
|    approx_kl            | 0.048640758 |
|    clip_fraction        | 0.0744      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 13900       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.297       |
-----------------------------------------
Eval num_timesteps=850000, episode_reward=8.83 +/- 0.58
Episode length: 20.67 +/- 4.11
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20.7       |
|    mean_reward          | 8.83       |
| time/                   |            |
|    total_timesteps      | 850000     |
| train/                  |            |
|    approx_kl            | 0.06251798 |
|    clip_fraction        | 0.0882     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.266      |
|    n_updates            | 13910      |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 0.527      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.1      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 416      |
|    time_elapsed    | 3980     |
|    total_timesteps | 851968   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 417         |
|    time_elapsed         | 3990        |
|    total_timesteps      | 854016      |
| train/                  |             |
|    approx_kl            | 0.048617102 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0826     |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    n_updates            | 13920       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.292       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.58       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 418        |
|    time_elapsed         | 4001       |
|    total_timesteps      | 856064     |
| train/                  |            |
|    approx_kl            | 0.67522544 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.078     |
|    explained_variance   | 0.537      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.144      |
|    n_updates            | 13930      |
|    policy_gradient_loss | -0.0347    |
|    value_loss           | 0.48       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.64        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 419         |
|    time_elapsed         | 4011        |
|    total_timesteps      | 858112      |
| train/                  |             |
|    approx_kl            | 0.042408757 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.139       |
|    n_updates            | 13940       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.651       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=860000, episode_reward=2.35 +/- 7.35
Episode length: 30.33 +/- 14.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.3       |
|    mean_reward          | 2.35       |
| time/                   |            |
|    total_timesteps      | 860000     |
| train/                  |            |
|    approx_kl            | 0.06298959 |
|    clip_fraction        | 0.0816     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0968    |
|    explained_variance   | 0.703      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0794     |
|    n_updates            | 13950      |
|    policy_gradient_loss | -0.0199    |
|    value_loss           | 0.419      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 420      |
|    time_elapsed    | 4022     |
|    total_timesteps | 860160   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 8.09       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 421        |
|    time_elapsed         | 4035       |
|    total_timesteps      | 862208     |
| train/                  |            |
|    approx_kl            | 0.02852312 |
|    clip_fraction        | 0.0574     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0936    |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0748     |
|    n_updates            | 13960      |
|    policy_gradient_loss | -0.0125    |
|    value_loss           | 0.326      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 422         |
|    time_elapsed         | 4046        |
|    total_timesteps      | 864256      |
| train/                  |             |
|    approx_kl            | 0.093530245 |
|    clip_fraction        | 0.0638      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0804     |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.125       |
|    n_updates            | 13970       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.3         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 423         |
|    time_elapsed         | 4058        |
|    total_timesteps      | 866304      |
| train/                  |             |
|    approx_kl            | 0.060275972 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0948     |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 13980       |
|    policy_gradient_loss | 0.0376      |
|    value_loss           | 0.493       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 424         |
|    time_elapsed         | 4069        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.032870572 |
|    clip_fraction        | 0.0594      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0829     |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0775      |
|    n_updates            | 13990       |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.283       |
-----------------------------------------
Eval num_timesteps=870000, episode_reward=8.74 +/- 1.45
Episode length: 18.00 +/- 2.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 8.74        |
| time/                   |             |
|    total_timesteps      | 870000      |
| train/                  |             |
|    approx_kl            | 0.032952093 |
|    clip_fraction        | 0.0661      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0882     |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0551      |
|    n_updates            | 14000       |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.267       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 7.83     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 425      |
|    time_elapsed    | 4080     |
|    total_timesteps | 870400   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.57        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 426         |
|    time_elapsed         | 4091        |
|    total_timesteps      | 872448      |
| train/                  |             |
|    approx_kl            | 0.059534393 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0793      |
|    n_updates            | 14010       |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.301       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.2      |
|    ep_rew_mean          | 7.54      |
| time/                   |           |
|    fps                  | 213       |
|    iterations           | 427       |
|    time_elapsed         | 4101      |
|    total_timesteps      | 874496    |
| train/                  |           |
|    approx_kl            | 0.1426501 |
|    clip_fraction        | 0.0882    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.117    |
|    explained_variance   | 0.707     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0624    |
|    n_updates            | 14020     |
|    policy_gradient_loss | -0.022    |
|    value_loss           | 0.39      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.8       |
|    ep_rew_mean          | 8.15       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 428        |
|    time_elapsed         | 4111       |
|    total_timesteps      | 876544     |
| train/                  |            |
|    approx_kl            | 0.08144711 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.589      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0915     |
|    n_updates            | 14030      |
|    policy_gradient_loss | -0.0307    |
|    value_loss           | 0.449      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 429        |
|    time_elapsed         | 4121       |
|    total_timesteps      | 878592     |
| train/                  |            |
|    approx_kl            | 0.03722285 |
|    clip_fraction        | 0.0566     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0752    |
|    explained_variance   | 0.797      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.103      |
|    n_updates            | 14040      |
|    policy_gradient_loss | -0.0126    |
|    value_loss           | 0.24       |
----------------------------------------
Eval num_timesteps=880000, episode_reward=9.55 +/- 0.74
Episode length: 17.33 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 9.55        |
| time/                   |             |
|    total_timesteps      | 880000      |
| train/                  |             |
|    approx_kl            | 0.018736817 |
|    clip_fraction        | 0.0608      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0894     |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.225       |
|    n_updates            | 14050       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.449       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 8.1      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 430      |
|    time_elapsed    | 4132     |
|    total_timesteps | 880640   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 431         |
|    time_elapsed         | 4142        |
|    total_timesteps      | 882688      |
| train/                  |             |
|    approx_kl            | 0.062206388 |
|    clip_fraction        | 0.0495      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0487     |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.065       |
|    n_updates            | 14060       |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.224       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 432         |
|    time_elapsed         | 4152        |
|    total_timesteps      | 884736      |
| train/                  |             |
|    approx_kl            | 0.048777554 |
|    clip_fraction        | 0.0596      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0752     |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.252       |
|    n_updates            | 14070       |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.306       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.98       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 433        |
|    time_elapsed         | 4159       |
|    total_timesteps      | 886784     |
| train/                  |            |
|    approx_kl            | 0.18507102 |
|    clip_fraction        | 0.0958     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0821    |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.177      |
|    n_updates            | 14080      |
|    policy_gradient_loss | 0.00312    |
|    value_loss           | 0.517      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.97        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 434         |
|    time_elapsed         | 4166        |
|    total_timesteps      | 888832      |
| train/                  |             |
|    approx_kl            | 0.037899565 |
|    clip_fraction        | 0.0625      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0668     |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0458      |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.249       |
-----------------------------------------
Eval num_timesteps=890000, episode_reward=8.51 +/- 0.18
Episode length: 16.00 +/- 0.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 8.51       |
| time/                   |            |
|    total_timesteps      | 890000     |
| train/                  |            |
|    approx_kl            | 0.04917945 |
|    clip_fraction        | 0.0734     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0833    |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0545     |
|    n_updates            | 14100      |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.332      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 435      |
|    time_elapsed    | 4174     |
|    total_timesteps | 890880   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 436         |
|    time_elapsed         | 4181        |
|    total_timesteps      | 892928      |
| train/                  |             |
|    approx_kl            | 0.039833393 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0853     |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0453      |
|    n_updates            | 14110       |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.242       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.61       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 437        |
|    time_elapsed         | 4190       |
|    total_timesteps      | 894976     |
| train/                  |            |
|    approx_kl            | 0.11365968 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.399      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.367      |
|    n_updates            | 14120      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.904      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 438        |
|    time_elapsed         | 4198       |
|    total_timesteps      | 897024     |
| train/                  |            |
|    approx_kl            | 0.04757078 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.129     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.136      |
|    n_updates            | 14130      |
|    policy_gradient_loss | -0.021     |
|    value_loss           | 0.598      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 439         |
|    time_elapsed         | 4206        |
|    total_timesteps      | 899072      |
| train/                  |             |
|    approx_kl            | 0.057076167 |
|    clip_fraction        | 0.0752      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 14140       |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.402       |
-----------------------------------------
Eval num_timesteps=900000, episode_reward=9.41 +/- 0.79
Episode length: 18.00 +/- 0.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18         |
|    mean_reward          | 9.41       |
| time/                   |            |
|    total_timesteps      | 900000     |
| train/                  |            |
|    approx_kl            | 0.12652092 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.137      |
|    n_updates            | 14150      |
|    policy_gradient_loss | -0.0199    |
|    value_loss           | 0.41       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 213      |
|    iterations      | 440      |
|    time_elapsed    | 4216     |
|    total_timesteps | 901120   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 441         |
|    time_elapsed         | 4224        |
|    total_timesteps      | 903168      |
| train/                  |             |
|    approx_kl            | 0.036732107 |
|    clip_fraction        | 0.0978      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0506      |
|    n_updates            | 14160       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.566       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 442         |
|    time_elapsed         | 4231        |
|    total_timesteps      | 905216      |
| train/                  |             |
|    approx_kl            | 0.039744746 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0898      |
|    n_updates            | 14170       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.567       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.4      |
|    ep_rew_mean          | 7.81      |
| time/                   |           |
|    fps                  | 214       |
|    iterations           | 443       |
|    time_elapsed         | 4238      |
|    total_timesteps      | 907264    |
| train/                  |           |
|    approx_kl            | 0.0350268 |
|    clip_fraction        | 0.111     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.179    |
|    explained_variance   | 0.669     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.145     |
|    n_updates            | 14180     |
|    policy_gradient_loss | -0.0187   |
|    value_loss           | 0.616     |
---------------------------------------
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 18.9     |
|    ep_rew_mean          | 7.55     |
| time/                   |          |
|    fps                  | 214      |
|    iterations           | 444      |
|    time_elapsed         | 4248     |
|    total_timesteps      | 909312   |
| train/                  |          |
|    approx_kl            | 0.060838 |
|    clip_fraction        | 0.12     |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.171   |
|    explained_variance   | 0.72     |
|    learning_rate        | 0.0003   |
|    loss                 | 0.131    |
|    n_updates            | 14190    |
|    policy_gradient_loss | -0.0204  |
|    value_loss           | 0.425    |
--------------------------------------
Eval num_timesteps=910000, episode_reward=8.67 +/- 0.69
Episode length: 18.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 8.67       |
| time/                   |            |
|    total_timesteps      | 910000     |
| train/                  |            |
|    approx_kl            | 0.04174981 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.452      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0728     |
|    n_updates            | 14200      |
|    policy_gradient_loss | -0.0305    |
|    value_loss           | 0.454      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 214      |
|    iterations      | 445      |
|    time_elapsed    | 4257     |
|    total_timesteps | 911360   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 446         |
|    time_elapsed         | 4268        |
|    total_timesteps      | 913408      |
| train/                  |             |
|    approx_kl            | 0.030651433 |
|    clip_fraction        | 0.0787      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 14210       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.498       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 447         |
|    time_elapsed         | 4278        |
|    total_timesteps      | 915456      |
| train/                  |             |
|    approx_kl            | 0.038761668 |
|    clip_fraction        | 0.0999      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.176       |
|    n_updates            | 14220       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.366       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.07       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 448        |
|    time_elapsed         | 4285       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.03244072 |
|    clip_fraction        | 0.079      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0837     |
|    n_updates            | 14230      |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 0.374      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 449         |
|    time_elapsed         | 4293        |
|    total_timesteps      | 919552      |
| train/                  |             |
|    approx_kl            | 0.033156976 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.116       |
|    n_updates            | 14240       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.383       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=920000, episode_reward=3.92 +/- 5.60
Episode length: 32.33 +/- 13.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 32.3        |
|    mean_reward          | 3.92        |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.058010112 |
|    clip_fraction        | 0.0996      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0532      |
|    n_updates            | 14250       |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.352       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 8.02     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 450      |
|    time_elapsed    | 4301     |
|    total_timesteps | 921600   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.39        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 451         |
|    time_elapsed         | 4308        |
|    total_timesteps      | 923648      |
| train/                  |             |
|    approx_kl            | 0.041485175 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0461      |
|    n_updates            | 14260       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.342       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 452         |
|    time_elapsed         | 4315        |
|    total_timesteps      | 925696      |
| train/                  |             |
|    approx_kl            | 0.020921547 |
|    clip_fraction        | 0.0617      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0995     |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0796      |
|    n_updates            | 14270       |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.287       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.83        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 453         |
|    time_elapsed         | 4322        |
|    total_timesteps      | 927744      |
| train/                  |             |
|    approx_kl            | 0.028920699 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.081       |
|    n_updates            | 14280       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.24        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 454        |
|    time_elapsed         | 4329       |
|    total_timesteps      | 929792     |
| train/                  |            |
|    approx_kl            | 0.10412754 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.105     |
|    explained_variance   | 0.395      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.103      |
|    n_updates            | 14290      |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.504      |
----------------------------------------
Eval num_timesteps=930000, episode_reward=9.63 +/- 0.63
Episode length: 17.00 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 9.63        |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.045940176 |
|    clip_fraction        | 0.0681      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0961     |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.24        |
|    n_updates            | 14300       |
|    policy_gradient_loss | -0.0113     |
|    value_loss           | 0.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 8.05     |
| time/              |          |
|    fps             | 214      |
|    iterations      | 455      |
|    time_elapsed    | 4337     |
|    total_timesteps | 931840   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.54        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 456         |
|    time_elapsed         | 4344        |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.044592805 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0676     |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0595      |
|    n_updates            | 14310       |
|    policy_gradient_loss | -0.0089     |
|    value_loss           | 0.273       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.77       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 457        |
|    time_elapsed         | 4351       |
|    total_timesteps      | 935936     |
| train/                  |            |
|    approx_kl            | 0.10183975 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.138     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0842     |
|    n_updates            | 14320      |
|    policy_gradient_loss | -0.0293    |
|    value_loss           | 0.56       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.95       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 458        |
|    time_elapsed         | 4360       |
|    total_timesteps      | 937984     |
| train/                  |            |
|    approx_kl            | 0.03895863 |
|    clip_fraction        | 0.0861     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.092      |
|    n_updates            | 14330      |
|    policy_gradient_loss | -0.0142    |
|    value_loss           | 0.47       |
----------------------------------------
Eval num_timesteps=940000, episode_reward=7.32 +/- 1.53
Episode length: 15.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 7.32        |
| time/                   |             |
|    total_timesteps      | 940000      |
| train/                  |             |
|    approx_kl            | 0.030479446 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 14340       |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.483       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 8.13     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 459      |
|    time_elapsed    | 4367     |
|    total_timesteps | 940032   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 460        |
|    time_elapsed         | 4376       |
|    total_timesteps      | 942080     |
| train/                  |            |
|    approx_kl            | 0.05800181 |
|    clip_fraction        | 0.0733     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0737    |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.123      |
|    n_updates            | 14350      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.233      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 461         |
|    time_elapsed         | 4383        |
|    total_timesteps      | 944128      |
| train/                  |             |
|    approx_kl            | 0.036369644 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.097      |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.1         |
|    n_updates            | 14360       |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.302       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 462        |
|    time_elapsed         | 4390       |
|    total_timesteps      | 946176     |
| train/                  |            |
|    approx_kl            | 0.03974487 |
|    clip_fraction        | 0.0842     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0949     |
|    n_updates            | 14370      |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 0.348      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.23        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 463         |
|    time_elapsed         | 4398        |
|    total_timesteps      | 948224      |
| train/                  |             |
|    approx_kl            | 0.051107377 |
|    clip_fraction        | 0.0939      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.177       |
|    n_updates            | 14380       |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.536       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=950000, episode_reward=7.55 +/- 1.47
Episode length: 17.33 +/- 1.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 7.55        |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.038738046 |
|    clip_fraction        | 0.0837      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0953      |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 0.338       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.89     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 464      |
|    time_elapsed    | 4405     |
|    total_timesteps | 950272   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 465        |
|    time_elapsed         | 4411       |
|    total_timesteps      | 952320     |
| train/                  |            |
|    approx_kl            | 0.07565188 |
|    clip_fraction        | 0.0919     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.109     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.13       |
|    n_updates            | 14400      |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.444      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8.28       |
| time/                   |            |
|    fps                  | 215        |
|    iterations           | 466        |
|    time_elapsed         | 4418       |
|    total_timesteps      | 954368     |
| train/                  |            |
|    approx_kl            | 0.05993975 |
|    clip_fraction        | 0.0873     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.258      |
|    n_updates            | 14410      |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.521      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.3         |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 467         |
|    time_elapsed         | 4425        |
|    total_timesteps      | 956416      |
| train/                  |             |
|    approx_kl            | 0.053550478 |
|    clip_fraction        | 0.0675      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0877     |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 14420       |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.316       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 8.13       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 468        |
|    time_elapsed         | 4432       |
|    total_timesteps      | 958464     |
| train/                  |            |
|    approx_kl            | 0.04834359 |
|    clip_fraction        | 0.0733     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.081     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.03       |
|    n_updates            | 14430      |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 0.19       |
----------------------------------------
Eval num_timesteps=960000, episode_reward=8.15 +/- 0.27
Episode length: 17.67 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.15       |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.04111079 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.133      |
|    n_updates            | 14440      |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 0.318      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.95     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 469      |
|    time_elapsed    | 4440     |
|    total_timesteps | 960512   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 470        |
|    time_elapsed         | 4447       |
|    total_timesteps      | 962560     |
| train/                  |            |
|    approx_kl            | 0.06945171 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0874     |
|    n_updates            | 14450      |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.351      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8.06       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 471        |
|    time_elapsed         | 4454       |
|    total_timesteps      | 964608     |
| train/                  |            |
|    approx_kl            | 0.12622176 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.598      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0511     |
|    n_updates            | 14460      |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.427      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 8.12       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 472        |
|    time_elapsed         | 4461       |
|    total_timesteps      | 966656     |
| train/                  |            |
|    approx_kl            | 0.05166225 |
|    clip_fraction        | 0.0788     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.111     |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 14470      |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 0.34       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 473         |
|    time_elapsed         | 4468        |
|    total_timesteps      | 968704      |
| train/                  |             |
|    approx_kl            | 0.069308534 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0849     |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.202       |
|    n_updates            | 14480       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.467       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=970000, episode_reward=9.05 +/- 0.56
Episode length: 19.67 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 9.05        |
| time/                   |             |
|    total_timesteps      | 970000      |
| train/                  |             |
|    approx_kl            | 0.024303092 |
|    clip_fraction        | 0.0728      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0847      |
|    n_updates            | 14490       |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.306       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.69     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 474      |
|    time_elapsed    | 4475     |
|    total_timesteps | 970752   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 8.07       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 475        |
|    time_elapsed         | 4482       |
|    total_timesteps      | 972800     |
| train/                  |            |
|    approx_kl            | 0.05628094 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.146     |
|    explained_variance   | 0.669      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.237      |
|    n_updates            | 14500      |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.69       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 476         |
|    time_elapsed         | 4489        |
|    total_timesteps      | 974848      |
| train/                  |             |
|    approx_kl            | 0.033012778 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.69        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.168       |
|    n_updates            | 14510       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.509       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.94       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 477        |
|    time_elapsed         | 4496       |
|    total_timesteps      | 976896     |
| train/                  |            |
|    approx_kl            | 0.03571956 |
|    clip_fraction        | 0.0755     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.175      |
|    n_updates            | 14520      |
|    policy_gradient_loss | -0.00757   |
|    value_loss           | 0.652      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 478         |
|    time_elapsed         | 4503        |
|    total_timesteps      | 978944      |
| train/                  |             |
|    approx_kl            | 0.054226097 |
|    clip_fraction        | 0.0955      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.538       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0988      |
|    n_updates            | 14530       |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.411       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=980000, episode_reward=2.86 +/- 6.27
Episode length: 28.00 +/- 15.90
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | 2.86        |
| time/                   |             |
|    total_timesteps      | 980000      |
| train/                  |             |
|    approx_kl            | 0.043066267 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0929      |
|    n_updates            | 14540       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.472       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.03     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 479      |
|    time_elapsed    | 4510     |
|    total_timesteps | 980992   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.92       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 480        |
|    time_elapsed         | 4519       |
|    total_timesteps      | 983040     |
| train/                  |            |
|    approx_kl            | 0.03376788 |
|    clip_fraction        | 0.0718     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.1       |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0628     |
|    n_updates            | 14550      |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.418      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.9        |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 481        |
|    time_elapsed         | 4529       |
|    total_timesteps      | 985088     |
| train/                  |            |
|    approx_kl            | 0.04578287 |
|    clip_fraction        | 0.0936     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.122      |
|    n_updates            | 14560      |
|    policy_gradient_loss | -0.0159    |
|    value_loss           | 0.428      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.06       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 482        |
|    time_elapsed         | 4539       |
|    total_timesteps      | 987136     |
| train/                  |            |
|    approx_kl            | 0.06891951 |
|    clip_fraction        | 0.0815     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.105     |
|    explained_variance   | 0.734      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.076      |
|    n_updates            | 14570      |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.353      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.98       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 483        |
|    time_elapsed         | 4549       |
|    total_timesteps      | 989184     |
| train/                  |            |
|    approx_kl            | 0.03695732 |
|    clip_fraction        | 0.0656     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0751    |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0285     |
|    n_updates            | 14580      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.247      |
----------------------------------------
reached max steps=100
Eval num_timesteps=990000, episode_reward=9.49 +/- 0.21
Episode length: 20.67 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.7        |
|    mean_reward          | 9.49        |
| time/                   |             |
|    total_timesteps      | 990000      |
| train/                  |             |
|    approx_kl            | 0.024837777 |
|    clip_fraction        | 0.0825      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 14590       |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.451       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.14     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 484      |
|    time_elapsed    | 4560     |
|    total_timesteps | 991232   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.81        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 485         |
|    time_elapsed         | 4569        |
|    total_timesteps      | 993280      |
| train/                  |             |
|    approx_kl            | 0.048962556 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.087      |
|    explained_variance   | 0.685       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0979      |
|    n_updates            | 14600       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.423       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 486         |
|    time_elapsed         | 4579        |
|    total_timesteps      | 995328      |
| train/                  |             |
|    approx_kl            | 0.032547675 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.185       |
|    n_updates            | 14610       |
|    policy_gradient_loss | -0.024      |
|    value_loss           | 0.576       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 8.23       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 487        |
|    time_elapsed         | 4590       |
|    total_timesteps      | 997376     |
| train/                  |            |
|    approx_kl            | 0.04706995 |
|    clip_fraction        | 0.0723     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0899    |
|    explained_variance   | 0.764      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.137      |
|    n_updates            | 14620      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 0.326      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 488         |
|    time_elapsed         | 4600        |
|    total_timesteps      | 999424      |
| train/                  |             |
|    approx_kl            | 0.028595382 |
|    clip_fraction        | 0.0563      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0698     |
|    explained_variance   | 0.816       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0551      |
|    n_updates            | 14630       |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.229       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=1000000, episode_reward=7.55 +/- 0.73
Episode length: 17.33 +/- 2.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 7.55        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.041623726 |
|    clip_fraction        | 0.07        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0771     |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0306      |
|    n_updates            | 14640       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.249       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.98     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 489      |
|    time_elapsed    | 4611     |
|    total_timesteps | 1001472  |
---------------------------------
