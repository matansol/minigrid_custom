Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Loaded model from models\2,2,2,-3,0.2Steps100Grid8_20241230\best_model. Continuing training.
Logging to ./logs/ppo/minigrid_custom_tensorboard/20241231_1
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x0000020035CC0BB0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002002F7B9180>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 7.86     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 1        |
|    time_elapsed    | 9        |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 8.3         |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 2           |
|    time_elapsed         | 18          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.038164917 |
|    clip_fraction        | 0.0805      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0923     |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 13130       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.303       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 3           |
|    time_elapsed         | 27          |
|    total_timesteps      | 6144        |
| train/                  |             |
|    approx_kl            | 0.028668452 |
|    clip_fraction        | 0.0591      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0614     |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.036       |
|    n_updates            | 13140       |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.215       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.54        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 4           |
|    time_elapsed         | 35          |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.032982156 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0942      |
|    n_updates            | 13150       |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.361       |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=7.25 +/- 1.47
Episode length: 15.67 +/- 1.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 7.25        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.049236953 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0694      |
|    n_updates            | 13160       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.451       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.5     |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 5        |
|    time_elapsed    | 44       |
|    total_timesteps | 10240    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.01       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 6          |
|    time_elapsed         | 53         |
|    total_timesteps      | 12288      |
| train/                  |            |
|    approx_kl            | 0.03256617 |
|    clip_fraction        | 0.0927     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0738     |
|    n_updates            | 13170      |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 0.331      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.73       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 7          |
|    time_elapsed         | 61         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.02400805 |
|    clip_fraction        | 0.0834     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.131     |
|    explained_variance   | 0.7        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.014      |
|    n_updates            | 13180      |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.368      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.8       |
|    ep_rew_mean          | 7.99       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 8          |
|    time_elapsed         | 70         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.03268413 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 13190      |
|    policy_gradient_loss | -0.0186    |
|    value_loss           | 0.617      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 9           |
|    time_elapsed         | 83          |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.025868574 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.103       |
|    n_updates            | 13200       |
|    policy_gradient_loss | -0.00656    |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=8.52 +/- 1.01
Episode length: 19.00 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 8.52        |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.032328926 |
|    clip_fraction        | 0.0965      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.005       |
|    n_updates            | 13210       |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.267       |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 8.03     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 10       |
|    time_elapsed    | 96       |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 11          |
|    time_elapsed         | 114         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.020125343 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0975      |
|    n_updates            | 13220       |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.283       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.53        |
| time/                   |             |
|    fps                  | 185         |
|    iterations           | 12          |
|    time_elapsed         | 132         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.043398857 |
|    clip_fraction        | 0.0872      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.158      |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.061       |
|    n_updates            | 13230       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.348       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.22        |
| time/                   |             |
|    fps                  | 176         |
|    iterations           | 13          |
|    time_elapsed         | 151         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.027802505 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 13240       |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.541       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.97        |
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 14          |
|    time_elapsed         | 170         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.022277044 |
|    clip_fraction        | 0.0878      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0587      |
|    n_updates            | 13250       |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.241       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=30000, episode_reward=0.03 +/- 5.69
Episode length: 39.33 +/- 15.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.3        |
|    mean_reward          | 0.0253      |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.032484192 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.125       |
|    n_updates            | 13260       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.407       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 161      |
|    iterations      | 15       |
|    time_elapsed    | 190      |
|    total_timesteps | 30720    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 156         |
|    iterations           | 16          |
|    time_elapsed         | 209         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.034574334 |
|    clip_fraction        | 0.0914      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 13270       |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.5         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 151         |
|    iterations           | 17          |
|    time_elapsed         | 230         |
|    total_timesteps      | 34816       |
| train/                  |             |
|    approx_kl            | 0.036431905 |
|    clip_fraction        | 0.0823      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.131       |
|    n_updates            | 13280       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.383       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 147         |
|    iterations           | 18          |
|    time_elapsed         | 250         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.033821404 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.167      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0779      |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.377       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.5      |
|    ep_rew_mean          | 8.03      |
| time/                   |           |
|    fps                  | 148       |
|    iterations           | 19        |
|    time_elapsed         | 261       |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.0414517 |
|    clip_fraction        | 0.106     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.151    |
|    explained_variance   | 0.736     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.237     |
|    n_updates            | 13300     |
|    policy_gradient_loss | -0.0257   |
|    value_loss           | 0.449     |
---------------------------------------
reached max steps=100
Eval num_timesteps=40000, episode_reward=1.01 +/- 7.78
Episode length: 27.33 +/- 16.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.3        |
|    mean_reward          | 1.01        |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.023526702 |
|    clip_fraction        | 0.0858      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0566      |
|    n_updates            | 13310       |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.308       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 150      |
|    iterations      | 20       |
|    time_elapsed    | 272      |
|    total_timesteps | 40960    |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 150         |
|    iterations           | 21          |
|    time_elapsed         | 285         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.045519814 |
|    clip_fraction        | 0.0959      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.721       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.124       |
|    n_updates            | 13320       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.423       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 22         |
|    time_elapsed         | 298        |
|    total_timesteps      | 45056      |
| train/                  |            |
|    approx_kl            | 0.05281188 |
|    clip_fraction        | 0.0989     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.146     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0671     |
|    n_updates            | 13330      |
|    policy_gradient_loss | -0.0205    |
|    value_loss           | 0.504      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.99       |
| time/                   |            |
|    fps                  | 151        |
|    iterations           | 23         |
|    time_elapsed         | 310        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.06568584 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.242      |
|    n_updates            | 13340      |
|    policy_gradient_loss | -0.019     |
|    value_loss           | 0.757      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 152         |
|    iterations           | 24          |
|    time_elapsed         | 322         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.035503272 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.115      |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.316       |
|    n_updates            | 13350       |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.513       |
-----------------------------------------
Eval num_timesteps=50000, episode_reward=7.90 +/- 1.85
Episode length: 20.33 +/- 3.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20.3       |
|    mean_reward          | 7.9        |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.04881949 |
|    clip_fraction        | 0.0872     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.111      |
|    n_updates            | 13360      |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 0.428      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 7.98     |
| time/              |          |
|    fps             | 152      |
|    iterations      | 25       |
|    time_elapsed    | 335      |
|    total_timesteps | 51200    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 7.15       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 26         |
|    time_elapsed         | 346        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.03919927 |
|    clip_fraction        | 0.0771     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0596     |
|    n_updates            | 13370      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.284      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.89       |
| time/                   |            |
|    fps                  | 153        |
|    iterations           | 27         |
|    time_elapsed         | 360        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.04193235 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.181     |
|    explained_variance   | 0.601      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.167      |
|    n_updates            | 13380      |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.63       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 28          |
|    time_elapsed         | 370         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.031159092 |
|    clip_fraction        | 0.0886      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.297       |
|    n_updates            | 13390       |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.444       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 29          |
|    time_elapsed         | 383         |
|    total_timesteps      | 59392       |
| train/                  |             |
|    approx_kl            | 0.039590493 |
|    clip_fraction        | 0.105       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 13400       |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.487       |
-----------------------------------------
Eval num_timesteps=60000, episode_reward=7.36 +/- 0.90
Episode length: 24.33 +/- 8.34
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 24.3       |
|    mean_reward          | 7.36       |
| time/                   |            |
|    total_timesteps      | 60000      |
| train/                  |            |
|    approx_kl            | 0.03193789 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.76       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0399     |
|    n_updates            | 13410      |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.29       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 155      |
|    iterations      | 30       |
|    time_elapsed    | 396      |
|    total_timesteps | 61440    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.64       |
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 31         |
|    time_elapsed         | 410        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.03190907 |
|    clip_fraction        | 0.0902     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.142     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0615     |
|    n_updates            | 13420      |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.321      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.97        |
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 32          |
|    time_elapsed         | 426         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.034508206 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.143      |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0944      |
|    n_updates            | 13430       |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.411       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 33          |
|    time_elapsed         | 440         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.046422355 |
|    clip_fraction        | 0.0903      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.066       |
|    n_updates            | 13440       |
|    policy_gradient_loss | -0.00914    |
|    value_loss           | 0.334       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 153         |
|    iterations           | 34          |
|    time_elapsed         | 453         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.041005258 |
|    clip_fraction        | 0.0813      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.789       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0356      |
|    n_updates            | 13450       |
|    policy_gradient_loss | -0.0204     |
|    value_loss           | 0.267       |
-----------------------------------------
Eval num_timesteps=70000, episode_reward=8.29 +/- 0.18
Episode length: 17.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 8.29        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.027301967 |
|    clip_fraction        | 0.0526      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0798     |
|    explained_variance   | 0.848       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0759      |
|    n_updates            | 13460       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.195       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.08     |
| time/              |          |
|    fps             | 153      |
|    iterations      | 35       |
|    time_elapsed    | 466      |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 36          |
|    time_elapsed         | 477         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.052881714 |
|    clip_fraction        | 0.0783      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0999     |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0543      |
|    n_updates            | 13470       |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.249       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.97        |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 37          |
|    time_elapsed         | 489         |
|    total_timesteps      | 75776       |
| train/                  |             |
|    approx_kl            | 0.034899104 |
|    clip_fraction        | 0.0649      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0919     |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0623      |
|    n_updates            | 13480       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.229       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 8.36       |
| time/                   |            |
|    fps                  | 155        |
|    iterations           | 38         |
|    time_elapsed         | 501        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.07468322 |
|    clip_fraction        | 0.0842     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0957    |
|    explained_variance   | 0.686      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0531     |
|    n_updates            | 13490      |
|    policy_gradient_loss | -0.0222    |
|    value_loss           | 0.426      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 39          |
|    time_elapsed         | 513         |
|    total_timesteps      | 79872       |
| train/                  |             |
|    approx_kl            | 0.035934344 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0927     |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0734      |
|    n_updates            | 13500       |
|    policy_gradient_loss | -0.0144     |
|    value_loss           | 0.269       |
-----------------------------------------
Eval num_timesteps=80000, episode_reward=7.65 +/- 0.81
Episode length: 23.00 +/- 4.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23          |
|    mean_reward          | 7.65        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.037529852 |
|    clip_fraction        | 0.0753      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0949     |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.1         |
|    n_updates            | 13510       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.334       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.71     |
| time/              |          |
|    fps             | 156      |
|    iterations      | 40       |
|    time_elapsed    | 523      |
|    total_timesteps | 81920    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 157         |
|    iterations           | 41          |
|    time_elapsed         | 532         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.056377247 |
|    clip_fraction        | 0.0945      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0681      |
|    n_updates            | 13520       |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.375       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 42          |
|    time_elapsed         | 542         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.045295723 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0862     |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0779      |
|    n_updates            | 13530       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.184       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 43          |
|    time_elapsed         | 554         |
|    total_timesteps      | 88064       |
| train/                  |             |
|    approx_kl            | 0.031234536 |
|    clip_fraction        | 0.0819      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0509      |
|    n_updates            | 13540       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.487       |
-----------------------------------------
Eval num_timesteps=90000, episode_reward=8.02 +/- 0.89
Episode length: 21.33 +/- 3.77
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.3        |
|    mean_reward          | 8.02        |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.053128183 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0433      |
|    n_updates            | 13550       |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.347       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 8.19     |
| time/              |          |
|    fps             | 158      |
|    iterations      | 44       |
|    time_elapsed    | 567      |
|    total_timesteps | 90112    |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.96        |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 45          |
|    time_elapsed         | 580         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.031456687 |
|    clip_fraction        | 0.0655      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0809     |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0502      |
|    n_updates            | 13560       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.258       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 46         |
|    time_elapsed         | 591        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.06146822 |
|    clip_fraction        | 0.0794     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0796    |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 13570      |
|    policy_gradient_loss | -0.0202    |
|    value_loss           | 0.357      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 47         |
|    time_elapsed         | 602        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.03614477 |
|    clip_fraction        | 0.089      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0985    |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0689     |
|    n_updates            | 13580      |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.342      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 160         |
|    iterations           | 48          |
|    time_elapsed         | 613         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.037428953 |
|    clip_fraction        | 0.0676      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0892     |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0669      |
|    n_updates            | 13590       |
|    policy_gradient_loss | -0.0199     |
|    value_loss           | 0.285       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=100000, episode_reward=7.79 +/- 1.57
Episode length: 19.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19.3       |
|    mean_reward          | 7.79       |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.03594847 |
|    clip_fraction        | 0.0666     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0787    |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00446    |
|    n_updates            | 13600      |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.21       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 160      |
|    iterations      | 49       |
|    time_elapsed    | 623      |
|    total_timesteps | 100352   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 161         |
|    iterations           | 50          |
|    time_elapsed         | 634         |
|    total_timesteps      | 102400      |
| train/                  |             |
|    approx_kl            | 0.036681302 |
|    clip_fraction        | 0.0721      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0954     |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0643      |
|    n_updates            | 13610       |
|    policy_gradient_loss | -0.00948    |
|    value_loss           | 0.306       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.73       |
| time/                   |            |
|    fps                  | 161        |
|    iterations           | 51         |
|    time_elapsed         | 645        |
|    total_timesteps      | 104448     |
| train/                  |            |
|    approx_kl            | 0.04490765 |
|    clip_fraction        | 0.0786     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0838    |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0311     |
|    n_updates            | 13620      |
|    policy_gradient_loss | -0.00653   |
|    value_loss           | 0.278      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 162        |
|    iterations           | 52         |
|    time_elapsed         | 656        |
|    total_timesteps      | 106496     |
| train/                  |            |
|    approx_kl            | 0.12791306 |
|    clip_fraction        | 0.0864     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0926    |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00732    |
|    n_updates            | 13630      |
|    policy_gradient_loss | -0.0279    |
|    value_loss           | 0.318      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.21       |
| time/                   |            |
|    fps                  | 162        |
|    iterations           | 53         |
|    time_elapsed         | 668        |
|    total_timesteps      | 108544     |
| train/                  |            |
|    approx_kl            | 0.08347006 |
|    clip_fraction        | 0.0979     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0999    |
|    explained_variance   | 0.66       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0559     |
|    n_updates            | 13640      |
|    policy_gradient_loss | -0.0267    |
|    value_loss           | 0.379      |
----------------------------------------
Eval num_timesteps=110000, episode_reward=7.18 +/- 1.59
Episode length: 16.00 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16          |
|    mean_reward          | 7.18        |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.032123104 |
|    clip_fraction        | 0.0545      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0624     |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.099       |
|    n_updates            | 13650       |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 0.208       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.97     |
| time/              |          |
|    fps             | 162      |
|    iterations      | 54       |
|    time_elapsed    | 679      |
|    total_timesteps | 110592   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.05        |
| time/                   |             |
|    fps                  | 163         |
|    iterations           | 55          |
|    time_elapsed         | 690         |
|    total_timesteps      | 112640      |
| train/                  |             |
|    approx_kl            | 0.029713817 |
|    clip_fraction        | 0.0629      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.068      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0581      |
|    n_updates            | 13660       |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.249       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 163         |
|    iterations           | 56          |
|    time_elapsed         | 701         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.033131257 |
|    clip_fraction        | 0.0696      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0756     |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0598      |
|    n_updates            | 13670       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.289       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.45       |
| time/                   |            |
|    fps                  | 163        |
|    iterations           | 57         |
|    time_elapsed         | 713        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.03835263 |
|    clip_fraction        | 0.0729     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0872    |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.112      |
|    n_updates            | 13680      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.28       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 163         |
|    iterations           | 58          |
|    time_elapsed         | 724         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.035673127 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0689      |
|    n_updates            | 13690       |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 0.511       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=120000, episode_reward=3.60 +/- 5.40
Episode length: 27.67 +/- 15.84
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.7        |
|    mean_reward          | 3.6         |
| time/                   |             |
|    total_timesteps      | 120000      |
| train/                  |             |
|    approx_kl            | 0.053069144 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0395      |
|    n_updates            | 13700       |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.396       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 8.07     |
| time/              |          |
|    fps             | 164      |
|    iterations      | 59       |
|    time_elapsed    | 735      |
|    total_timesteps | 120832   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 7.88       |
| time/                   |            |
|    fps                  | 164        |
|    iterations           | 60         |
|    time_elapsed         | 747        |
|    total_timesteps      | 122880     |
| train/                  |            |
|    approx_kl            | 0.07111254 |
|    clip_fraction        | 0.0748     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0685    |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0801     |
|    n_updates            | 13710      |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 0.244      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 164        |
|    iterations           | 61         |
|    time_elapsed         | 758        |
|    total_timesteps      | 124928     |
| train/                  |            |
|    approx_kl            | 0.08045333 |
|    clip_fraction        | 0.0875     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.701      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 13720      |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.313      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 8.05        |
| time/                   |             |
|    fps                  | 164         |
|    iterations           | 62          |
|    time_elapsed         | 769         |
|    total_timesteps      | 126976      |
| train/                  |             |
|    approx_kl            | 0.058076814 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0787      |
|    n_updates            | 13730       |
|    policy_gradient_loss | -0.0203     |
|    value_loss           | 0.559       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 165        |
|    iterations           | 63         |
|    time_elapsed         | 781        |
|    total_timesteps      | 129024     |
| train/                  |            |
|    approx_kl            | 0.14761019 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0762     |
|    n_updates            | 13740      |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 0.478      |
----------------------------------------
Eval num_timesteps=130000, episode_reward=8.15 +/- 0.93
Episode length: 17.67 +/- 3.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.15       |
| time/                   |            |
|    total_timesteps      | 130000     |
| train/                  |            |
|    approx_kl            | 0.09717073 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.122      |
|    n_updates            | 13750      |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.515      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 165      |
|    iterations      | 64       |
|    time_elapsed    | 792      |
|    total_timesteps | 131072   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 165         |
|    iterations           | 65          |
|    time_elapsed         | 804         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.049996726 |
|    clip_fraction        | 0.0709      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0824     |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0987      |
|    n_updates            | 13760       |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.3         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.23       |
| time/                   |            |
|    fps                  | 165        |
|    iterations           | 66         |
|    time_elapsed         | 816        |
|    total_timesteps      | 135168     |
| train/                  |            |
|    approx_kl            | 0.06858739 |
|    clip_fraction        | 0.0864     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.111     |
|    explained_variance   | 0.691      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.341      |
|    n_updates            | 13770      |
|    policy_gradient_loss | -0.0143    |
|    value_loss           | 0.493      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 165         |
|    iterations           | 67          |
|    time_elapsed         | 827         |
|    total_timesteps      | 137216      |
| train/                  |             |
|    approx_kl            | 0.044423074 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0632     |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0907      |
|    n_updates            | 13780       |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.246       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 165         |
|    iterations           | 68          |
|    time_elapsed         | 839         |
|    total_timesteps      | 139264      |
| train/                  |             |
|    approx_kl            | 0.073997945 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.399       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.441       |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.605       |
-----------------------------------------
Eval num_timesteps=140000, episode_reward=9.06 +/- 1.09
Episode length: 22.67 +/- 4.99
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 22.7      |
|    mean_reward          | 9.06      |
| time/                   |           |
|    total_timesteps      | 140000    |
| train/                  |           |
|    approx_kl            | 0.0434954 |
|    clip_fraction        | 0.0792    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0818   |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0986    |
|    n_updates            | 13800     |
|    policy_gradient_loss | -0.0101   |
|    value_loss           | 0.249     |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.19     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 69       |
|    time_elapsed    | 851      |
|    total_timesteps | 141312   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 70          |
|    time_elapsed         | 862         |
|    total_timesteps      | 143360      |
| train/                  |             |
|    approx_kl            | 0.055489294 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0814     |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0705      |
|    n_updates            | 13810       |
|    policy_gradient_loss | -0.00805    |
|    value_loss           | 0.32        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 71         |
|    time_elapsed         | 873        |
|    total_timesteps      | 145408     |
| train/                  |            |
|    approx_kl            | 0.03533317 |
|    clip_fraction        | 0.0818     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.095     |
|    explained_variance   | 0.677      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.269      |
|    n_updates            | 13820      |
|    policy_gradient_loss | -0.0168    |
|    value_loss           | 0.335      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 72          |
|    time_elapsed         | 885         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.057000447 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0753     |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0933      |
|    n_updates            | 13830       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.321       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 73          |
|    time_elapsed         | 897         |
|    total_timesteps      | 149504      |
| train/                  |             |
|    approx_kl            | 0.032630157 |
|    clip_fraction        | 0.0517      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0678     |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.135       |
|    n_updates            | 13840       |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.267       |
-----------------------------------------
Eval num_timesteps=150000, episode_reward=8.83 +/- 1.63
Episode length: 20.67 +/- 3.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 20.7      |
|    mean_reward          | 8.83      |
| time/                   |           |
|    total_timesteps      | 150000    |
| train/                  |           |
|    approx_kl            | 0.0605096 |
|    clip_fraction        | 0.0695    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0842   |
|    explained_variance   | 0.776     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0908    |
|    n_updates            | 13850     |
|    policy_gradient_loss | -0.014    |
|    value_loss           | 0.33      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 8.25     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 74       |
|    time_elapsed    | 909      |
|    total_timesteps | 151552   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | 7.68       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 75         |
|    time_elapsed         | 921        |
|    total_timesteps      | 153600     |
| train/                  |            |
|    approx_kl            | 0.03566123 |
|    clip_fraction        | 0.0576     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0633    |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0782     |
|    n_updates            | 13860      |
|    policy_gradient_loss | -0.0108    |
|    value_loss           | 0.228      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 8.11       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 76         |
|    time_elapsed         | 933        |
|    total_timesteps      | 155648     |
| train/                  |            |
|    approx_kl            | 0.04163842 |
|    clip_fraction        | 0.0618     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.083     |
|    explained_variance   | 0.757      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.15       |
|    n_updates            | 13870      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 0.298      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 77          |
|    time_elapsed         | 945         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.029086204 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0793     |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.081       |
|    n_updates            | 13880       |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.208       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 8.24       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 78         |
|    time_elapsed         | 957        |
|    total_timesteps      | 159744     |
| train/                  |            |
|    approx_kl            | 0.08349476 |
|    clip_fraction        | 0.0908     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0924     |
|    n_updates            | 13890      |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 0.429      |
----------------------------------------
Eval num_timesteps=160000, episode_reward=8.89 +/- 0.84
Episode length: 17.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 8.89       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.03703592 |
|    clip_fraction        | 0.0656     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0833    |
|    explained_variance   | 0.823      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0609     |
|    n_updates            | 13900      |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 0.248      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 79       |
|    time_elapsed    | 969      |
|    total_timesteps | 161792   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 80          |
|    time_elapsed         | 981         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.045912713 |
|    clip_fraction        | 0.0832      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 13910       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.383       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 81          |
|    time_elapsed         | 992         |
|    total_timesteps      | 165888      |
| train/                  |             |
|    approx_kl            | 0.044863492 |
|    clip_fraction        | 0.0615      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0793     |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.119       |
|    n_updates            | 13920       |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.188       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.86       |
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 82         |
|    time_elapsed         | 1004       |
|    total_timesteps      | 167936     |
| train/                  |            |
|    approx_kl            | 0.04473293 |
|    clip_fraction        | 0.0654     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0912    |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.05       |
|    n_updates            | 13930      |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.253      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.7        |
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 83         |
|    time_elapsed         | 1016       |
|    total_timesteps      | 169984     |
| train/                  |            |
|    approx_kl            | 0.05757665 |
|    clip_fraction        | 0.0939     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.687      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.099      |
|    n_updates            | 13940      |
|    policy_gradient_loss | -0.0169    |
|    value_loss           | 0.431      |
----------------------------------------
Eval num_timesteps=170000, episode_reward=9.26 +/- 0.82
Episode length: 18.67 +/- 2.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.7        |
|    mean_reward          | 9.26        |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.031171054 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.156      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0521      |
|    n_updates            | 13950       |
|    policy_gradient_loss | -0.026      |
|    value_loss           | 0.58        |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 8.14     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 84       |
|    time_elapsed    | 1028     |
|    total_timesteps | 172032   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 85          |
|    time_elapsed         | 1039        |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.035269156 |
|    clip_fraction        | 0.0732      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.101       |
|    n_updates            | 13960       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 0.249       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 8.06        |
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 86          |
|    time_elapsed         | 1052        |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.023633473 |
|    clip_fraction        | 0.0936      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0466      |
|    n_updates            | 13970       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.318       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 167         |
|    iterations           | 87          |
|    time_elapsed         | 1065        |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.027832953 |
|    clip_fraction        | 0.0835      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0863      |
|    n_updates            | 13980       |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.29        |
-----------------------------------------
Eval num_timesteps=180000, episode_reward=8.31 +/- 1.43
Episode length: 20.00 +/- 2.45
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20         |
|    mean_reward          | 8.31       |
| time/                   |            |
|    total_timesteps      | 180000     |
| train/                  |            |
|    approx_kl            | 0.04834283 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.172     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.131      |
|    n_updates            | 13990      |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.492      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.11     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 88       |
|    time_elapsed    | 1079     |
|    total_timesteps | 180224   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 89          |
|    time_elapsed         | 1091        |
|    total_timesteps      | 182272      |
| train/                  |             |
|    approx_kl            | 0.037699677 |
|    clip_fraction        | 0.0746      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 14000       |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.337       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 90          |
|    time_elapsed         | 1104        |
|    total_timesteps      | 184320      |
| train/                  |             |
|    approx_kl            | 0.029103318 |
|    clip_fraction        | 0.0966      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0898      |
|    n_updates            | 14010       |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.348       |
-----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.3      |
|    ep_rew_mean          | 8.11      |
| time/                   |           |
|    fps                  | 166       |
|    iterations           | 91        |
|    time_elapsed         | 1116      |
|    total_timesteps      | 186368    |
| train/                  |           |
|    approx_kl            | 0.0452961 |
|    clip_fraction        | 0.0987    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.159    |
|    explained_variance   | 0.709     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.00457   |
|    n_updates            | 14020     |
|    policy_gradient_loss | -0.0282   |
|    value_loss           | 0.284     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.64        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 92          |
|    time_elapsed         | 1128        |
|    total_timesteps      | 188416      |
| train/                  |             |
|    approx_kl            | 0.028966349 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0642      |
|    n_updates            | 14030       |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.399       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=190000, episode_reward=7.80 +/- 1.53
Episode length: 22.33 +/- 5.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 22.3      |
|    mean_reward          | 7.8       |
| time/                   |           |
|    total_timesteps      | 190000    |
| train/                  |           |
|    approx_kl            | 0.0335852 |
|    clip_fraction        | 0.0962    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.144    |
|    explained_variance   | 0.674     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0701    |
|    n_updates            | 14040     |
|    policy_gradient_loss | -0.0199   |
|    value_loss           | 0.417     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.62     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 93       |
|    time_elapsed    | 1140     |
|    total_timesteps | 190464   |
---------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21.3      |
|    ep_rew_mean          | 7.65      |
| time/                   |           |
|    fps                  | 167       |
|    iterations           | 94        |
|    time_elapsed         | 1152      |
|    total_timesteps      | 192512    |
| train/                  |           |
|    approx_kl            | 0.0738468 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.134    |
|    explained_variance   | 0.315     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.182     |
|    n_updates            | 14050     |
|    policy_gradient_loss | -0.0224   |
|    value_loss           | 0.557     |
---------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 95         |
|    time_elapsed         | 1164       |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.06386836 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 14060      |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.74       |
----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 7.68      |
| time/                   |           |
|    fps                  | 167       |
|    iterations           | 96        |
|    time_elapsed         | 1176      |
|    total_timesteps      | 196608    |
| train/                  |           |
|    approx_kl            | 0.0259704 |
|    clip_fraction        | 0.085     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.118    |
|    explained_variance   | 0.589     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.268     |
|    n_updates            | 14070     |
|    policy_gradient_loss | -0.0196   |
|    value_loss           | 0.616     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 97         |
|    time_elapsed         | 1189       |
|    total_timesteps      | 198656     |
| train/                  |            |
|    approx_kl            | 0.06871106 |
|    clip_fraction        | 0.0863     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.11      |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0951     |
|    n_updates            | 14080      |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.363      |
----------------------------------------
Eval num_timesteps=200000, episode_reward=8.39 +/- 0.37
Episode length: 22.67 +/- 4.99
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22.7        |
|    mean_reward          | 8.39        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.033845272 |
|    clip_fraction        | 0.0698      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0994     |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0839      |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.245       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 8.12     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 98       |
|    time_elapsed    | 1201     |
|    total_timesteps | 200704   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 99          |
|    time_elapsed         | 1214        |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.031978786 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0944     |
|    explained_variance   | 0.796       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0746      |
|    n_updates            | 14100       |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.233       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 100        |
|    time_elapsed         | 1226       |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.06209654 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.109      |
|    n_updates            | 14110      |
|    policy_gradient_loss | -0.0208    |
|    value_loss           | 0.401      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 101         |
|    time_elapsed         | 1239        |
|    total_timesteps      | 206848      |
| train/                  |             |
|    approx_kl            | 0.045697644 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0857      |
|    n_updates            | 14120       |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.301       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 102         |
|    time_elapsed         | 1252        |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.049554363 |
|    clip_fraction        | 0.0923      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 14130       |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.261       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=210000, episode_reward=7.89 +/- 2.05
Episode length: 17.33 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 7.89        |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.044965677 |
|    clip_fraction        | 0.0639      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0777     |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0601      |
|    n_updates            | 14140       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.231       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 7.96     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 103      |
|    time_elapsed    | 1265     |
|    total_timesteps | 210944   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.38        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 104         |
|    time_elapsed         | 1277        |
|    total_timesteps      | 212992      |
| train/                  |             |
|    approx_kl            | 0.036473766 |
|    clip_fraction        | 0.0847      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.726       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0715      |
|    n_updates            | 14150       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.49        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.75       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 105        |
|    time_elapsed         | 1290       |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.02972038 |
|    clip_fraction        | 0.0781     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0708     |
|    n_updates            | 14160      |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.222      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 106         |
|    time_elapsed         | 1302        |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.041133966 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.265       |
|    n_updates            | 14170       |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 0.524       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.61       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 107        |
|    time_elapsed         | 1314       |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.04036187 |
|    clip_fraction        | 0.0797     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.121      |
|    n_updates            | 14180      |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 0.391      |
----------------------------------------
Eval num_timesteps=220000, episode_reward=8.31 +/- 0.37
Episode length: 20.00 +/- 2.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20         |
|    mean_reward          | 8.31       |
| time/                   |            |
|    total_timesteps      | 220000     |
| train/                  |            |
|    approx_kl            | 0.04753697 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0646     |
|    n_updates            | 14190      |
|    policy_gradient_loss | -0.0241    |
|    value_loss           | 0.506      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 7.98     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 108      |
|    time_elapsed    | 1327     |
|    total_timesteps | 221184   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.97        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 109         |
|    time_elapsed         | 1339        |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.035845384 |
|    clip_fraction        | 0.0853      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0563      |
|    n_updates            | 14200       |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.366       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 110         |
|    time_elapsed         | 1350        |
|    total_timesteps      | 225280      |
| train/                  |             |
|    approx_kl            | 0.048824757 |
|    clip_fraction        | 0.0856      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.121       |
|    n_updates            | 14210       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.403       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.21        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 111         |
|    time_elapsed         | 1362        |
|    total_timesteps      | 227328      |
| train/                  |             |
|    approx_kl            | 0.028872795 |
|    clip_fraction        | 0.065       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0957     |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0967      |
|    n_updates            | 14220       |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.358       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 112         |
|    time_elapsed         | 1374        |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.028147522 |
|    clip_fraction        | 0.0647      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0895     |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 14230       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.236       |
-----------------------------------------
Eval num_timesteps=230000, episode_reward=8.89 +/- 0.69
Episode length: 17.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.89        |
| time/                   |             |
|    total_timesteps      | 230000      |
| train/                  |             |
|    approx_kl            | 0.051034674 |
|    clip_fraction        | 0.0738      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0879     |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0858      |
|    n_updates            | 14240       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.416       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.92     |
| time/              |          |
|    fps             | 167      |
|    iterations      | 113      |
|    time_elapsed    | 1383     |
|    total_timesteps | 231424   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.55       |
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 114        |
|    time_elapsed         | 1392       |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.04426168 |
|    clip_fraction        | 0.0725     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.098     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0616     |
|    n_updates            | 14250      |
|    policy_gradient_loss | -0.0165    |
|    value_loss           | 0.256      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 7.33        |
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 115         |
|    time_elapsed         | 1400        |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.046762183 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.267       |
|    n_updates            | 14260       |
|    policy_gradient_loss | -0.0254     |
|    value_loss           | 0.479       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 116         |
|    time_elapsed         | 1408        |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.041902706 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.316       |
|    n_updates            | 14270       |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.787       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8           |
| time/                   |             |
|    fps                  | 168         |
|    iterations           | 117         |
|    time_elapsed         | 1419        |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.034290254 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.265       |
|    n_updates            | 14280       |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.652       |
-----------------------------------------
Eval num_timesteps=240000, episode_reward=8.51 +/- 0.18
Episode length: 16.00 +/- 0.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 8.51       |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.08011284 |
|    clip_fraction        | 0.0937     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.651      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.103      |
|    n_updates            | 14290      |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.422      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.65     |
| time/              |          |
|    fps             | 169      |
|    iterations      | 118      |
|    time_elapsed    | 1427     |
|    total_timesteps | 241664   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.74        |
| time/                   |             |
|    fps                  | 169         |
|    iterations           | 119         |
|    time_elapsed         | 1436        |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.052835755 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.339       |
|    n_updates            | 14300       |
|    policy_gradient_loss | -0.00866    |
|    value_loss           | 0.496       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.69       |
| time/                   |            |
|    fps                  | 170        |
|    iterations           | 120        |
|    time_elapsed         | 1444       |
|    total_timesteps      | 245760     |
| train/                  |            |
|    approx_kl            | 0.05061102 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.153      |
|    n_updates            | 14310      |
|    policy_gradient_loss | -0.0205    |
|    value_loss           | 0.335      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 170         |
|    iterations           | 121         |
|    time_elapsed         | 1452        |
|    total_timesteps      | 247808      |
| train/                  |             |
|    approx_kl            | 0.051725954 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0808      |
|    n_updates            | 14320       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.405       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 122         |
|    time_elapsed         | 1460        |
|    total_timesteps      | 249856      |
| train/                  |             |
|    approx_kl            | 0.043908004 |
|    clip_fraction        | 0.0902      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0505      |
|    n_updates            | 14330       |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.428       |
-----------------------------------------
Eval num_timesteps=250000, episode_reward=8.67 +/- 1.53
Episode length: 18.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 8.67        |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.030817553 |
|    clip_fraction        | 0.0941      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.617       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0565      |
|    n_updates            | 14340       |
|    policy_gradient_loss | -0.0264     |
|    value_loss           | 0.36        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.37     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 123      |
|    time_elapsed    | 1468     |
|    total_timesteps | 251904   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 124         |
|    time_elapsed         | 1478        |
|    total_timesteps      | 253952      |
| train/                  |             |
|    approx_kl            | 0.043557703 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.439       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.171       |
|    n_updates            | 14350       |
|    policy_gradient_loss | -0.0284     |
|    value_loss           | 0.373       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.26        |
| time/                   |             |
|    fps                  | 172         |
|    iterations           | 125         |
|    time_elapsed         | 1486        |
|    total_timesteps      | 256000      |
| train/                  |             |
|    approx_kl            | 0.043639436 |
|    clip_fraction        | 0.0875      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.116       |
|    n_updates            | 14360       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.377       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 172        |
|    iterations           | 126        |
|    time_elapsed         | 1493       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.03655506 |
|    clip_fraction        | 0.0632     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0829    |
|    explained_variance   | 0.732      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0629     |
|    n_updates            | 14370      |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.308      |
----------------------------------------
reached max steps=100
Eval num_timesteps=260000, episode_reward=9.05 +/- 1.00
Episode length: 19.67 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 9.05        |
| time/                   |             |
|    total_timesteps      | 260000      |
| train/                  |             |
|    approx_kl            | 0.046011522 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0756      |
|    n_updates            | 14380       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 0.391       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 7.75     |
| time/              |          |
|    fps             | 173      |
|    iterations      | 127      |
|    time_elapsed    | 1501     |
|    total_timesteps | 260096   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.7         |
| time/                   |             |
|    fps                  | 173         |
|    iterations           | 128         |
|    time_elapsed         | 1509        |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.077283405 |
|    clip_fraction        | 0.0854      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.547       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0839      |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.0225     |
|    value_loss           | 0.324       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.84        |
| time/                   |             |
|    fps                  | 173         |
|    iterations           | 129         |
|    time_elapsed         | 1518        |
|    total_timesteps      | 264192      |
| train/                  |             |
|    approx_kl            | 0.040836677 |
|    clip_fraction        | 0.0915      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0187      |
|    n_updates            | 14400       |
|    policy_gradient_loss | 0.0104      |
|    value_loss           | 0.33        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 8.15        |
| time/                   |             |
|    fps                  | 174         |
|    iterations           | 130         |
|    time_elapsed         | 1525        |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.036116064 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0548      |
|    n_updates            | 14410       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.29        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 175         |
|    iterations           | 131         |
|    time_elapsed         | 1532        |
|    total_timesteps      | 268288      |
| train/                  |             |
|    approx_kl            | 0.052310813 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.194       |
|    n_updates            | 14420       |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 0.351       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=270000, episode_reward=8.82 +/- 0.76
Episode length: 17.67 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 8.82        |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.049359635 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0891     |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0711      |
|    n_updates            | 14430       |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 0.235       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 7.94     |
| time/              |          |
|    fps             | 175      |
|    iterations      | 132      |
|    time_elapsed    | 1539     |
|    total_timesteps | 270336   |
---------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.6      |
|    ep_rew_mean          | 7.95      |
| time/                   |           |
|    fps                  | 176       |
|    iterations           | 133       |
|    time_elapsed         | 1546      |
|    total_timesteps      | 272384    |
| train/                  |           |
|    approx_kl            | 0.0498644 |
|    clip_fraction        | 0.0821    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.105    |
|    explained_variance   | 0.635     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0782    |
|    n_updates            | 14440     |
|    policy_gradient_loss | -0.0159   |
|    value_loss           | 0.481     |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 176        |
|    iterations           | 134        |
|    time_elapsed         | 1553       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.04048238 |
|    clip_fraction        | 0.0766     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.18       |
|    n_updates            | 14450      |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 0.501      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 177         |
|    iterations           | 135         |
|    time_elapsed         | 1560        |
|    total_timesteps      | 276480      |
| train/                  |             |
|    approx_kl            | 0.058385428 |
|    clip_fraction        | 0.0853      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.28        |
|    n_updates            | 14460       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.552       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.86        |
| time/                   |             |
|    fps                  | 177         |
|    iterations           | 136         |
|    time_elapsed         | 1567        |
|    total_timesteps      | 278528      |
| train/                  |             |
|    approx_kl            | 0.062355153 |
|    clip_fraction        | 0.0792      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0415      |
|    n_updates            | 14470       |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.318       |
-----------------------------------------
Eval num_timesteps=280000, episode_reward=8.74 +/- 0.63
Episode length: 18.00 +/- 1.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18         |
|    mean_reward          | 8.74       |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.05730425 |
|    clip_fraction        | 0.0952     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0274     |
|    n_updates            | 14480      |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 0.354      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 178      |
|    iterations      | 137      |
|    time_elapsed    | 1575     |
|    total_timesteps | 280576   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.74        |
| time/                   |             |
|    fps                  | 178         |
|    iterations           | 138         |
|    time_elapsed         | 1582        |
|    total_timesteps      | 282624      |
| train/                  |             |
|    approx_kl            | 0.030260846 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0752      |
|    n_updates            | 14490       |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 0.331       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 179        |
|    iterations           | 139        |
|    time_elapsed         | 1589       |
|    total_timesteps      | 284672     |
| train/                  |            |
|    approx_kl            | 0.03743299 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.168     |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0617     |
|    n_updates            | 14500      |
|    policy_gradient_loss | -0.0249    |
|    value_loss           | 0.35       |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.3      |
|    ep_rew_mean          | 7.52      |
| time/                   |           |
|    fps                  | 179       |
|    iterations           | 140       |
|    time_elapsed         | 1596      |
|    total_timesteps      | 286720    |
| train/                  |           |
|    approx_kl            | 0.0585884 |
|    clip_fraction        | 0.0996    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.152    |
|    explained_variance   | 0.704     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0997    |
|    n_updates            | 14510     |
|    policy_gradient_loss | -0.012    |
|    value_loss           | 0.513     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 180         |
|    iterations           | 141         |
|    time_elapsed         | 1603        |
|    total_timesteps      | 288768      |
| train/                  |             |
|    approx_kl            | 0.089397654 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0796      |
|    n_updates            | 14520       |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.429       |
-----------------------------------------
Eval num_timesteps=290000, episode_reward=8.82 +/- 0.90
Episode length: 17.67 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.82       |
| time/                   |            |
|    total_timesteps      | 290000     |
| train/                  |            |
|    approx_kl            | 0.03763705 |
|    clip_fraction        | 0.0908     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.109      |
|    n_updates            | 14530      |
|    policy_gradient_loss | -0.0186    |
|    value_loss           | 0.302      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 180      |
|    iterations      | 142      |
|    time_elapsed    | 1610     |
|    total_timesteps | 290816   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.8      |
|    ep_rew_mean          | 7.88      |
| time/                   |           |
|    fps                  | 181       |
|    iterations           | 143       |
|    time_elapsed         | 1617      |
|    total_timesteps      | 292864    |
| train/                  |           |
|    approx_kl            | 0.0422682 |
|    clip_fraction        | 0.0824    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.129    |
|    explained_variance   | 0.7       |
|    learning_rate        | 0.0003    |
|    loss                 | 0.24      |
|    n_updates            | 14540     |
|    policy_gradient_loss | -0.0205   |
|    value_loss           | 0.384     |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 181         |
|    iterations           | 144         |
|    time_elapsed         | 1624        |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.057508703 |
|    clip_fraction        | 0.097       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0919      |
|    n_updates            | 14550       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.322       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 181         |
|    iterations           | 145         |
|    time_elapsed         | 1631        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.037921097 |
|    clip_fraction        | 0.0717      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0991     |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 14560       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.355       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.86       |
| time/                   |            |
|    fps                  | 182        |
|    iterations           | 146        |
|    time_elapsed         | 1638       |
|    total_timesteps      | 299008     |
| train/                  |            |
|    approx_kl            | 0.04568085 |
|    clip_fraction        | 0.0725     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.745      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0871     |
|    n_updates            | 14570      |
|    policy_gradient_loss | -0.0122    |
|    value_loss           | 0.324      |
----------------------------------------
reached max steps=100
Eval num_timesteps=300000, episode_reward=8.15 +/- 1.01
Episode length: 17.67 +/- 2.87
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.15       |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.03422047 |
|    clip_fraction        | 0.0844     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.132      |
|    n_updates            | 14580      |
|    policy_gradient_loss | -0.0223    |
|    value_loss           | 0.331      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 7.76     |
| time/              |          |
|    fps             | 182      |
|    iterations      | 147      |
|    time_elapsed    | 1646     |
|    total_timesteps | 301056   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 7.69       |
| time/                   |            |
|    fps                  | 183        |
|    iterations           | 148        |
|    time_elapsed         | 1653       |
|    total_timesteps      | 303104     |
| train/                  |            |
|    approx_kl            | 0.12524661 |
|    clip_fraction        | 0.0984     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.53       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 14590      |
|    policy_gradient_loss | -0.0323    |
|    value_loss           | 0.38       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.24       |
| time/                   |            |
|    fps                  | 183        |
|    iterations           | 149        |
|    time_elapsed         | 1659       |
|    total_timesteps      | 305152     |
| train/                  |            |
|    approx_kl            | 0.04704476 |
|    clip_fraction        | 0.0634     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.07      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0686     |
|    n_updates            | 14600      |
|    policy_gradient_loss | -0.011     |
|    value_loss           | 0.217      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 184         |
|    iterations           | 150         |
|    time_elapsed         | 1667        |
|    total_timesteps      | 307200      |
| train/                  |             |
|    approx_kl            | 0.034635067 |
|    clip_fraction        | 0.068       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0796     |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0488      |
|    n_updates            | 14610       |
|    policy_gradient_loss | -0.0166     |
|    value_loss           | 0.26        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 184         |
|    iterations           | 151         |
|    time_elapsed         | 1674        |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.037559398 |
|    clip_fraction        | 0.0653      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0757     |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.058       |
|    n_updates            | 14620       |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.229       |
-----------------------------------------
Eval num_timesteps=310000, episode_reward=7.49 +/- 0.93
Episode length: 20.67 +/- 3.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.7        |
|    mean_reward          | 7.49        |
| time/                   |             |
|    total_timesteps      | 310000      |
| train/                  |             |
|    approx_kl            | 0.022762919 |
|    clip_fraction        | 0.0657      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0953     |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0873      |
|    n_updates            | 14630       |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.289       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 8.15     |
| time/              |          |
|    fps             | 185      |
|    iterations      | 152      |
|    time_elapsed    | 1681     |
|    total_timesteps | 311296   |
---------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.4      |
|    ep_rew_mean          | 8.06      |
| time/                   |           |
|    fps                  | 185       |
|    iterations           | 153       |
|    time_elapsed         | 1688      |
|    total_timesteps      | 313344    |
| train/                  |           |
|    approx_kl            | 0.0435629 |
|    clip_fraction        | 0.0861    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.11     |
|    explained_variance   | 0.779     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.108     |
|    n_updates            | 14640     |
|    policy_gradient_loss | -0.012    |
|    value_loss           | 0.364     |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.84       |
| time/                   |            |
|    fps                  | 186        |
|    iterations           | 154        |
|    time_elapsed         | 1695       |
|    total_timesteps      | 315392     |
| train/                  |            |
|    approx_kl            | 0.01940778 |
|    clip_fraction        | 0.0665     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0981    |
|    explained_variance   | 0.714      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.058      |
|    n_updates            | 14650      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.341      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.79       |
| time/                   |            |
|    fps                  | 186        |
|    iterations           | 155        |
|    time_elapsed         | 1702       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.04008343 |
|    clip_fraction        | 0.0905     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.118     |
|    explained_variance   | 0.684      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0979     |
|    n_updates            | 14660      |
|    policy_gradient_loss | -0.0234    |
|    value_loss           | 0.323      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 8.15        |
| time/                   |             |
|    fps                  | 186         |
|    iterations           | 156         |
|    time_elapsed         | 1709        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.057090092 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.051       |
|    n_updates            | 14670       |
|    policy_gradient_loss | -0.029      |
|    value_loss           | 0.399       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=320000, episode_reward=-4.27 +/- 8.11
Episode length: 40.67 +/- 13.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 40.7       |
|    mean_reward          | -4.27      |
| time/                   |            |
|    total_timesteps      | 320000     |
| train/                  |            |
|    approx_kl            | 0.04110626 |
|    clip_fraction        | 0.0947     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.146      |
|    n_updates            | 14680      |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 0.572      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 8.04     |
| time/              |          |
|    fps             | 187      |
|    iterations      | 157      |
|    time_elapsed    | 1716     |
|    total_timesteps | 321536   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 187        |
|    iterations           | 158        |
|    time_elapsed         | 1723       |
|    total_timesteps      | 323584     |
| train/                  |            |
|    approx_kl            | 0.03450761 |
|    clip_fraction        | 0.0997     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.111      |
|    n_updates            | 14690      |
|    policy_gradient_loss | -0.0211    |
|    value_loss           | 0.369      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 188         |
|    iterations           | 159         |
|    time_elapsed         | 1730        |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.044293903 |
|    clip_fraction        | 0.122       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.586       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.235       |
|    n_updates            | 14700       |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.422       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 188        |
|    iterations           | 160        |
|    time_elapsed         | 1738       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.05110161 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.131      |
|    n_updates            | 14710      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.32       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 188        |
|    iterations           | 161        |
|    time_elapsed         | 1745       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.06304754 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.146     |
|    explained_variance   | 0.627      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0759     |
|    n_updates            | 14720      |
|    policy_gradient_loss | -0.025     |
|    value_loss           | 0.289      |
----------------------------------------
Eval num_timesteps=330000, episode_reward=9.64 +/- 0.64
Episode length: 20.00 +/- 2.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20         |
|    mean_reward          | 9.64       |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.07641475 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0607     |
|    n_updates            | 14730      |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.47       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 7.96     |
| time/              |          |
|    fps             | 189      |
|    iterations      | 162      |
|    time_elapsed    | 1752     |
|    total_timesteps | 331776   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 189        |
|    iterations           | 163        |
|    time_elapsed         | 1759       |
|    total_timesteps      | 333824     |
| train/                  |            |
|    approx_kl            | 0.03279441 |
|    clip_fraction        | 0.0828     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.138     |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0967     |
|    n_updates            | 14740      |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.312      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 164        |
|    time_elapsed         | 1766       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.04054559 |
|    clip_fraction        | 0.0981     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.123      |
|    n_updates            | 14750      |
|    policy_gradient_loss | -0.0167    |
|    value_loss           | 0.391      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.98       |
| time/                   |            |
|    fps                  | 190        |
|    iterations           | 165        |
|    time_elapsed         | 1773       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.06641138 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.289      |
|    n_updates            | 14760      |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 0.318      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 190         |
|    iterations           | 166         |
|    time_elapsed         | 1780        |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.034114853 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.127      |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0358      |
|    n_updates            | 14770       |
|    policy_gradient_loss | -0.0228     |
|    value_loss           | 0.295       |
-----------------------------------------
Eval num_timesteps=340000, episode_reward=5.22 +/- 1.06
Episode length: 17.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 5.22        |
| time/                   |             |
|    total_timesteps      | 340000      |
| train/                  |             |
|    approx_kl            | 0.032659426 |
|    clip_fraction        | 0.0695      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.202       |
|    n_updates            | 14780       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 191      |
|    iterations      | 167      |
|    time_elapsed    | 1788     |
|    total_timesteps | 342016   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 168         |
|    time_elapsed         | 1795        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.056885518 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0277      |
|    n_updates            | 14790       |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 0.278       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 191         |
|    iterations           | 169         |
|    time_elapsed         | 1802        |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.027926808 |
|    clip_fraction        | 0.0761      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0919     |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0334      |
|    n_updates            | 14800       |
|    policy_gradient_loss | -0.0188     |
|    value_loss           | 0.306       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 192        |
|    iterations           | 170        |
|    time_elapsed         | 1809       |
|    total_timesteps      | 348160     |
| train/                  |            |
|    approx_kl            | 0.05948617 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0982     |
|    n_updates            | 14810      |
|    policy_gradient_loss | -0.0305    |
|    value_loss           | 0.487      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=350000, episode_reward=5.46 +/- 6.69
Episode length: 28.33 +/- 15.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.3       |
|    mean_reward          | 5.46       |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.04026644 |
|    clip_fraction        | 0.0887     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.149      |
|    n_updates            | 14820      |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 0.402      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.79     |
| time/              |          |
|    fps             | 192      |
|    iterations      | 171      |
|    time_elapsed    | 1817     |
|    total_timesteps | 350208   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.21       |
| time/                   |            |
|    fps                  | 193        |
|    iterations           | 172        |
|    time_elapsed         | 1824       |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.07112156 |
|    clip_fraction        | 0.0937     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.109     |
|    explained_variance   | 0.649      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0865     |
|    n_updates            | 14830      |
|    policy_gradient_loss | -0.0266    |
|    value_loss           | 0.349      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 193         |
|    iterations           | 173         |
|    time_elapsed         | 1831        |
|    total_timesteps      | 354304      |
| train/                  |             |
|    approx_kl            | 0.035821315 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0791     |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0189      |
|    n_updates            | 14840       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.233       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 193         |
|    iterations           | 174         |
|    time_elapsed         | 1838        |
|    total_timesteps      | 356352      |
| train/                  |             |
|    approx_kl            | 0.043499634 |
|    clip_fraction        | 0.0781      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0914     |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0638      |
|    n_updates            | 14850       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.254       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.73        |
| time/                   |             |
|    fps                  | 194         |
|    iterations           | 175         |
|    time_elapsed         | 1846        |
|    total_timesteps      | 358400      |
| train/                  |             |
|    approx_kl            | 0.026438138 |
|    clip_fraction        | 0.063       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0912     |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0501      |
|    n_updates            | 14860       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.24        |
-----------------------------------------
Eval num_timesteps=360000, episode_reward=8.22 +/- 0.21
Episode length: 17.33 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.22        |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.040332172 |
|    clip_fraction        | 0.0848      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.388       |
|    n_updates            | 14870       |
|    policy_gradient_loss | -0.0209     |
|    value_loss           | 0.479       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.2      |
| time/              |          |
|    fps             | 194      |
|    iterations      | 176      |
|    time_elapsed    | 1853     |
|    total_timesteps | 360448   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.3         |
| time/                   |             |
|    fps                  | 194         |
|    iterations           | 177         |
|    time_elapsed         | 1860        |
|    total_timesteps      | 362496      |
| train/                  |             |
|    approx_kl            | 0.039365485 |
|    clip_fraction        | 0.0776      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0727     |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.166       |
|    n_updates            | 14880       |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.287       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 195         |
|    iterations           | 178         |
|    time_elapsed         | 1867        |
|    total_timesteps      | 364544      |
| train/                  |             |
|    approx_kl            | 0.042867392 |
|    clip_fraction        | 0.069       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0765     |
|    explained_variance   | 0.788       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0644      |
|    n_updates            | 14890       |
|    policy_gradient_loss | -0.0186     |
|    value_loss           | 0.316       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 8.32        |
| time/                   |             |
|    fps                  | 195         |
|    iterations           | 179         |
|    time_elapsed         | 1874        |
|    total_timesteps      | 366592      |
| train/                  |             |
|    approx_kl            | 0.034250766 |
|    clip_fraction        | 0.0718      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0712      |
|    n_updates            | 14900       |
|    policy_gradient_loss | -0.018      |
|    value_loss           | 0.33        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.24       |
| time/                   |            |
|    fps                  | 195        |
|    iterations           | 180        |
|    time_elapsed         | 1881       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.03330695 |
|    clip_fraction        | 0.0569     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0609    |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.149      |
|    n_updates            | 14910      |
|    policy_gradient_loss | -0.0139    |
|    value_loss           | 0.263      |
----------------------------------------
reached max steps=100
Eval num_timesteps=370000, episode_reward=9.41 +/- 1.01
Episode length: 18.00 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 9.41        |
| time/                   |             |
|    total_timesteps      | 370000      |
| train/                  |             |
|    approx_kl            | 0.042506143 |
|    clip_fraction        | 0.0744      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0723     |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.141       |
|    n_updates            | 14920       |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.451       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.02     |
| time/              |          |
|    fps             | 196      |
|    iterations      | 181      |
|    time_elapsed    | 1888     |
|    total_timesteps | 370688   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.22        |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 182         |
|    time_elapsed         | 1895        |
|    total_timesteps      | 372736      |
| train/                  |             |
|    approx_kl            | 0.082838945 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0761     |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0977      |
|    n_updates            | 14930       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.329       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 196         |
|    iterations           | 183         |
|    time_elapsed         | 1902        |
|    total_timesteps      | 374784      |
| train/                  |             |
|    approx_kl            | 0.032581408 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0629     |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 14940       |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.322       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 184         |
|    time_elapsed         | 1909        |
|    total_timesteps      | 376832      |
| train/                  |             |
|    approx_kl            | 0.031332273 |
|    clip_fraction        | 0.0634      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0831     |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0496      |
|    n_updates            | 14950       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.236       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 197         |
|    iterations           | 185         |
|    time_elapsed         | 1916        |
|    total_timesteps      | 378880      |
| train/                  |             |
|    approx_kl            | 0.057554796 |
|    clip_fraction        | 0.0929      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0896      |
|    n_updates            | 14960       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.338       |
-----------------------------------------
Eval num_timesteps=380000, episode_reward=8.19 +/- 1.16
Episode length: 19.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 19         |
|    mean_reward          | 8.19       |
| time/                   |            |
|    total_timesteps      | 380000     |
| train/                  |            |
|    approx_kl            | 0.05335202 |
|    clip_fraction        | 0.0839     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.111     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0743     |
|    n_updates            | 14970      |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 0.371      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 8        |
| time/              |          |
|    fps             | 197      |
|    iterations      | 186      |
|    time_elapsed    | 1924     |
|    total_timesteps | 380928   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.79       |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 187        |
|    time_elapsed         | 1931       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.04655858 |
|    clip_fraction        | 0.0835     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0266     |
|    n_updates            | 14980      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.305      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 198        |
|    iterations           | 188        |
|    time_elapsed         | 1938       |
|    total_timesteps      | 385024     |
| train/                  |            |
|    approx_kl            | 0.05345443 |
|    clip_fraction        | 0.085      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.117     |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.146      |
|    n_updates            | 14990      |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 0.451      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 198         |
|    iterations           | 189         |
|    time_elapsed         | 1945        |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.057780195 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.648       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.28        |
|    n_updates            | 15000       |
|    policy_gradient_loss | -0.0227     |
|    value_loss           | 0.473       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.44        |
| time/                   |             |
|    fps                  | 199         |
|    iterations           | 190         |
|    time_elapsed         | 1952        |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.028730756 |
|    clip_fraction        | 0.0684      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0878     |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0773      |
|    n_updates            | 15010       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.259       |
-----------------------------------------
Eval num_timesteps=390000, episode_reward=9.11 +/- 0.84
Episode length: 16.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 9.11       |
| time/                   |            |
|    total_timesteps      | 390000     |
| train/                  |            |
|    approx_kl            | 0.06933783 |
|    clip_fraction        | 0.0688     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0901    |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0502     |
|    n_updates            | 15020      |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 0.238      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 8.26     |
| time/              |          |
|    fps             | 199      |
|    iterations      | 191      |
|    time_elapsed    | 1959     |
|    total_timesteps | 391168   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.13       |
| time/                   |            |
|    fps                  | 199        |
|    iterations           | 192        |
|    time_elapsed         | 1966       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.04079485 |
|    clip_fraction        | 0.0628     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0805    |
|    explained_variance   | 0.816      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0499     |
|    n_updates            | 15030      |
|    policy_gradient_loss | -0.0131    |
|    value_loss           | 0.221      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 193         |
|    time_elapsed         | 1973        |
|    total_timesteps      | 395264      |
| train/                  |             |
|    approx_kl            | 0.044449277 |
|    clip_fraction        | 0.0804      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0979     |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0764      |
|    n_updates            | 15040       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.337       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 200         |
|    iterations           | 194         |
|    time_elapsed         | 1980        |
|    total_timesteps      | 397312      |
| train/                  |             |
|    approx_kl            | 0.037984747 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0987     |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0948      |
|    n_updates            | 15050       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.369       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 200        |
|    iterations           | 195        |
|    time_elapsed         | 1987       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.04381785 |
|    clip_fraction        | 0.0875     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.746      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.109      |
|    n_updates            | 15060      |
|    policy_gradient_loss | -0.0209    |
|    value_loss           | 0.354      |
----------------------------------------
Eval num_timesteps=400000, episode_reward=8.31 +/- 1.36
Episode length: 20.00 +/- 2.83
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 8.31        |
| time/                   |             |
|    total_timesteps      | 400000      |
| train/                  |             |
|    approx_kl            | 0.030271363 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.198       |
|    n_updates            | 15070       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.559       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.64     |
| time/              |          |
|    fps             | 201      |
|    iterations      | 196      |
|    time_elapsed    | 1995     |
|    total_timesteps | 401408   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 201         |
|    iterations           | 197         |
|    time_elapsed         | 2002        |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.037894476 |
|    clip_fraction        | 0.0974      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 15080       |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.735       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.51       |
| time/                   |            |
|    fps                  | 201        |
|    iterations           | 198        |
|    time_elapsed         | 2009       |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.04231005 |
|    clip_fraction        | 0.0727     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0918    |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0746     |
|    n_updates            | 15090      |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 0.295      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 199        |
|    time_elapsed         | 2016       |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.06292777 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.659      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.217      |
|    n_updates            | 15100      |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.537      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 202         |
|    iterations           | 200         |
|    time_elapsed         | 2023        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.031916946 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0923     |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0826      |
|    n_updates            | 15110       |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 0.429       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=410000, episode_reward=8.67 +/- 1.00
Episode length: 18.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 8.67       |
| time/                   |            |
|    total_timesteps      | 410000     |
| train/                  |            |
|    approx_kl            | 0.10375476 |
|    clip_fraction        | 0.0905     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.105     |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0685     |
|    n_updates            | 15120      |
|    policy_gradient_loss | -0.0195    |
|    value_loss           | 0.45       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 202      |
|    iterations      | 201      |
|    time_elapsed    | 2030     |
|    total_timesteps | 411648   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 202        |
|    iterations           | 202        |
|    time_elapsed         | 2037       |
|    total_timesteps      | 413696     |
| train/                  |            |
|    approx_kl            | 0.10667646 |
|    clip_fraction        | 0.0925     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0927    |
|    explained_variance   | 0.611      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 15130      |
|    policy_gradient_loss | -0.0194    |
|    value_loss           | 0.424      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 203         |
|    time_elapsed         | 2044        |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.082076296 |
|    clip_fraction        | 0.0916      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0906     |
|    explained_variance   | 0.638       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 15140       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 0.48        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 203        |
|    iterations           | 204        |
|    time_elapsed         | 2051       |
|    total_timesteps      | 417792     |
| train/                  |            |
|    approx_kl            | 0.30918595 |
|    clip_fraction        | 0.0974     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0906    |
|    explained_variance   | 0.44       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.269      |
|    n_updates            | 15150      |
|    policy_gradient_loss | -0.0288    |
|    value_loss           | 0.772      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 203         |
|    iterations           | 205         |
|    time_elapsed         | 2058        |
|    total_timesteps      | 419840      |
| train/                  |             |
|    approx_kl            | 0.054233737 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0849     |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0369      |
|    n_updates            | 15160       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.585       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=420000, episode_reward=5.24 +/- 6.53
Episode length: 29.33 +/- 14.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.3        |
|    mean_reward          | 5.24        |
| time/                   |             |
|    total_timesteps      | 420000      |
| train/                  |             |
|    approx_kl            | 0.045383684 |
|    clip_fraction        | 0.0627      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0675     |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.157       |
|    n_updates            | 15170       |
|    policy_gradient_loss | -0.00864    |
|    value_loss           | 0.297       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.7     |
|    ep_rew_mean     | 7.43     |
| time/              |          |
|    fps             | 204      |
|    iterations      | 206      |
|    time_elapsed    | 2066     |
|    total_timesteps | 421888   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.71        |
| time/                   |             |
|    fps                  | 204         |
|    iterations           | 207         |
|    time_elapsed         | 2074        |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.072466195 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.578       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.413       |
|    n_updates            | 15180       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.722       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.7      |
|    ep_rew_mean          | 7.87      |
| time/                   |           |
|    fps                  | 204       |
|    iterations           | 208       |
|    time_elapsed         | 2081      |
|    total_timesteps      | 425984    |
| train/                  |           |
|    approx_kl            | 0.0642207 |
|    clip_fraction        | 0.0919    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.104    |
|    explained_variance   | 0.586     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0866    |
|    n_updates            | 15190     |
|    policy_gradient_loss | -0.00289  |
|    value_loss           | 0.474     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 8.01       |
| time/                   |            |
|    fps                  | 204        |
|    iterations           | 209        |
|    time_elapsed         | 2088       |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.03427238 |
|    clip_fraction        | 0.0822     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0475     |
|    n_updates            | 15200      |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.263      |
----------------------------------------
Eval num_timesteps=430000, episode_reward=8.17 +/- 1.04
Episode length: 23.67 +/- 4.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.7        |
|    mean_reward          | 8.17        |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.109106846 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 15210       |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.366       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 205      |
|    iterations      | 210      |
|    time_elapsed    | 2095     |
|    total_timesteps | 430080   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.12        |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 211         |
|    time_elapsed         | 2102        |
|    total_timesteps      | 432128      |
| train/                  |             |
|    approx_kl            | 0.051398665 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.051       |
|    n_updates            | 15220       |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.366       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.78        |
| time/                   |             |
|    fps                  | 205         |
|    iterations           | 212         |
|    time_elapsed         | 2109        |
|    total_timesteps      | 434176      |
| train/                  |             |
|    approx_kl            | 0.035636105 |
|    clip_fraction        | 0.0809      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0951     |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0723      |
|    n_updates            | 15230       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.341       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.21       |
| time/                   |            |
|    fps                  | 206        |
|    iterations           | 213        |
|    time_elapsed         | 2117       |
|    total_timesteps      | 436224     |
| train/                  |            |
|    approx_kl            | 0.04107459 |
|    clip_fraction        | 0.0743     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0983    |
|    explained_variance   | 0.697      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 15240      |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 0.409      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.2         |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 214         |
|    time_elapsed         | 2124        |
|    total_timesteps      | 438272      |
| train/                  |             |
|    approx_kl            | 0.023070872 |
|    clip_fraction        | 0.0638      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0951     |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0946      |
|    n_updates            | 15250       |
|    policy_gradient_loss | -0.0135     |
|    value_loss           | 0.2         |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=440000, episode_reward=7.87 +/- 1.12
Episode length: 22.00 +/- 3.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22         |
|    mean_reward          | 7.87       |
| time/                   |            |
|    total_timesteps      | 440000     |
| train/                  |            |
|    approx_kl            | 0.03504332 |
|    clip_fraction        | 0.0624     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0778    |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 15260      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.226      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.78     |
| time/              |          |
|    fps             | 206      |
|    iterations      | 215      |
|    time_elapsed    | 2131     |
|    total_timesteps | 440320   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.05        |
| time/                   |             |
|    fps                  | 206         |
|    iterations           | 216         |
|    time_elapsed         | 2139        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.058539964 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0969      |
|    n_updates            | 15270       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.551       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 217         |
|    time_elapsed         | 2146        |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.038063105 |
|    clip_fraction        | 0.0703      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0939     |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0954      |
|    n_updates            | 15280       |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.307       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 8.12        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 218         |
|    time_elapsed         | 2153        |
|    total_timesteps      | 446464      |
| train/                  |             |
|    approx_kl            | 0.037170663 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.76        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0978      |
|    n_updates            | 15290       |
|    policy_gradient_loss | -0.0191     |
|    value_loss           | 0.371       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 207         |
|    iterations           | 219         |
|    time_elapsed         | 2160        |
|    total_timesteps      | 448512      |
| train/                  |             |
|    approx_kl            | 0.027669555 |
|    clip_fraction        | 0.0737      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.767       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0958      |
|    n_updates            | 15300       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.253       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=450000, episode_reward=6.96 +/- 0.49
Episode length: 17.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17         |
|    mean_reward          | 6.96       |
| time/                   |            |
|    total_timesteps      | 450000     |
| train/                  |            |
|    approx_kl            | 0.04847377 |
|    clip_fraction        | 0.0939     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.13      |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0375     |
|    n_updates            | 15310      |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.374      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.95     |
| time/              |          |
|    fps             | 207      |
|    iterations      | 220      |
|    time_elapsed    | 2168     |
|    total_timesteps | 450560   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.78       |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 221        |
|    time_elapsed         | 2175       |
|    total_timesteps      | 452608     |
| train/                  |            |
|    approx_kl            | 0.07652878 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.109     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0855     |
|    n_updates            | 15320      |
|    policy_gradient_loss | -0.0206    |
|    value_loss           | 0.443      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.71       |
| time/                   |            |
|    fps                  | 208        |
|    iterations           | 222        |
|    time_elapsed         | 2182       |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.03465714 |
|    clip_fraction        | 0.0836     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.683      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0661     |
|    n_updates            | 15330      |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.473      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 223         |
|    time_elapsed         | 2189        |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.060365066 |
|    clip_fraction        | 0.0911      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0757      |
|    n_updates            | 15340       |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.372       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.36        |
| time/                   |             |
|    fps                  | 208         |
|    iterations           | 224         |
|    time_elapsed         | 2196        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.047391407 |
|    clip_fraction        | 0.0934      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0635      |
|    n_updates            | 15350       |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.401       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=460000, episode_reward=3.60 +/- 6.79
Episode length: 27.67 +/- 15.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.7        |
|    mean_reward          | 3.6         |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.034573123 |
|    clip_fraction        | 0.059       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0719     |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.166       |
|    n_updates            | 15360       |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.227       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.06     |
| time/              |          |
|    fps             | 209      |
|    iterations      | 225      |
|    time_elapsed    | 2203     |
|    total_timesteps | 460800   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.28       |
| time/                   |            |
|    fps                  | 209        |
|    iterations           | 226        |
|    time_elapsed         | 2211       |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.04503435 |
|    clip_fraction        | 0.0759     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0801    |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0556     |
|    n_updates            | 15370      |
|    policy_gradient_loss | -0.0224    |
|    value_loss           | 0.348      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.8         |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 227         |
|    time_elapsed         | 2218        |
|    total_timesteps      | 464896      |
| train/                  |             |
|    approx_kl            | 0.031922445 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0519     |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0901      |
|    n_updates            | 15380       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.192       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.08        |
| time/                   |             |
|    fps                  | 209         |
|    iterations           | 228         |
|    time_elapsed         | 2225        |
|    total_timesteps      | 466944      |
| train/                  |             |
|    approx_kl            | 0.059228525 |
|    clip_fraction        | 0.0801      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0821     |
|    explained_variance   | 0.674       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0984      |
|    n_updates            | 15390       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.377       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.8      |
|    ep_rew_mean          | 8.26      |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 229       |
|    time_elapsed         | 2232      |
|    total_timesteps      | 468992    |
| train/                  |           |
|    approx_kl            | 0.0320408 |
|    clip_fraction        | 0.0753    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0934   |
|    explained_variance   | 0.731     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.185     |
|    n_updates            | 15400     |
|    policy_gradient_loss | -0.0195   |
|    value_loss           | 0.433     |
---------------------------------------
Eval num_timesteps=470000, episode_reward=8.97 +/- 0.37
Episode length: 20.00 +/- 2.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 8.97        |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.064907126 |
|    clip_fraction        | 0.0768      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0859     |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0474      |
|    n_updates            | 15410       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.227       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.4     |
|    ep_rew_mean     | 7.96     |
| time/              |          |
|    fps             | 210      |
|    iterations      | 230      |
|    time_elapsed    | 2239     |
|    total_timesteps | 471040   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 8.21      |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 231       |
|    time_elapsed         | 2246      |
|    total_timesteps      | 473088    |
| train/                  |           |
|    approx_kl            | 0.4790358 |
|    clip_fraction        | 0.122     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0904   |
|    explained_variance   | 0.728     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.111     |
|    n_updates            | 15420     |
|    policy_gradient_loss | -0.0176   |
|    value_loss           | 0.423     |
---------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20.9      |
|    ep_rew_mean          | 8.19      |
| time/                   |           |
|    fps                  | 210       |
|    iterations           | 232       |
|    time_elapsed         | 2253      |
|    total_timesteps      | 475136    |
| train/                  |           |
|    approx_kl            | 0.0303804 |
|    clip_fraction        | 0.0736    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.092    |
|    explained_variance   | 0.745     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.168     |
|    n_updates            | 15430     |
|    policy_gradient_loss | -0.0151   |
|    value_loss           | 0.377     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 233         |
|    time_elapsed         | 2260        |
|    total_timesteps      | 477184      |
| train/                  |             |
|    approx_kl            | 0.045804683 |
|    clip_fraction        | 0.0713      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0844     |
|    explained_variance   | 0.748       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 15440       |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.397       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 211        |
|    iterations           | 234        |
|    time_elapsed         | 2267       |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.03328908 |
|    clip_fraction        | 0.0684     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0915    |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 15450      |
|    policy_gradient_loss | -0.0135    |
|    value_loss           | 0.281      |
----------------------------------------
Eval num_timesteps=480000, episode_reward=7.77 +/- 0.76
Episode length: 16.33 +/- 1.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16.3      |
|    mean_reward          | 7.77      |
| time/                   |           |
|    total_timesteps      | 480000    |
| train/                  |           |
|    approx_kl            | 0.0413481 |
|    clip_fraction        | 0.0876    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.11     |
|    explained_variance   | 0.719     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0984    |
|    n_updates            | 15460     |
|    policy_gradient_loss | -0.0214   |
|    value_loss           | 0.409     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.21     |
| time/              |          |
|    fps             | 211      |
|    iterations      | 235      |
|    time_elapsed    | 2274     |
|    total_timesteps | 481280   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.33        |
| time/                   |             |
|    fps                  | 211         |
|    iterations           | 236         |
|    time_elapsed         | 2281        |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.037539862 |
|    clip_fraction        | 0.0681      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0991     |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0875      |
|    n_updates            | 15470       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.237       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 237         |
|    time_elapsed         | 2288        |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.060236696 |
|    clip_fraction        | 0.084       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0949     |
|    explained_variance   | 0.728       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0618      |
|    n_updates            | 15480       |
|    policy_gradient_loss | -0.00723    |
|    value_loss           | 0.389       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 238         |
|    time_elapsed         | 2296        |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.053079557 |
|    clip_fraction        | 0.0943      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 15490       |
|    policy_gradient_loss | -0.0226     |
|    value_loss           | 0.41        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 212         |
|    iterations           | 239         |
|    time_elapsed         | 2303        |
|    total_timesteps      | 489472      |
| train/                  |             |
|    approx_kl            | 0.047447823 |
|    clip_fraction        | 0.0958      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.165       |
|    n_updates            | 15500       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.549       |
-----------------------------------------
Eval num_timesteps=490000, episode_reward=9.34 +/- 0.64
Episode length: 18.33 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 9.34        |
| time/                   |             |
|    total_timesteps      | 490000      |
| train/                  |             |
|    approx_kl            | 0.048812434 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.108      |
|    explained_variance   | 0.663       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.177       |
|    n_updates            | 15510       |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.502       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 7.99     |
| time/              |          |
|    fps             | 212      |
|    iterations      | 240      |
|    time_elapsed    | 2310     |
|    total_timesteps | 491520   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8.31       |
| time/                   |            |
|    fps                  | 212        |
|    iterations           | 241        |
|    time_elapsed         | 2317       |
|    total_timesteps      | 493568     |
| train/                  |            |
|    approx_kl            | 0.04559069 |
|    clip_fraction        | 0.067      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0857    |
|    explained_variance   | 0.727      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0313     |
|    n_updates            | 15520      |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 0.358      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 8.29        |
| time/                   |             |
|    fps                  | 213         |
|    iterations           | 242         |
|    time_elapsed         | 2324        |
|    total_timesteps      | 495616      |
| train/                  |             |
|    approx_kl            | 0.044623278 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0873     |
|    explained_variance   | 0.658       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.119       |
|    n_updates            | 15530       |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.485       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 8.24       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 243        |
|    time_elapsed         | 2331       |
|    total_timesteps      | 497664     |
| train/                  |            |
|    approx_kl            | 0.03969357 |
|    clip_fraction        | 0.0604     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0828    |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0353     |
|    n_updates            | 15540      |
|    policy_gradient_loss | -0.0116    |
|    value_loss           | 0.242      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 213        |
|    iterations           | 244        |
|    time_elapsed         | 2338       |
|    total_timesteps      | 499712     |
| train/                  |            |
|    approx_kl            | 0.05212252 |
|    clip_fraction        | 0.089      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0724     |
|    n_updates            | 15550      |
|    policy_gradient_loss | -0.0154    |
|    value_loss           | 0.304      |
----------------------------------------
reached max steps=100
Eval num_timesteps=500000, episode_reward=5.98 +/- 5.64
Episode length: 29.00 +/- 14.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 5.98        |
| time/                   |             |
|    total_timesteps      | 500000      |
| train/                  |             |
|    approx_kl            | 0.114239804 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0345      |
|    n_updates            | 15560       |
|    policy_gradient_loss | -0.0223     |
|    value_loss           | 0.284       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.21     |
| time/              |          |
|    fps             | 213      |
|    iterations      | 245      |
|    time_elapsed    | 2345     |
|    total_timesteps | 501760   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 214         |
|    iterations           | 246         |
|    time_elapsed         | 2352        |
|    total_timesteps      | 503808      |
| train/                  |             |
|    approx_kl            | 0.048511684 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.15        |
|    n_updates            | 15570       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.319       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.93       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 247        |
|    time_elapsed         | 2359       |
|    total_timesteps      | 505856     |
| train/                  |            |
|    approx_kl            | 0.10166219 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.547      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0329     |
|    n_updates            | 15580      |
|    policy_gradient_loss | -0.0264    |
|    value_loss           | 0.299      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 7.92       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 248        |
|    time_elapsed         | 2366       |
|    total_timesteps      | 507904     |
| train/                  |            |
|    approx_kl            | 0.03477773 |
|    clip_fraction        | 0.0823     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | 0.741      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0457     |
|    n_updates            | 15590      |
|    policy_gradient_loss | -0.0174    |
|    value_loss           | 0.258      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.91       |
| time/                   |            |
|    fps                  | 214        |
|    iterations           | 249        |
|    time_elapsed         | 2373       |
|    total_timesteps      | 509952     |
| train/                  |            |
|    approx_kl            | 0.04996737 |
|    clip_fraction        | 0.0771     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.105     |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0666     |
|    n_updates            | 15600      |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.264      |
----------------------------------------
Eval num_timesteps=510000, episode_reward=7.57 +/- 0.91
Episode length: 20.33 +/- 4.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.3        |
|    mean_reward          | 7.57        |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.052327458 |
|    clip_fraction        | 0.0896      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0367      |
|    n_updates            | 15610       |
|    policy_gradient_loss | -0.0151     |
|    value_loss           | 0.252       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 250      |
|    time_elapsed    | 2381     |
|    total_timesteps | 512000   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.72        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 251         |
|    time_elapsed         | 2388        |
|    total_timesteps      | 514048      |
| train/                  |             |
|    approx_kl            | 0.037496917 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0847     |
|    explained_variance   | 0.77        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.099       |
|    n_updates            | 15620       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.309       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 252         |
|    time_elapsed         | 2395        |
|    total_timesteps      | 516096      |
| train/                  |             |
|    approx_kl            | 0.060808577 |
|    clip_fraction        | 0.0972      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0333      |
|    n_updates            | 15630       |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.393       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.17        |
| time/                   |             |
|    fps                  | 215         |
|    iterations           | 253         |
|    time_elapsed         | 2402        |
|    total_timesteps      | 518144      |
| train/                  |             |
|    approx_kl            | 0.047076993 |
|    clip_fraction        | 0.0759      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0968     |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0916      |
|    n_updates            | 15640       |
|    policy_gradient_loss | -0.0101     |
|    value_loss           | 0.316       |
-----------------------------------------
Eval num_timesteps=520000, episode_reward=8.67 +/- 0.11
Episode length: 18.33 +/- 4.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 8.67       |
| time/                   |            |
|    total_timesteps      | 520000     |
| train/                  |            |
|    approx_kl            | 0.05628945 |
|    clip_fraction        | 0.0805     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0882    |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0844     |
|    n_updates            | 15650      |
|    policy_gradient_loss | 0.0219     |
|    value_loss           | 0.399      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.01     |
| time/              |          |
|    fps             | 215      |
|    iterations      | 254      |
|    time_elapsed    | 2409     |
|    total_timesteps | 520192   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 255         |
|    time_elapsed         | 2416        |
|    total_timesteps      | 522240      |
| train/                  |             |
|    approx_kl            | 0.042197224 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0682      |
|    n_updates            | 15660       |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.352       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.72        |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 256         |
|    time_elapsed         | 2423        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.039896592 |
|    clip_fraction        | 0.0724      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0939     |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0729      |
|    n_updates            | 15670       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.284       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.2         |
| time/                   |             |
|    fps                  | 216         |
|    iterations           | 257         |
|    time_elapsed         | 2430        |
|    total_timesteps      | 526336      |
| train/                  |             |
|    approx_kl            | 0.046526924 |
|    clip_fraction        | 0.0886      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0949     |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.063       |
|    n_updates            | 15680       |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.34        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 8.36       |
| time/                   |            |
|    fps                  | 216        |
|    iterations           | 258        |
|    time_elapsed         | 2437       |
|    total_timesteps      | 528384     |
| train/                  |            |
|    approx_kl            | 0.03210132 |
|    clip_fraction        | 0.0609     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0651    |
|    explained_variance   | 0.79       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0548     |
|    n_updates            | 15690      |
|    policy_gradient_loss | -0.0167    |
|    value_loss           | 0.245      |
----------------------------------------
reached max steps=100
Eval num_timesteps=530000, episode_reward=3.38 +/- 6.64
Episode length: 28.67 +/- 15.11
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.7        |
|    mean_reward          | 3.38        |
| time/                   |             |
|    total_timesteps      | 530000      |
| train/                  |             |
|    approx_kl            | 0.041683964 |
|    clip_fraction        | 0.0609      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0705     |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0428      |
|    n_updates            | 15700       |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 0.243       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.04     |
| time/              |          |
|    fps             | 216      |
|    iterations      | 259      |
|    time_elapsed    | 2445     |
|    total_timesteps | 530432   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.96       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 260        |
|    time_elapsed         | 2452       |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.06202022 |
|    clip_fraction        | 0.0781     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0814    |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.126      |
|    n_updates            | 15710      |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 0.299      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 7.74       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 261        |
|    time_elapsed         | 2459       |
|    total_timesteps      | 534528     |
| train/                  |            |
|    approx_kl            | 0.07786975 |
|    clip_fraction        | 0.0813     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0865    |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0384     |
|    n_updates            | 15720      |
|    policy_gradient_loss | 0.0162     |
|    value_loss           | 0.388      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 7.89       |
| time/                   |            |
|    fps                  | 217        |
|    iterations           | 262        |
|    time_elapsed         | 2467       |
|    total_timesteps      | 536576     |
| train/                  |            |
|    approx_kl            | 0.03446922 |
|    clip_fraction        | 0.0767     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.102     |
|    explained_variance   | 0.622      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 15730      |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.429      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.97        |
| time/                   |             |
|    fps                  | 217         |
|    iterations           | 263         |
|    time_elapsed         | 2474        |
|    total_timesteps      | 538624      |
| train/                  |             |
|    approx_kl            | 0.055012792 |
|    clip_fraction        | 0.08        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0793     |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.185       |
|    n_updates            | 15740       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.319       |
-----------------------------------------
Eval num_timesteps=540000, episode_reward=8.15 +/- 0.87
Episode length: 17.67 +/- 3.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.15       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.04685875 |
|    clip_fraction        | 0.0729     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0854    |
|    explained_variance   | 0.602      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.161      |
|    n_updates            | 15750      |
|    policy_gradient_loss | 0.00397    |
|    value_loss           | 0.417      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 217      |
|    iterations      | 264      |
|    time_elapsed    | 2482     |
|    total_timesteps | 540672   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.97       |
| time/                   |            |
|    fps                  | 218        |
|    iterations           | 265        |
|    time_elapsed         | 2489       |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.07331651 |
|    clip_fraction        | 0.0875     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.088     |
|    explained_variance   | 0.584      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0571     |
|    n_updates            | 15760      |
|    policy_gradient_loss | -0.0235    |
|    value_loss           | 0.445      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 266         |
|    time_elapsed         | 2496        |
|    total_timesteps      | 544768      |
| train/                  |             |
|    approx_kl            | 0.053309824 |
|    clip_fraction        | 0.0923      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0955     |
|    explained_variance   | 0.41        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0901      |
|    n_updates            | 15770       |
|    policy_gradient_loss | -0.0247     |
|    value_loss           | 0.332       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 267         |
|    time_elapsed         | 2503        |
|    total_timesteps      | 546816      |
| train/                  |             |
|    approx_kl            | 0.062490612 |
|    clip_fraction        | 0.0905      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.096      |
|    explained_variance   | 0.719       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0445      |
|    n_updates            | 15780       |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.357       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 268         |
|    time_elapsed         | 2510        |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.059650257 |
|    clip_fraction        | 0.0877      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.151       |
|    n_updates            | 15790       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.525       |
-----------------------------------------
Eval num_timesteps=550000, episode_reward=9.19 +/- 1.01
Episode length: 19.00 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 9.19        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.050545633 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.098      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 15800       |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.391       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 7.66     |
| time/              |          |
|    fps             | 218      |
|    iterations      | 269      |
|    time_elapsed    | 2518     |
|    total_timesteps | 550912   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.72        |
| time/                   |             |
|    fps                  | 218         |
|    iterations           | 270         |
|    time_elapsed         | 2525        |
|    total_timesteps      | 552960      |
| train/                  |             |
|    approx_kl            | 0.111247495 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.284       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 15810       |
|    policy_gradient_loss | -0.0389     |
|    value_loss           | 0.737       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.92       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 271        |
|    time_elapsed         | 2532       |
|    total_timesteps      | 555008     |
| train/                  |            |
|    approx_kl            | 0.07909556 |
|    clip_fraction        | 0.0836     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.089     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0182     |
|    n_updates            | 15820      |
|    policy_gradient_loss | -0.0277    |
|    value_loss           | 0.41       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 272        |
|    time_elapsed         | 2539       |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.04507232 |
|    clip_fraction        | 0.0646     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0761    |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 15830      |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 0.311      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.05        |
| time/                   |             |
|    fps                  | 219         |
|    iterations           | 273         |
|    time_elapsed         | 2546        |
|    total_timesteps      | 559104      |
| train/                  |             |
|    approx_kl            | 0.041808374 |
|    clip_fraction        | 0.0688      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0882     |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.103       |
|    n_updates            | 15840       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.286       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=560000, episode_reward=3.60 +/- 5.42
Episode length: 27.67 +/- 15.80
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27.7       |
|    mean_reward          | 3.6        |
| time/                   |            |
|    total_timesteps      | 560000     |
| train/                  |            |
|    approx_kl            | 0.06801989 |
|    clip_fraction        | 0.0777     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0888    |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0438     |
|    n_updates            | 15850      |
|    policy_gradient_loss | -0.0171    |
|    value_loss           | 0.283      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.81     |
| time/              |          |
|    fps             | 219      |
|    iterations      | 274      |
|    time_elapsed    | 2554     |
|    total_timesteps | 561152   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.28       |
| time/                   |            |
|    fps                  | 219        |
|    iterations           | 275        |
|    time_elapsed         | 2561       |
|    total_timesteps      | 563200     |
| train/                  |            |
|    approx_kl            | 0.04803285 |
|    clip_fraction        | 0.0807     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0884    |
|    explained_variance   | 0.567      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.114      |
|    n_updates            | 15860      |
|    policy_gradient_loss | 0.00451    |
|    value_loss           | 0.432      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.04        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 276         |
|    time_elapsed         | 2568        |
|    total_timesteps      | 565248      |
| train/                  |             |
|    approx_kl            | 0.026027035 |
|    clip_fraction        | 0.0716      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0912     |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0154      |
|    n_updates            | 15870       |
|    policy_gradient_loss | -0.0172     |
|    value_loss           | 0.269       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 277        |
|    time_elapsed         | 2576       |
|    total_timesteps      | 567296     |
| train/                  |            |
|    approx_kl            | 0.05504025 |
|    clip_fraction        | 0.0822     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0864    |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0335     |
|    n_updates            | 15880      |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.317      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 278         |
|    time_elapsed         | 2583        |
|    total_timesteps      | 569344      |
| train/                  |             |
|    approx_kl            | 0.097998664 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.332       |
|    n_updates            | 15890       |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 0.741       |
-----------------------------------------
Eval num_timesteps=570000, episode_reward=8.90 +/- 0.68
Episode length: 20.33 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.3        |
|    mean_reward          | 8.9         |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.042585127 |
|    clip_fraction        | 0.0797      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.17        |
|    n_updates            | 15900       |
|    policy_gradient_loss | -0.00869    |
|    value_loss           | 0.648       |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 220      |
|    iterations      | 279      |
|    time_elapsed    | 2590     |
|    total_timesteps | 571392   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.83       |
| time/                   |            |
|    fps                  | 220        |
|    iterations           | 280        |
|    time_elapsed         | 2597       |
|    total_timesteps      | 573440     |
| train/                  |            |
|    approx_kl            | 0.04484857 |
|    clip_fraction        | 0.0896     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.221      |
|    n_updates            | 15910      |
|    policy_gradient_loss | -0.00707   |
|    value_loss           | 0.564      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 220         |
|    iterations           | 281         |
|    time_elapsed         | 2604        |
|    total_timesteps      | 575488      |
| train/                  |             |
|    approx_kl            | 0.051995613 |
|    clip_fraction        | 0.0983      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0574      |
|    n_updates            | 15920       |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.431       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 282         |
|    time_elapsed         | 2611        |
|    total_timesteps      | 577536      |
| train/                  |             |
|    approx_kl            | 0.033767354 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0836     |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 15930       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.323       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 283         |
|    time_elapsed         | 2618        |
|    total_timesteps      | 579584      |
| train/                  |             |
|    approx_kl            | 0.043237396 |
|    clip_fraction        | 0.0838      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0809      |
|    n_updates            | 15940       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.551       |
-----------------------------------------
Eval num_timesteps=580000, episode_reward=8.90 +/- 0.68
Episode length: 20.33 +/- 4.19
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 20.3       |
|    mean_reward          | 8.9        |
| time/                   |            |
|    total_timesteps      | 580000     |
| train/                  |            |
|    approx_kl            | 0.03305511 |
|    clip_fraction        | 0.0892     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.126     |
|    explained_variance   | 0.735      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0624     |
|    n_updates            | 15950      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.377      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.7     |
|    ep_rew_mean     | 7.94     |
| time/              |          |
|    fps             | 221      |
|    iterations      | 284      |
|    time_elapsed    | 2625     |
|    total_timesteps | 581632   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 221         |
|    iterations           | 285         |
|    time_elapsed         | 2632        |
|    total_timesteps      | 583680      |
| train/                  |             |
|    approx_kl            | 0.043150455 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 15960       |
|    policy_gradient_loss | -0.0153     |
|    value_loss           | 0.358       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 221        |
|    iterations           | 286        |
|    time_elapsed         | 2639       |
|    total_timesteps      | 585728     |
| train/                  |            |
|    approx_kl            | 0.04568543 |
|    clip_fraction        | 0.0775     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0865    |
|    explained_variance   | 0.418      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.118      |
|    n_updates            | 15970      |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.464      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 287         |
|    time_elapsed         | 2646        |
|    total_timesteps      | 587776      |
| train/                  |             |
|    approx_kl            | 0.065777026 |
|    clip_fraction        | 0.0733      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0876     |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.153       |
|    n_updates            | 15980       |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 0.384       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 8.18        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 288         |
|    time_elapsed         | 2654        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.041095473 |
|    clip_fraction        | 0.0773      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0891     |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.116       |
|    n_updates            | 15990       |
|    policy_gradient_loss | -0.0114     |
|    value_loss           | 0.294       |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=7.64 +/- 0.83
Episode length: 20.00 +/- 7.07
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 7.64        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.037201535 |
|    clip_fraction        | 0.0727      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0834     |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.148       |
|    n_updates            | 16000       |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.308       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.99     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 289      |
|    time_elapsed    | 2661     |
|    total_timesteps | 591872   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.79       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 290        |
|    time_elapsed         | 2668       |
|    total_timesteps      | 593920     |
| train/                  |            |
|    approx_kl            | 0.12761265 |
|    clip_fraction        | 0.0911     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0873    |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0719     |
|    n_updates            | 16010      |
|    policy_gradient_loss | -0.0188    |
|    value_loss           | 0.315      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 20.2     |
|    ep_rew_mean          | 7.78     |
| time/                   |          |
|    fps                  | 222      |
|    iterations           | 291      |
|    time_elapsed         | 2675     |
|    total_timesteps      | 595968   |
| train/                  |          |
|    approx_kl            | 0.129879 |
|    clip_fraction        | 0.0967   |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.0942  |
|    explained_variance   | 0.539    |
|    learning_rate        | 0.0003   |
|    loss                 | 0.163    |
|    n_updates            | 16020    |
|    policy_gradient_loss | -0.0184  |
|    value_loss           | 0.69     |
--------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 8.4        |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 292        |
|    time_elapsed         | 2682       |
|    total_timesteps      | 598016     |
| train/                  |            |
|    approx_kl            | 0.05542612 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.128     |
|    explained_variance   | 0.58       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0699     |
|    n_updates            | 16030      |
|    policy_gradient_loss | -0.0369    |
|    value_loss           | 0.496      |
----------------------------------------
reached max steps=100
Eval num_timesteps=600000, episode_reward=9.05 +/- 0.75
Episode length: 19.67 +/- 3.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 9.05        |
| time/                   |             |
|    total_timesteps      | 600000      |
| train/                  |             |
|    approx_kl            | 0.030835839 |
|    clip_fraction        | 0.0635      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0794     |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.165       |
|    n_updates            | 16040       |
|    policy_gradient_loss | -0.0178     |
|    value_loss           | 0.382       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.21     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 293      |
|    time_elapsed    | 2689     |
|    total_timesteps | 600064   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.06        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 294         |
|    time_elapsed         | 2696        |
|    total_timesteps      | 602112      |
| train/                  |             |
|    approx_kl            | 0.035185724 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.074      |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0647      |
|    n_updates            | 16050       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.291       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 8.07       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 295        |
|    time_elapsed         | 2703       |
|    total_timesteps      | 604160     |
| train/                  |            |
|    approx_kl            | 0.02988169 |
|    clip_fraction        | 0.0712     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0883    |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.116      |
|    n_updates            | 16060      |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 0.351      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.3         |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 296         |
|    time_elapsed         | 2710        |
|    total_timesteps      | 606208      |
| train/                  |             |
|    approx_kl            | 0.022261823 |
|    clip_fraction        | 0.0655      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0869     |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.159       |
|    n_updates            | 16070       |
|    policy_gradient_loss | -0.0121     |
|    value_loss           | 0.333       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 297         |
|    time_elapsed         | 2717        |
|    total_timesteps      | 608256      |
| train/                  |             |
|    approx_kl            | 0.018717064 |
|    clip_fraction        | 0.0553      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.084      |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0838      |
|    n_updates            | 16080       |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.287       |
-----------------------------------------
Eval num_timesteps=610000, episode_reward=8.45 +/- 1.27
Episode length: 19.33 +/- 2.62
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 8.45        |
| time/                   |             |
|    total_timesteps      | 610000      |
| train/                  |             |
|    approx_kl            | 0.034457795 |
|    clip_fraction        | 0.0982      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.75        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.207       |
|    n_updates            | 16090       |
|    policy_gradient_loss | -0.0198     |
|    value_loss           | 0.468       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.13     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 298      |
|    time_elapsed    | 2724     |
|    total_timesteps | 610304   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 299         |
|    time_elapsed         | 2731        |
|    total_timesteps      | 612352      |
| train/                  |             |
|    approx_kl            | 0.054204028 |
|    clip_fraction        | 0.0798      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0773     |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 16100       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.425       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 300         |
|    time_elapsed         | 2738        |
|    total_timesteps      | 614400      |
| train/                  |             |
|    approx_kl            | 0.034298733 |
|    clip_fraction        | 0.0741      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.747       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0794      |
|    n_updates            | 16110       |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.334       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.03       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 301        |
|    time_elapsed         | 2745       |
|    total_timesteps      | 616448     |
| train/                  |            |
|    approx_kl            | 0.02541092 |
|    clip_fraction        | 0.0707     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.769      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0929     |
|    n_updates            | 16120      |
|    policy_gradient_loss | -0.0158    |
|    value_loss           | 0.333      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.01        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 302         |
|    time_elapsed         | 2753        |
|    total_timesteps      | 618496      |
| train/                  |             |
|    approx_kl            | 0.026289765 |
|    clip_fraction        | 0.0568      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0815     |
|    explained_variance   | 0.828       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0394      |
|    n_updates            | 16130       |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.241       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=620000, episode_reward=-2.50 +/- 7.95
Episode length: 38.67 +/- 16.03
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 38.7       |
|    mean_reward          | -2.5       |
| time/                   |            |
|    total_timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.04159194 |
|    clip_fraction        | 0.0754     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.738      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.256      |
|    n_updates            | 16140      |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 0.381      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.09     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 303      |
|    time_elapsed    | 2760     |
|    total_timesteps | 620544   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.66        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 304         |
|    time_elapsed         | 2767        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.042426694 |
|    clip_fraction        | 0.0731      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0975     |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.228       |
|    n_updates            | 16150       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.556       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 8.08        |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 305         |
|    time_elapsed         | 2775        |
|    total_timesteps      | 624640      |
| train/                  |             |
|    approx_kl            | 0.044697165 |
|    clip_fraction        | 0.0968      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0916     |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.144       |
|    n_updates            | 16160       |
|    policy_gradient_loss | -0.0136     |
|    value_loss           | 0.358       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 306        |
|    time_elapsed         | 2782       |
|    total_timesteps      | 626688     |
| train/                  |            |
|    approx_kl            | 0.13152923 |
|    clip_fraction        | 0.0721     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0582    |
|    explained_variance   | 0.557      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0885     |
|    n_updates            | 16170      |
|    policy_gradient_loss | -0.0252    |
|    value_loss           | 0.296      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.2        |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 307        |
|    time_elapsed         | 2789       |
|    total_timesteps      | 628736     |
| train/                  |            |
|    approx_kl            | 0.04078557 |
|    clip_fraction        | 0.0729     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.077     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00521    |
|    n_updates            | 16180      |
|    policy_gradient_loss | -0.0217    |
|    value_loss           | 0.295      |
----------------------------------------
reached max steps=100
Eval num_timesteps=630000, episode_reward=8.44 +/- 1.37
Episode length: 16.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 8.44       |
| time/                   |            |
|    total_timesteps      | 630000     |
| train/                  |            |
|    approx_kl            | 0.03611638 |
|    clip_fraction        | 0.056      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0629    |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0657     |
|    n_updates            | 16190      |
|    policy_gradient_loss | -0.00374   |
|    value_loss           | 0.208      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.95     |
| time/              |          |
|    fps             | 225      |
|    iterations      | 308      |
|    time_elapsed    | 2796     |
|    total_timesteps | 630784   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 309         |
|    time_elapsed         | 2804        |
|    total_timesteps      | 632832      |
| train/                  |             |
|    approx_kl            | 0.028572261 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0981     |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.122       |
|    n_updates            | 16200       |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.445       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.96        |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 310         |
|    time_elapsed         | 2811        |
|    total_timesteps      | 634880      |
| train/                  |             |
|    approx_kl            | 0.043149717 |
|    clip_fraction        | 0.0879      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.08        |
|    n_updates            | 16210       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 0.557       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.29        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 311         |
|    time_elapsed         | 2818        |
|    total_timesteps      | 636928      |
| train/                  |             |
|    approx_kl            | 0.051813133 |
|    clip_fraction        | 0.0966      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0946     |
|    explained_variance   | 0.647       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0305      |
|    n_updates            | 16220       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.378       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 312         |
|    time_elapsed         | 2825        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.024384862 |
|    clip_fraction        | 0.0638      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0883     |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0649      |
|    n_updates            | 16230       |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 0.235       |
-----------------------------------------
Eval num_timesteps=640000, episode_reward=9.64 +/- 0.47
Episode length: 20.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20          |
|    mean_reward          | 9.64        |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.041390553 |
|    clip_fraction        | 0.089       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.119      |
|    explained_variance   | 0.725       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0769      |
|    n_updates            | 16240       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 0.409       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.06     |
| time/              |          |
|    fps             | 226      |
|    iterations      | 313      |
|    time_elapsed    | 2832     |
|    total_timesteps | 641024   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 8.09       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 314        |
|    time_elapsed         | 2839       |
|    total_timesteps      | 643072     |
| train/                  |            |
|    approx_kl            | 0.04524393 |
|    clip_fraction        | 0.0905     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0896     |
|    n_updates            | 16250      |
|    policy_gradient_loss | -0.0183    |
|    value_loss           | 0.331      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.18        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 315         |
|    time_elapsed         | 2847        |
|    total_timesteps      | 645120      |
| train/                  |             |
|    approx_kl            | 0.037591137 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.104      |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 16260       |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.27        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 7.68        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 316         |
|    time_elapsed         | 2854        |
|    total_timesteps      | 647168      |
| train/                  |             |
|    approx_kl            | 0.037286974 |
|    clip_fraction        | 0.0719      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0839     |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.056       |
|    n_updates            | 16270       |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 0.228       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 317         |
|    time_elapsed         | 2861        |
|    total_timesteps      | 649216      |
| train/                  |             |
|    approx_kl            | 0.046407998 |
|    clip_fraction        | 0.0799      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0921     |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.222       |
|    n_updates            | 16280       |
|    policy_gradient_loss | -0.0184     |
|    value_loss           | 0.364       |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=7.25 +/- 1.90
Episode length: 23.33 +/- 4.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 23.3        |
|    mean_reward          | 7.25        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.039881174 |
|    clip_fraction        | 0.0906      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.107       |
|    n_updates            | 16290       |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.369       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.91     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 318      |
|    time_elapsed    | 2868     |
|    total_timesteps | 651264   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.18        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 319         |
|    time_elapsed         | 2876        |
|    total_timesteps      | 653312      |
| train/                  |             |
|    approx_kl            | 0.044089586 |
|    clip_fraction        | 0.075       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.134       |
|    n_updates            | 16300       |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.429       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.05        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 320         |
|    time_elapsed         | 2883        |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.023303825 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.076      |
|    explained_variance   | 0.772       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0915      |
|    n_updates            | 16310       |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.33        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.32        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 321         |
|    time_elapsed         | 2891        |
|    total_timesteps      | 657408      |
| train/                  |             |
|    approx_kl            | 0.026325582 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0923     |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 16320       |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.375       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.95       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 322        |
|    time_elapsed         | 2898       |
|    total_timesteps      | 659456     |
| train/                  |            |
|    approx_kl            | 0.03550581 |
|    clip_fraction        | 0.066      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0684    |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0297     |
|    n_updates            | 16330      |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 0.288      |
----------------------------------------
Eval num_timesteps=660000, episode_reward=8.74 +/- 0.63
Episode length: 18.00 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 8.74        |
| time/                   |             |
|    total_timesteps      | 660000      |
| train/                  |             |
|    approx_kl            | 0.039032802 |
|    clip_fraction        | 0.0644      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0807     |
|    explained_variance   | 0.768       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0872      |
|    n_updates            | 16340       |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.3         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 8.24     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 323      |
|    time_elapsed    | 2905     |
|    total_timesteps | 661504   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.21        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 324         |
|    time_elapsed         | 2913        |
|    total_timesteps      | 663552      |
| train/                  |             |
|    approx_kl            | 0.029329363 |
|    clip_fraction        | 0.0826      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0946     |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0662      |
|    n_updates            | 16350       |
|    policy_gradient_loss | -0.0193     |
|    value_loss           | 0.334       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 21        |
|    ep_rew_mean          | 7.95      |
| time/                   |           |
|    fps                  | 227       |
|    iterations           | 325       |
|    time_elapsed         | 2920      |
|    total_timesteps      | 665600    |
| train/                  |           |
|    approx_kl            | 0.0880052 |
|    clip_fraction        | 0.0732    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0784   |
|    explained_variance   | 0.595     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0902    |
|    n_updates            | 16360     |
|    policy_gradient_loss | -0.0263   |
|    value_loss           | 0.299     |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 326        |
|    time_elapsed         | 2927       |
|    total_timesteps      | 667648     |
| train/                  |            |
|    approx_kl            | 0.04745193 |
|    clip_fraction        | 0.088      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.103     |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0665     |
|    n_updates            | 16370      |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 0.314      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 8.2         |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 327         |
|    time_elapsed         | 2934        |
|    total_timesteps      | 669696      |
| train/                  |             |
|    approx_kl            | 0.034695975 |
|    clip_fraction        | 0.0613      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0801     |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0732      |
|    n_updates            | 16380       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.267       |
-----------------------------------------
Eval num_timesteps=670000, episode_reward=10.00 +/- 0.10
Episode length: 18.33 +/- 0.47
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18.3       |
|    mean_reward          | 10         |
| time/                   |            |
|    total_timesteps      | 670000     |
| train/                  |            |
|    approx_kl            | 0.04720234 |
|    clip_fraction        | 0.0869     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0911    |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0862     |
|    n_updates            | 16390      |
|    policy_gradient_loss | -0.0239    |
|    value_loss           | 0.379      |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.08     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 328      |
|    time_elapsed    | 2941     |
|    total_timesteps | 671744   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.14        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 329         |
|    time_elapsed         | 2948        |
|    total_timesteps      | 673792      |
| train/                  |             |
|    approx_kl            | 0.035026994 |
|    clip_fraction        | 0.0659      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0835     |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.138       |
|    n_updates            | 16400       |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.316       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 330         |
|    time_elapsed         | 2956        |
|    total_timesteps      | 675840      |
| train/                  |             |
|    approx_kl            | 0.040236164 |
|    clip_fraction        | 0.0704      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0884     |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0823      |
|    n_updates            | 16410       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.515       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.51        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 331         |
|    time_elapsed         | 2963        |
|    total_timesteps      | 677888      |
| train/                  |             |
|    approx_kl            | 0.055198953 |
|    clip_fraction        | 0.0994      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.114      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00165     |
|    n_updates            | 16420       |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.475       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22          |
|    ep_rew_mean          | 7.39        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 332         |
|    time_elapsed         | 2971        |
|    total_timesteps      | 679936      |
| train/                  |             |
|    approx_kl            | 0.044432353 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.174       |
|    n_updates            | 16430       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.733       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=680000, episode_reward=3.31 +/- 8.02
Episode length: 29.00 +/- 14.90
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 29         |
|    mean_reward          | 3.31       |
| time/                   |            |
|    total_timesteps      | 680000     |
| train/                  |            |
|    approx_kl            | 0.05000625 |
|    clip_fraction        | 0.0995     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.637      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.399      |
|    n_updates            | 16440      |
|    policy_gradient_loss | -0.0118    |
|    value_loss           | 1.15       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.99     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 333      |
|    time_elapsed    | 2978     |
|    total_timesteps | 681984   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 334        |
|    time_elapsed         | 2985       |
|    total_timesteps      | 684032     |
| train/                  |            |
|    approx_kl            | 0.06508282 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.529      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.274      |
|    n_updates            | 16450      |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.707      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.5        |
|    ep_rew_mean          | 7.16        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 335         |
|    time_elapsed         | 2992        |
|    total_timesteps      | 686080      |
| train/                  |             |
|    approx_kl            | 0.053142704 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.254       |
|    n_updates            | 16460       |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.711       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.07        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 336         |
|    time_elapsed         | 3000        |
|    total_timesteps      | 688128      |
| train/                  |             |
|    approx_kl            | 0.035858154 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.608       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.312       |
|    n_updates            | 16470       |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.779       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=690000, episode_reward=8.51 +/- 1.37
Episode length: 16.00 +/- 1.41
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 8.51       |
| time/                   |            |
|    total_timesteps      | 690000     |
| train/                  |            |
|    approx_kl            | 0.09623098 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0732     |
|    n_updates            | 16480      |
|    policy_gradient_loss | -0.023     |
|    value_loss           | 0.441      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 337      |
|    time_elapsed    | 3007     |
|    total_timesteps | 690176   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 7.62       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 338        |
|    time_elapsed         | 3014       |
|    total_timesteps      | 692224     |
| train/                  |            |
|    approx_kl            | 0.03575188 |
|    clip_fraction        | 0.0868     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0957     |
|    n_updates            | 16490      |
|    policy_gradient_loss | -0.0197    |
|    value_loss           | 0.327      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.03       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 339        |
|    time_elapsed         | 3021       |
|    total_timesteps      | 694272     |
| train/                  |            |
|    approx_kl            | 0.17885314 |
|    clip_fraction        | 0.0918     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.128     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0771     |
|    n_updates            | 16500      |
|    policy_gradient_loss | -0.0176    |
|    value_loss           | 0.47       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 7.77       |
| time/                   |            |
|    fps                  | 229        |
|    iterations           | 340        |
|    time_elapsed         | 3028       |
|    total_timesteps      | 696320     |
| train/                  |            |
|    approx_kl            | 0.03045575 |
|    clip_fraction        | 0.0855     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.13       |
|    n_updates            | 16510      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.338      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 341         |
|    time_elapsed         | 3035        |
|    total_timesteps      | 698368      |
| train/                  |             |
|    approx_kl            | 0.042023875 |
|    clip_fraction        | 0.091       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.671       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.276       |
|    n_updates            | 16520       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 0.727       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=700000, episode_reward=3.09 +/- 9.26
Episode length: 30.00 +/- 14.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 3.09        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.044729345 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.112      |
|    explained_variance   | 0.705       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.154       |
|    n_updates            | 16530       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.529       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.1     |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 230      |
|    iterations      | 342      |
|    time_elapsed    | 3043     |
|    total_timesteps | 700416   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8.1        |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 343        |
|    time_elapsed         | 3050       |
|    total_timesteps      | 702464     |
| train/                  |            |
|    approx_kl            | 0.26787192 |
|    clip_fraction        | 0.0992     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0994    |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0672     |
|    n_updates            | 16540      |
|    policy_gradient_loss | -0.0258    |
|    value_loss           | 0.432      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 344         |
|    time_elapsed         | 3057        |
|    total_timesteps      | 704512      |
| train/                  |             |
|    approx_kl            | 0.040864173 |
|    clip_fraction        | 0.0699      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0815     |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.127       |
|    n_updates            | 16550       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.256       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.84        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 345         |
|    time_elapsed         | 3064        |
|    total_timesteps      | 706560      |
| train/                  |             |
|    approx_kl            | 0.045800272 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0865     |
|    explained_variance   | 0.765       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0918      |
|    n_updates            | 16560       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.313       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.09       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 346        |
|    time_elapsed         | 3071       |
|    total_timesteps      | 708608     |
| train/                  |            |
|    approx_kl            | 0.05323337 |
|    clip_fraction        | 0.0885     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.67       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0513     |
|    n_updates            | 16570      |
|    policy_gradient_loss | -0.00241   |
|    value_loss           | 0.364      |
----------------------------------------
Eval num_timesteps=710000, episode_reward=8.74 +/- 1.43
Episode length: 18.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18         |
|    mean_reward          | 8.74       |
| time/                   |            |
|    total_timesteps      | 710000     |
| train/                  |            |
|    approx_kl            | 0.04183151 |
|    clip_fraction        | 0.0745     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0952    |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.159      |
|    n_updates            | 16580      |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 0.381      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.32     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 347      |
|    time_elapsed    | 3078     |
|    total_timesteps | 710656   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8.23       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 348        |
|    time_elapsed         | 3086       |
|    total_timesteps      | 712704     |
| train/                  |            |
|    approx_kl            | 0.02574613 |
|    clip_fraction        | 0.0582     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.077     |
|    explained_variance   | 0.838      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0509     |
|    n_updates            | 16590      |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 0.227      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 7.8        |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 349        |
|    time_elapsed         | 3093       |
|    total_timesteps      | 714752     |
| train/                  |            |
|    approx_kl            | 0.04293195 |
|    clip_fraction        | 0.0758     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0976    |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.131      |
|    n_updates            | 16600      |
|    policy_gradient_loss | -0.0185    |
|    value_loss           | 0.318      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 350         |
|    time_elapsed         | 3100        |
|    total_timesteps      | 716800      |
| train/                  |             |
|    approx_kl            | 0.060603395 |
|    clip_fraction        | 0.0941      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 16610       |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 0.511       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 351        |
|    time_elapsed         | 3107       |
|    total_timesteps      | 718848     |
| train/                  |            |
|    approx_kl            | 0.04584163 |
|    clip_fraction        | 0.0837     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0529     |
|    n_updates            | 16620      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.423      |
----------------------------------------
Eval num_timesteps=720000, episode_reward=8.60 +/- 0.64
Episode length: 18.67 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.7        |
|    mean_reward          | 8.6         |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.033493802 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0989     |
|    explained_variance   | 0.699       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0838      |
|    n_updates            | 16630       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 0.338       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.36     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 352      |
|    time_elapsed    | 3114     |
|    total_timesteps | 720896   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 353         |
|    time_elapsed         | 3121        |
|    total_timesteps      | 722944      |
| train/                  |             |
|    approx_kl            | 0.027842123 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0659     |
|    explained_variance   | 0.784       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0881      |
|    n_updates            | 16640       |
|    policy_gradient_loss | -0.0155     |
|    value_loss           | 0.254       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 231        |
|    iterations           | 354        |
|    time_elapsed         | 3129       |
|    total_timesteps      | 724992     |
| train/                  |            |
|    approx_kl            | 0.05415461 |
|    clip_fraction        | 0.0903     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 16650      |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 0.481      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 355         |
|    time_elapsed         | 3136        |
|    total_timesteps      | 727040      |
| train/                  |             |
|    approx_kl            | 0.042621046 |
|    clip_fraction        | 0.093       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.706       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.166       |
|    n_updates            | 16660       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.688       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 356         |
|    time_elapsed         | 3143        |
|    total_timesteps      | 729088      |
| train/                  |             |
|    approx_kl            | 0.028096218 |
|    clip_fraction        | 0.0834      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0697      |
|    n_updates            | 16670       |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.52        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=730000, episode_reward=9.48 +/- 1.00
Episode length: 17.67 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 9.48        |
| time/                   |             |
|    total_timesteps      | 730000      |
| train/                  |             |
|    approx_kl            | 0.045434095 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.145      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.199       |
|    n_updates            | 16680       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 0.571       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 7.59     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 357      |
|    time_elapsed    | 3151     |
|    total_timesteps | 731136   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 358        |
|    time_elapsed         | 3157       |
|    total_timesteps      | 733184     |
| train/                  |            |
|    approx_kl            | 0.06662278 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.159     |
|    explained_variance   | 0.653      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.158      |
|    n_updates            | 16690      |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.589      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 359         |
|    time_elapsed         | 3165        |
|    total_timesteps      | 735232      |
| train/                  |             |
|    approx_kl            | 0.046221875 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.626       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0834      |
|    n_updates            | 16700       |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 0.615       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.7       |
|    ep_rew_mean          | 7.23       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 360        |
|    time_elapsed         | 3172       |
|    total_timesteps      | 737280     |
| train/                  |            |
|    approx_kl            | 0.04215943 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.157     |
|    explained_variance   | 0.632      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.327      |
|    n_updates            | 16710      |
|    policy_gradient_loss | -0.0214    |
|    value_loss           | 0.663      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 7.89       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 361        |
|    time_elapsed         | 3180       |
|    total_timesteps      | 739328     |
| train/                  |            |
|    approx_kl            | 0.05049405 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.167     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.105      |
|    n_updates            | 16720      |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.707      |
----------------------------------------
Eval num_timesteps=740000, episode_reward=8.44 +/- 0.10
Episode length: 16.33 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 8.44        |
| time/                   |             |
|    total_timesteps      | 740000      |
| train/                  |             |
|    approx_kl            | 0.050110847 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.134      |
|    explained_variance   | 0.675       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0335      |
|    n_updates            | 16730       |
|    policy_gradient_loss | -0.0194     |
|    value_loss           | 0.489       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 7.76     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 362      |
|    time_elapsed    | 3187     |
|    total_timesteps | 741376   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.45       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 363        |
|    time_elapsed         | 3194       |
|    total_timesteps      | 743424     |
| train/                  |            |
|    approx_kl            | 0.03128598 |
|    clip_fraction        | 0.0834     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.739      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0836     |
|    n_updates            | 16740      |
|    policy_gradient_loss | -0.0147    |
|    value_loss           | 0.29       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20         |
|    ep_rew_mean          | 7.81       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 364        |
|    time_elapsed         | 3201       |
|    total_timesteps      | 745472     |
| train/                  |            |
|    approx_kl            | 0.06254343 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.153     |
|    explained_variance   | 0.586      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0791     |
|    n_updates            | 16750      |
|    policy_gradient_loss | -0.0255    |
|    value_loss           | 0.606      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 365         |
|    time_elapsed         | 3209        |
|    total_timesteps      | 747520      |
| train/                  |             |
|    approx_kl            | 0.038106233 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.123      |
|    explained_variance   | 0.669       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.121       |
|    n_updates            | 16760       |
|    policy_gradient_loss | 0.0035      |
|    value_loss           | 0.403       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 7.93        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 366         |
|    time_elapsed         | 3217        |
|    total_timesteps      | 749568      |
| train/                  |             |
|    approx_kl            | 0.111218855 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.149      |
|    explained_variance   | 0.416       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 16770       |
|    policy_gradient_loss | -0.0336     |
|    value_loss           | 0.596       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=750000, episode_reward=3.31 +/- 9.41
Episode length: 29.00 +/- 14.85
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29          |
|    mean_reward          | 3.31        |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.034281902 |
|    clip_fraction        | 0.0845      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.104       |
|    n_updates            | 16780       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 0.317       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 7.54     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 367      |
|    time_elapsed    | 3225     |
|    total_timesteps | 751616   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.55        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 368         |
|    time_elapsed         | 3233        |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.058002263 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.142      |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.285       |
|    n_updates            | 16790       |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 0.534       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 369         |
|    time_elapsed         | 3241        |
|    total_timesteps      | 755712      |
| train/                  |             |
|    approx_kl            | 0.028821649 |
|    clip_fraction        | 0.0976      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.124      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0929      |
|    n_updates            | 16800       |
|    policy_gradient_loss | -0.0208     |
|    value_loss           | 0.496       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.8       |
|    ep_rew_mean          | 8.07       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 370        |
|    time_elapsed         | 3248       |
|    total_timesteps      | 757760     |
| train/                  |            |
|    approx_kl            | 0.13460594 |
|    clip_fraction        | 0.0956     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.093     |
|    explained_variance   | 0.721      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0261     |
|    n_updates            | 16810      |
|    policy_gradient_loss | -0.0286    |
|    value_loss           | 0.501      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 7.88        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 371         |
|    time_elapsed         | 3258        |
|    total_timesteps      | 759808      |
| train/                  |             |
|    approx_kl            | 0.031987824 |
|    clip_fraction        | 0.0643      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0848     |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0581      |
|    n_updates            | 16820       |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.26        |
-----------------------------------------
Eval num_timesteps=760000, episode_reward=7.22 +/- 1.25
Episode length: 25.00 +/- 5.72
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25         |
|    mean_reward          | 7.22       |
| time/                   |            |
|    total_timesteps      | 760000     |
| train/                  |            |
|    approx_kl            | 0.21803513 |
|    clip_fraction        | 0.1        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.685      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.164      |
|    n_updates            | 16830      |
|    policy_gradient_loss | -0.00864   |
|    value_loss           | 0.699      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.13     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 372      |
|    time_elapsed    | 3267     |
|    total_timesteps | 761856   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.13        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 373         |
|    time_elapsed         | 3279        |
|    total_timesteps      | 763904      |
| train/                  |             |
|    approx_kl            | 0.040677797 |
|    clip_fraction        | 0.0782      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0897     |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 16840       |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.383       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.92        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 374         |
|    time_elapsed         | 3290        |
|    total_timesteps      | 765952      |
| train/                  |             |
|    approx_kl            | 0.045971382 |
|    clip_fraction        | 0.0937      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0931     |
|    explained_variance   | 0.803       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0947      |
|    n_updates            | 16850       |
|    policy_gradient_loss | -0.0147     |
|    value_loss           | 0.233       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 8.01       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 375        |
|    time_elapsed         | 3301       |
|    total_timesteps      | 768000     |
| train/                  |            |
|    approx_kl            | 0.09005801 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.131      |
|    n_updates            | 16860      |
|    policy_gradient_loss | -0.00945   |
|    value_loss           | 0.304      |
----------------------------------------
Eval num_timesteps=770000, episode_reward=8.89 +/- 0.53
Episode length: 17.33 +/- 1.89
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.89        |
| time/                   |             |
|    total_timesteps      | 770000      |
| train/                  |             |
|    approx_kl            | 0.040456515 |
|    clip_fraction        | 0.0804      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0973     |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.106       |
|    n_updates            | 16870       |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.461       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 7.8      |
| time/              |          |
|    fps             | 232      |
|    iterations      | 376      |
|    time_elapsed    | 3310     |
|    total_timesteps | 770048   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 377         |
|    time_elapsed         | 3318        |
|    total_timesteps      | 772096      |
| train/                  |             |
|    approx_kl            | 0.032359935 |
|    clip_fraction        | 0.0738      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.096      |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.126       |
|    n_updates            | 16880       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.32        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.02        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 378         |
|    time_elapsed         | 3328        |
|    total_timesteps      | 774144      |
| train/                  |             |
|    approx_kl            | 0.034104377 |
|    clip_fraction        | 0.0871      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.713       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.103       |
|    n_updates            | 16890       |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.352       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 379         |
|    time_elapsed         | 3338        |
|    total_timesteps      | 776192      |
| train/                  |             |
|    approx_kl            | 0.028788706 |
|    clip_fraction        | 0.0562      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0668     |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0861      |
|    n_updates            | 16900       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 0.324       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.79        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 380         |
|    time_elapsed         | 3349        |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.039797816 |
|    clip_fraction        | 0.0729      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0767     |
|    explained_variance   | 0.737       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.109       |
|    n_updates            | 16910       |
|    policy_gradient_loss | -0.0098     |
|    value_loss           | 0.278       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=780000, episode_reward=5.83 +/- 5.54
Episode length: 29.67 +/- 14.38
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 5.83        |
| time/                   |             |
|    total_timesteps      | 780000      |
| train/                  |             |
|    approx_kl            | 0.041953176 |
|    clip_fraction        | 0.0749      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.232       |
|    n_updates            | 16920       |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 0.551       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.11     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 381      |
|    time_elapsed    | 3357     |
|    total_timesteps | 780288   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.77        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 382         |
|    time_elapsed         | 3364        |
|    total_timesteps      | 782336      |
| train/                  |             |
|    approx_kl            | 0.025615253 |
|    clip_fraction        | 0.0633      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0886     |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.182       |
|    n_updates            | 16930       |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.462       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 7.69        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 383         |
|    time_elapsed         | 3371        |
|    total_timesteps      | 784384      |
| train/                  |             |
|    approx_kl            | 0.039010994 |
|    clip_fraction        | 0.0888      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.697       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0847      |
|    n_updates            | 16940       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.37        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 7.77       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 384        |
|    time_elapsed         | 3378       |
|    total_timesteps      | 786432     |
| train/                  |            |
|    approx_kl            | 0.12302217 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.5        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.137      |
|    n_updates            | 16950      |
|    policy_gradient_loss | -0.0266    |
|    value_loss           | 0.68       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 385        |
|    time_elapsed         | 3384       |
|    total_timesteps      | 788480     |
| train/                  |            |
|    approx_kl            | 0.04183559 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.134     |
|    explained_variance   | 0.626      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.118      |
|    n_updates            | 16960      |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 0.585      |
----------------------------------------
Eval num_timesteps=790000, episode_reward=8.61 +/- 0.42
Episode length: 21.67 +/- 3.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 21.7       |
|    mean_reward          | 8.61       |
| time/                   |            |
|    total_timesteps      | 790000     |
| train/                  |            |
|    approx_kl            | 0.04478818 |
|    clip_fraction        | 0.0773     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0992    |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0858     |
|    n_updates            | 16970      |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.42       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.01     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 386      |
|    time_elapsed    | 3391     |
|    total_timesteps | 790528   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 387         |
|    time_elapsed         | 3400        |
|    total_timesteps      | 792576      |
| train/                  |             |
|    approx_kl            | 0.051246475 |
|    clip_fraction        | 0.0691      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0707     |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0849      |
|    n_updates            | 16980       |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.452       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.95       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 388        |
|    time_elapsed         | 3408       |
|    total_timesteps      | 794624     |
| train/                  |            |
|    approx_kl            | 0.04182817 |
|    clip_fraction        | 0.0675     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.082     |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0773     |
|    n_updates            | 16990      |
|    policy_gradient_loss | -0.00753   |
|    value_loss           | 0.326      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 8.14        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 389         |
|    time_elapsed         | 3415        |
|    total_timesteps      | 796672      |
| train/                  |             |
|    approx_kl            | 0.045066573 |
|    clip_fraction        | 0.0693      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0799     |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0425      |
|    n_updates            | 17000       |
|    policy_gradient_loss | -0.0142     |
|    value_loss           | 0.248       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.11       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 390        |
|    time_elapsed         | 3422       |
|    total_timesteps      | 798720     |
| train/                  |            |
|    approx_kl            | 0.06753771 |
|    clip_fraction        | 0.0757     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0655    |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0584     |
|    n_updates            | 17010      |
|    policy_gradient_loss | -0.0184    |
|    value_loss           | 0.263      |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=800000, episode_reward=-2.64 +/- 9.03
Episode length: 39.33 +/- 15.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.3        |
|    mean_reward          | -2.64       |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.079003155 |
|    clip_fraction        | 0.0678      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0706     |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.103       |
|    n_updates            | 17020       |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.356       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 8.01     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 391      |
|    time_elapsed    | 3429     |
|    total_timesteps | 800768   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 392        |
|    time_elapsed         | 3438       |
|    total_timesteps      | 802816     |
| train/                  |            |
|    approx_kl            | 0.07660012 |
|    clip_fraction        | 0.0613     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.062     |
|    explained_variance   | 0.736      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.17       |
|    n_updates            | 17030      |
|    policy_gradient_loss | -0.0122    |
|    value_loss           | 0.275      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.2         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 393         |
|    time_elapsed         | 3445        |
|    total_timesteps      | 804864      |
| train/                  |             |
|    approx_kl            | 0.043208327 |
|    clip_fraction        | 0.0574      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0689     |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0811      |
|    n_updates            | 17040       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 0.437       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8           |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 394         |
|    time_elapsed         | 3452        |
|    total_timesteps      | 806912      |
| train/                  |             |
|    approx_kl            | 0.042700812 |
|    clip_fraction        | 0.0541      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0633     |
|    explained_variance   | 0.833       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.085       |
|    n_updates            | 17050       |
|    policy_gradient_loss | -0.0104     |
|    value_loss           | 0.199       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 8.09       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 395        |
|    time_elapsed         | 3460       |
|    total_timesteps      | 808960     |
| train/                  |            |
|    approx_kl            | 0.03520735 |
|    clip_fraction        | 0.0673     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0749    |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0946     |
|    n_updates            | 17060      |
|    policy_gradient_loss | -0.000941  |
|    value_loss           | 0.4        |
----------------------------------------
Eval num_timesteps=810000, episode_reward=8.29 +/- 0.47
Episode length: 17.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17         |
|    mean_reward          | 8.29       |
| time/                   |            |
|    total_timesteps      | 810000     |
| train/                  |            |
|    approx_kl            | 0.10173106 |
|    clip_fraction        | 0.0753     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0726    |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0545     |
|    n_updates            | 17070      |
|    policy_gradient_loss | -0.0182    |
|    value_loss           | 0.355      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 8.17     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 396      |
|    time_elapsed    | 3467     |
|    total_timesteps | 811008   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 7.67       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 397        |
|    time_elapsed         | 3474       |
|    total_timesteps      | 813056     |
| train/                  |            |
|    approx_kl            | 0.04977484 |
|    clip_fraction        | 0.0758     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0858    |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0899     |
|    n_updates            | 17080      |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 0.359      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.95       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 398        |
|    time_elapsed         | 3483       |
|    total_timesteps      | 815104     |
| train/                  |            |
|    approx_kl            | 0.13381271 |
|    clip_fraction        | 0.0793     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0871    |
|    explained_variance   | 0.723      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.031      |
|    n_updates            | 17090      |
|    policy_gradient_loss | -0.00601   |
|    value_loss           | 0.247      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 8.06        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 399         |
|    time_elapsed         | 3491        |
|    total_timesteps      | 817152      |
| train/                  |             |
|    approx_kl            | 0.035101067 |
|    clip_fraction        | 0.0565      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0612     |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 17100       |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.247       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 8.23        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 400         |
|    time_elapsed         | 3499        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.025486298 |
|    clip_fraction        | 0.0474      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0663     |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0944      |
|    n_updates            | 17110       |
|    policy_gradient_loss | -0.00925    |
|    value_loss           | 0.291       |
-----------------------------------------
Eval num_timesteps=820000, episode_reward=9.34 +/- 0.76
Episode length: 18.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 9.34        |
| time/                   |             |
|    total_timesteps      | 820000      |
| train/                  |             |
|    approx_kl            | 0.029402718 |
|    clip_fraction        | 0.0593      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0694     |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0799      |
|    n_updates            | 17120       |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.331       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.18     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 401      |
|    time_elapsed    | 3508     |
|    total_timesteps | 821248   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 8.05       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 402        |
|    time_elapsed         | 3519       |
|    total_timesteps      | 823296     |
| train/                  |            |
|    approx_kl            | 0.03002037 |
|    clip_fraction        | 0.0639     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0672    |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.108      |
|    n_updates            | 17130      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.275      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 8.37       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 403        |
|    time_elapsed         | 3530       |
|    total_timesteps      | 825344     |
| train/                  |            |
|    approx_kl            | 0.06964038 |
|    clip_fraction        | 0.0766     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0692    |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.131      |
|    n_updates            | 17140      |
|    policy_gradient_loss | -0.0157    |
|    value_loss           | 0.34       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 404         |
|    time_elapsed         | 3538        |
|    total_timesteps      | 827392      |
| train/                  |             |
|    approx_kl            | 0.028500803 |
|    clip_fraction        | 0.0614      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0632     |
|    explained_variance   | 0.562       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.146       |
|    n_updates            | 17150       |
|    policy_gradient_loss | -0.0239     |
|    value_loss           | 0.302       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.18        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 405         |
|    time_elapsed         | 3546        |
|    total_timesteps      | 829440      |
| train/                  |             |
|    approx_kl            | 0.064496204 |
|    clip_fraction        | 0.0806      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0657      |
|    n_updates            | 17160       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 0.48        |
-----------------------------------------
Eval num_timesteps=830000, episode_reward=7.32 +/- 1.72
Episode length: 23.00 +/- 4.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 23         |
|    mean_reward          | 7.32       |
| time/                   |            |
|    total_timesteps      | 830000     |
| train/                  |            |
|    approx_kl            | 0.07037019 |
|    clip_fraction        | 0.0662     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.07      |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.075      |
|    n_updates            | 17170      |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.327      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.9     |
|    ep_rew_mean     | 8.19     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 406      |
|    time_elapsed    | 3555     |
|    total_timesteps | 831488   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 8.16       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 407        |
|    time_elapsed         | 3562       |
|    total_timesteps      | 833536     |
| train/                  |            |
|    approx_kl            | 0.07757677 |
|    clip_fraction        | 0.0697     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0745    |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0819     |
|    n_updates            | 17180      |
|    policy_gradient_loss | -0.016     |
|    value_loss           | 0.299      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 8.19        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 408         |
|    time_elapsed         | 3569        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.040174056 |
|    clip_fraction        | 0.0696      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0797     |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.152       |
|    n_updates            | 17190       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.402       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 7.71        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 409         |
|    time_elapsed         | 3578        |
|    total_timesteps      | 837632      |
| train/                  |             |
|    approx_kl            | 0.039259657 |
|    clip_fraction        | 0.0677      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0788     |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.147       |
|    n_updates            | 17200       |
|    policy_gradient_loss | -0.0126     |
|    value_loss           | 0.295       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 8.03        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 410         |
|    time_elapsed         | 3586        |
|    total_timesteps      | 839680      |
| train/                  |             |
|    approx_kl            | 0.058168057 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 17210       |
|    policy_gradient_loss | -0.0158     |
|    value_loss           | 0.695       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=840000, episode_reward=7.70 +/- 0.69
Episode length: 16.67 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | 7.7         |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.043064732 |
|    clip_fraction        | 0.0766      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0883     |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.113       |
|    n_updates            | 17220       |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.396       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 7.98     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 411      |
|    time_elapsed    | 3594     |
|    total_timesteps | 841728   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.8       |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 412        |
|    time_elapsed         | 3601       |
|    total_timesteps      | 843776     |
| train/                  |            |
|    approx_kl            | 0.03985014 |
|    clip_fraction        | 0.0791     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0975    |
|    explained_variance   | 0.676      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.147      |
|    n_updates            | 17230      |
|    policy_gradient_loss | -0.0193    |
|    value_loss           | 0.553      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.83        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 413         |
|    time_elapsed         | 3608        |
|    total_timesteps      | 845824      |
| train/                  |             |
|    approx_kl            | 0.040467814 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.512       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.132       |
|    n_updates            | 17240       |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 0.76        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 414         |
|    time_elapsed         | 3616        |
|    total_timesteps      | 847872      |
| train/                  |             |
|    approx_kl            | 0.040736496 |
|    clip_fraction        | 0.0968      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.323       |
|    n_updates            | 17250       |
|    policy_gradient_loss | -0.00857    |
|    value_loss           | 0.719       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 415         |
|    time_elapsed         | 3627        |
|    total_timesteps      | 849920      |
| train/                  |             |
|    approx_kl            | 0.075440824 |
|    clip_fraction        | 0.0921      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 17260       |
|    policy_gradient_loss | -0.02       |
|    value_loss           | 0.563       |
-----------------------------------------
Eval num_timesteps=850000, episode_reward=9.41 +/- 0.79
Episode length: 18.00 +/- 0.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18         |
|    mean_reward          | 9.41       |
| time/                   |            |
|    total_timesteps      | 850000     |
| train/                  |            |
|    approx_kl            | 0.06464903 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.118     |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.165      |
|    n_updates            | 17270      |
|    policy_gradient_loss | -0.0117    |
|    value_loss           | 0.552      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.9     |
|    ep_rew_mean     | 7.43     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 416      |
|    time_elapsed    | 3636     |
|    total_timesteps | 851968   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.95       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 417        |
|    time_elapsed         | 3647       |
|    total_timesteps      | 854016     |
| train/                  |            |
|    approx_kl            | 0.07225239 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.497      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.328      |
|    n_updates            | 17280      |
|    policy_gradient_loss | -0.0234    |
|    value_loss           | 0.825      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8.23       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 418        |
|    time_elapsed         | 3655       |
|    total_timesteps      | 856064     |
| train/                  |            |
|    approx_kl            | 0.05706133 |
|    clip_fraction        | 0.0842     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.75       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0362     |
|    n_updates            | 17290      |
|    policy_gradient_loss | -0.0181    |
|    value_loss           | 0.295      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 419         |
|    time_elapsed         | 3663        |
|    total_timesteps      | 858112      |
| train/                  |             |
|    approx_kl            | 0.029166834 |
|    clip_fraction        | 0.0646      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0885     |
|    explained_variance   | 0.835       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0562      |
|    n_updates            | 17300       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.23        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=860000, episode_reward=1.53 +/- 8.37
Episode length: 28.00 +/- 15.75
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28         |
|    mean_reward          | 1.53       |
| time/                   |            |
|    total_timesteps      | 860000     |
| train/                  |            |
|    approx_kl            | 0.07367235 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.129     |
|    explained_variance   | 0.609      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.186      |
|    n_updates            | 17310      |
|    policy_gradient_loss | -0.0133    |
|    value_loss           | 0.635      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.14     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 420      |
|    time_elapsed    | 3672     |
|    total_timesteps | 860160   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 7.43       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 421        |
|    time_elapsed         | 3680       |
|    total_timesteps      | 862208     |
| train/                  |            |
|    approx_kl            | 0.08515896 |
|    clip_fraction        | 0.0696     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0843    |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0801     |
|    n_updates            | 17320      |
|    policy_gradient_loss | -0.015     |
|    value_loss           | 0.311      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 422         |
|    time_elapsed         | 3693        |
|    total_timesteps      | 864256      |
| train/                  |             |
|    approx_kl            | 0.105020374 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.463       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.13        |
|    n_updates            | 17330       |
|    policy_gradient_loss | 0.0575      |
|    value_loss           | 0.602       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 423        |
|    time_elapsed         | 3701       |
|    total_timesteps      | 866304     |
| train/                  |            |
|    approx_kl            | 0.06194703 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0508     |
|    n_updates            | 17340      |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 0.428      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 8.05        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 424         |
|    time_elapsed         | 3710        |
|    total_timesteps      | 868352      |
| train/                  |             |
|    approx_kl            | 0.028281862 |
|    clip_fraction        | 0.0995      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.632       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.035       |
|    n_updates            | 17350       |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.437       |
-----------------------------------------
Eval num_timesteps=870000, episode_reward=8.68 +/- 0.29
Episode length: 21.33 +/- 3.09
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21.3        |
|    mean_reward          | 8.68        |
| time/                   |             |
|    total_timesteps      | 870000      |
| train/                  |             |
|    approx_kl            | 0.036389414 |
|    clip_fraction        | 0.0712      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.1        |
|    explained_variance   | 0.758       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.158       |
|    n_updates            | 17360       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 0.458       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 233      |
|    iterations      | 425      |
|    time_elapsed    | 3724     |
|    total_timesteps | 870400   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.71       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 426        |
|    time_elapsed         | 3734       |
|    total_timesteps      | 872448     |
| train/                  |            |
|    approx_kl            | 0.04289514 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.141     |
|    explained_variance   | 0.749      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.113      |
|    n_updates            | 17370      |
|    policy_gradient_loss | -0.0228    |
|    value_loss           | 0.516      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 7.65       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 427        |
|    time_elapsed         | 3742       |
|    total_timesteps      | 874496     |
| train/                  |            |
|    approx_kl            | 0.06479901 |
|    clip_fraction        | 0.0856     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.111     |
|    explained_variance   | 0.682      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.185      |
|    n_updates            | 17380      |
|    policy_gradient_loss | -0.02      |
|    value_loss           | 0.548      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 428         |
|    time_elapsed         | 3751        |
|    total_timesteps      | 876544      |
| train/                  |             |
|    approx_kl            | 0.065793514 |
|    clip_fraction        | 0.0873      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.633       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.146       |
|    n_updates            | 17390       |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 0.548       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 7.71       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 429        |
|    time_elapsed         | 3760       |
|    total_timesteps      | 878592     |
| train/                  |            |
|    approx_kl            | 0.03235973 |
|    clip_fraction        | 0.0897     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.121     |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.253      |
|    n_updates            | 17400      |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 0.414      |
----------------------------------------
Eval num_timesteps=880000, episode_reward=8.40 +/- 0.37
Episode length: 25.67 +/- 1.70
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.7       |
|    mean_reward          | 8.4        |
| time/                   |            |
|    total_timesteps      | 880000     |
| train/                  |            |
|    approx_kl            | 0.30405748 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0995    |
|    explained_variance   | 0.635      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.195      |
|    n_updates            | 17410      |
|    policy_gradient_loss | -0.0272    |
|    value_loss           | 0.469      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 8.2      |
| time/              |          |
|    fps             | 233      |
|    iterations      | 430      |
|    time_elapsed    | 3768     |
|    total_timesteps | 880640   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 7.79       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 431        |
|    time_elapsed         | 3777       |
|    total_timesteps      | 882688     |
| train/                  |            |
|    approx_kl            | 0.03376746 |
|    clip_fraction        | 0.0875     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0973    |
|    explained_variance   | 0.811      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0765     |
|    n_updates            | 17420      |
|    policy_gradient_loss | -0.0119    |
|    value_loss           | 0.236      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 8.16        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 432         |
|    time_elapsed         | 3785        |
|    total_timesteps      | 884736      |
| train/                  |             |
|    approx_kl            | 0.057334423 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.708       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.14        |
|    n_updates            | 17430       |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 0.382       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 8.11       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 433        |
|    time_elapsed         | 3794       |
|    total_timesteps      | 886784     |
| train/                  |            |
|    approx_kl            | 0.08357942 |
|    clip_fraction        | 0.0903     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0941    |
|    explained_variance   | 0.591      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.121      |
|    n_updates            | 17440      |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.441      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.82        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 434         |
|    time_elapsed         | 3807        |
|    total_timesteps      | 888832      |
| train/                  |             |
|    approx_kl            | 0.031548366 |
|    clip_fraction        | 0.0689      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0946     |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.133       |
|    n_updates            | 17450       |
|    policy_gradient_loss | -0.0141     |
|    value_loss           | 0.334       |
-----------------------------------------
Eval num_timesteps=890000, episode_reward=8.74 +/- 1.45
Episode length: 18.00 +/- 2.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18          |
|    mean_reward          | 8.74        |
| time/                   |             |
|    total_timesteps      | 890000      |
| train/                  |             |
|    approx_kl            | 0.024439158 |
|    clip_fraction        | 0.0679      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0914     |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.123       |
|    n_updates            | 17460       |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.267       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 7.94     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 435      |
|    time_elapsed    | 3817     |
|    total_timesteps | 890880   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 8.12       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 436        |
|    time_elapsed         | 3825       |
|    total_timesteps      | 892928     |
| train/                  |            |
|    approx_kl            | 0.07511839 |
|    clip_fraction        | 0.0992     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.106     |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0665     |
|    n_updates            | 17470      |
|    policy_gradient_loss | -0.0233    |
|    value_loss           | 0.332      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 437         |
|    time_elapsed         | 3833        |
|    total_timesteps      | 894976      |
| train/                  |             |
|    approx_kl            | 0.035995737 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0919     |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0845      |
|    n_updates            | 17480       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.324       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 8.22       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 438        |
|    time_elapsed         | 3841       |
|    total_timesteps      | 897024     |
| train/                  |            |
|    approx_kl            | 0.03446764 |
|    clip_fraction        | 0.0772     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0835    |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0572     |
|    n_updates            | 17490      |
|    policy_gradient_loss | -0.0226    |
|    value_loss           | 0.257      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 8.38        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 439         |
|    time_elapsed         | 3850        |
|    total_timesteps      | 899072      |
| train/                  |             |
|    approx_kl            | 0.031316336 |
|    clip_fraction        | 0.0626      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0823     |
|    explained_variance   | 0.821       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.172       |
|    n_updates            | 17500       |
|    policy_gradient_loss | -0.0127     |
|    value_loss           | 0.24        |
-----------------------------------------
Eval num_timesteps=900000, episode_reward=8.83 +/- 0.29
Episode length: 20.67 +/- 3.40
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 20.7        |
|    mean_reward          | 8.83        |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.025469005 |
|    clip_fraction        | 0.0575      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0752     |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.079       |
|    n_updates            | 17510       |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 8.17     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 440      |
|    time_elapsed    | 3858     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 8.28        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 441         |
|    time_elapsed         | 3868        |
|    total_timesteps      | 903168      |
| train/                  |             |
|    approx_kl            | 0.037875216 |
|    clip_fraction        | 0.0605      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0736     |
|    explained_variance   | 0.754       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.129       |
|    n_updates            | 17520       |
|    policy_gradient_loss | -0.00971    |
|    value_loss           | 0.335       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.6       |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 442        |
|    time_elapsed         | 3880       |
|    total_timesteps      | 905216     |
| train/                  |            |
|    approx_kl            | 0.03404338 |
|    clip_fraction        | 0.0719     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0938    |
|    explained_variance   | 0.798      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0997     |
|    n_updates            | 17530      |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 0.289      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.54       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 443        |
|    time_elapsed         | 3892       |
|    total_timesteps      | 907264     |
| train/                  |            |
|    approx_kl            | 0.10272804 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.535      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.255      |
|    n_updates            | 17540      |
|    policy_gradient_loss | -0.0294    |
|    value_loss           | 0.871      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.1        |
|    ep_rew_mean          | 7.89        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 444         |
|    time_elapsed         | 3900        |
|    total_timesteps      | 909312      |
| train/                  |             |
|    approx_kl            | 0.077974334 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.528       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0877      |
|    n_updates            | 17550       |
|    policy_gradient_loss | -0.0349     |
|    value_loss           | 0.817       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=910000, episode_reward=8.54 +/- 1.30
Episode length: 22.00 +/- 4.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 22          |
|    mean_reward          | 8.54        |
| time/                   |             |
|    total_timesteps      | 910000      |
| train/                  |             |
|    approx_kl            | 0.045413308 |
|    clip_fraction        | 0.0889      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.121      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.147       |
|    n_updates            | 17560       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.726       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.8     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 233      |
|    iterations      | 445      |
|    time_elapsed    | 3909     |
|    total_timesteps | 911360   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.7        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 446         |
|    time_elapsed         | 3917        |
|    total_timesteps      | 913408      |
| train/                  |             |
|    approx_kl            | 0.033774562 |
|    clip_fraction        | 0.0924      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.11        |
|    n_updates            | 17570       |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 0.91        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.76        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 447         |
|    time_elapsed         | 3928        |
|    total_timesteps      | 915456      |
| train/                  |             |
|    approx_kl            | 0.039371703 |
|    clip_fraction        | 0.0894      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.219       |
|    n_updates            | 17580       |
|    policy_gradient_loss | -0.0173     |
|    value_loss           | 0.574       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 448        |
|    time_elapsed         | 3936       |
|    total_timesteps      | 917504     |
| train/                  |            |
|    approx_kl            | 0.04794788 |
|    clip_fraction        | 0.0956     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.663      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.102      |
|    n_updates            | 17590      |
|    policy_gradient_loss | -0.0274    |
|    value_loss           | 0.575      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.9         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 449         |
|    time_elapsed         | 3944        |
|    total_timesteps      | 919552      |
| train/                  |             |
|    approx_kl            | 0.035716645 |
|    clip_fraction        | 0.0898      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.109      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.206       |
|    n_updates            | 17600       |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.776       |
-----------------------------------------
Eval num_timesteps=920000, episode_reward=7.55 +/- 0.93
Episode length: 17.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 7.55        |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.031872205 |
|    clip_fraction        | 0.066       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0849     |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0973      |
|    n_updates            | 17610       |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 0.315       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 8.08     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 450      |
|    time_elapsed    | 3953     |
|    total_timesteps | 921600   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 7.91        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 451         |
|    time_elapsed         | 3962        |
|    total_timesteps      | 923648      |
| train/                  |             |
|    approx_kl            | 0.043565147 |
|    clip_fraction        | 0.0624      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0768     |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 17620       |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 0.294       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.8      |
|    ep_rew_mean          | 8.03      |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 452       |
|    time_elapsed         | 3972      |
|    total_timesteps      | 925696    |
| train/                  |           |
|    approx_kl            | 0.0505947 |
|    clip_fraction        | 0.0686    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0766   |
|    explained_variance   | 0.688     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0879    |
|    n_updates            | 17630     |
|    policy_gradient_loss | -0.0213   |
|    value_loss           | 0.316     |
---------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 453         |
|    time_elapsed         | 3980        |
|    total_timesteps      | 927744      |
| train/                  |             |
|    approx_kl            | 0.023785356 |
|    clip_fraction        | 0.0663      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0878     |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.111       |
|    n_updates            | 17640       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 0.317       |
-----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.5      |
|    ep_rew_mean          | 7.93      |
| time/                   |           |
|    fps                  | 233       |
|    iterations           | 454       |
|    time_elapsed         | 3989      |
|    total_timesteps      | 929792    |
| train/                  |           |
|    approx_kl            | 0.1184784 |
|    clip_fraction        | 0.102     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.122    |
|    explained_variance   | 0.457     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.067     |
|    n_updates            | 17650     |
|    policy_gradient_loss | -0.0373   |
|    value_loss           | 0.369     |
---------------------------------------
Eval num_timesteps=930000, episode_reward=8.52 +/- 1.43
Episode length: 19.00 +/- 2.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 8.52        |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.026065696 |
|    clip_fraction        | 0.0641      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0866     |
|    explained_variance   | 0.731       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0557      |
|    n_updates            | 17660       |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 0.391       |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 455      |
|    time_elapsed    | 3997     |
|    total_timesteps | 931840   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 8.13       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 456        |
|    time_elapsed         | 4005       |
|    total_timesteps      | 933888     |
| train/                  |            |
|    approx_kl            | 0.05425785 |
|    clip_fraction        | 0.0801     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0987    |
|    explained_variance   | 0.696      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.19       |
|    n_updates            | 17670      |
|    policy_gradient_loss | -0.0178    |
|    value_loss           | 0.463      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 8.12        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 457         |
|    time_elapsed         | 4012        |
|    total_timesteps      | 935936      |
| train/                  |             |
|    approx_kl            | 0.032895595 |
|    clip_fraction        | 0.0618      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.066      |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.108       |
|    n_updates            | 17680       |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.324       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 8.19        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 458         |
|    time_elapsed         | 4020        |
|    total_timesteps      | 937984      |
| train/                  |             |
|    approx_kl            | 0.045304023 |
|    clip_fraction        | 0.0652      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0698     |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0758      |
|    n_updates            | 17690       |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 0.311       |
-----------------------------------------
Eval num_timesteps=940000, episode_reward=6.44 +/- 2.18
Episode length: 16.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.3        |
|    mean_reward          | 6.44        |
| time/                   |             |
|    total_timesteps      | 940000      |
| train/                  |             |
|    approx_kl            | 0.023792699 |
|    clip_fraction        | 0.0527      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0626     |
|    explained_variance   | 0.786       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0794      |
|    n_updates            | 17700       |
|    policy_gradient_loss | -0.0115     |
|    value_loss           | 0.318       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 7.98     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 459      |
|    time_elapsed    | 4028     |
|    total_timesteps | 940032   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 8.1         |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 460         |
|    time_elapsed         | 4035        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.052294526 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.09       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0288      |
|    n_updates            | 17710       |
|    policy_gradient_loss | -0.0182     |
|    value_loss           | 0.235       |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 8.13       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 461        |
|    time_elapsed         | 4043       |
|    total_timesteps      | 944128     |
| train/                  |            |
|    approx_kl            | 0.03358297 |
|    clip_fraction        | 0.0637     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0782    |
|    explained_variance   | 0.789      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0602     |
|    n_updates            | 17720      |
|    policy_gradient_loss | -0.0137    |
|    value_loss           | 0.255      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 7.87        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 462         |
|    time_elapsed         | 4053        |
|    total_timesteps      | 946176      |
| train/                  |             |
|    approx_kl            | 0.044121005 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0915     |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0571      |
|    n_updates            | 17730       |
|    policy_gradient_loss | -0.0231     |
|    value_loss           | 0.336       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.5       |
|    ep_rew_mean          | 8.07       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 463        |
|    time_elapsed         | 4060       |
|    total_timesteps      | 948224     |
| train/                  |            |
|    approx_kl            | 0.03662011 |
|    clip_fraction        | 0.085      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0915    |
|    explained_variance   | 0.702      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0274     |
|    n_updates            | 17740      |
|    policy_gradient_loss | -0.0224    |
|    value_loss           | 0.3        |
----------------------------------------
Eval num_timesteps=950000, episode_reward=9.34 +/- 1.20
Episode length: 18.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 9.34        |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.018364666 |
|    clip_fraction        | 0.0532      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0715     |
|    explained_variance   | 0.801       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0667      |
|    n_updates            | 17750       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 0.224       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 8.19     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 464      |
|    time_elapsed    | 4067     |
|    total_timesteps | 950272   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.83        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 465         |
|    time_elapsed         | 4075        |
|    total_timesteps      | 952320      |
| train/                  |             |
|    approx_kl            | 0.033115227 |
|    clip_fraction        | 0.0674      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0799     |
|    explained_variance   | 0.817       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0919      |
|    n_updates            | 17760       |
|    policy_gradient_loss | -0.0156     |
|    value_loss           | 0.245       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 8.23        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 466         |
|    time_elapsed         | 4082        |
|    total_timesteps      | 954368      |
| train/                  |             |
|    approx_kl            | 0.023592168 |
|    clip_fraction        | 0.0739      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0886     |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.112       |
|    n_updates            | 17770       |
|    policy_gradient_loss | -0.0202     |
|    value_loss           | 0.298       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 8.22        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 467         |
|    time_elapsed         | 4090        |
|    total_timesteps      | 956416      |
| train/                  |             |
|    approx_kl            | 0.022511387 |
|    clip_fraction        | 0.06        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0784     |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0851      |
|    n_updates            | 17780       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 0.307       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 8.24        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 468         |
|    time_elapsed         | 4097        |
|    total_timesteps      | 958464      |
| train/                  |             |
|    approx_kl            | 0.045462973 |
|    clip_fraction        | 0.076       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0882     |
|    explained_variance   | 0.813       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0398      |
|    n_updates            | 17790       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.272       |
-----------------------------------------
Eval num_timesteps=960000, episode_reward=8.44 +/- 0.21
Episode length: 16.33 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 8.44       |
| time/                   |            |
|    total_timesteps      | 960000     |
| train/                  |            |
|    approx_kl            | 0.06345624 |
|    clip_fraction        | 0.0801     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0834    |
|    explained_variance   | 0.752      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.117      |
|    n_updates            | 17800      |
|    policy_gradient_loss | -0.0192    |
|    value_loss           | 0.245      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.27     |
| time/              |          |
|    fps             | 233      |
|    iterations      | 469      |
|    time_elapsed    | 4105     |
|    total_timesteps | 960512   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 470        |
|    time_elapsed         | 4112       |
|    total_timesteps      | 962560     |
| train/                  |            |
|    approx_kl            | 0.04527234 |
|    clip_fraction        | 0.0708     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0965    |
|    explained_variance   | 0.836      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0787     |
|    n_updates            | 17810      |
|    policy_gradient_loss | -0.0139    |
|    value_loss           | 0.21       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.6       |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 471        |
|    time_elapsed         | 4119       |
|    total_timesteps      | 964608     |
| train/                  |            |
|    approx_kl            | 0.13761699 |
|    clip_fraction        | 0.12       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0689     |
|    n_updates            | 17820      |
|    policy_gradient_loss | -0.0269    |
|    value_loss           | 0.353      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 472         |
|    time_elapsed         | 4126        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.041183107 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.156       |
|    n_updates            | 17830       |
|    policy_gradient_loss | -0.0229     |
|    value_loss           | 0.64        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 7.98        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 473         |
|    time_elapsed         | 4134        |
|    total_timesteps      | 968704      |
| train/                  |             |
|    approx_kl            | 0.034992006 |
|    clip_fraction        | 0.0986      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.573       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    n_updates            | 17840       |
|    policy_gradient_loss | -0.0222     |
|    value_loss           | 0.478       |
-----------------------------------------
Eval num_timesteps=970000, episode_reward=7.55 +/- 0.76
Episode length: 17.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 7.55        |
| time/                   |             |
|    total_timesteps      | 970000      |
| train/                  |             |
|    approx_kl            | 0.031946547 |
|    clip_fraction        | 0.0811      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0899     |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.089       |
|    n_updates            | 17850       |
|    policy_gradient_loss | -0.0134     |
|    value_loss           | 0.356       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 8.17     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 474      |
|    time_elapsed    | 4142     |
|    total_timesteps | 970752   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.6      |
|    ep_rew_mean          | 8.16      |
| time/                   |           |
|    fps                  | 234       |
|    iterations           | 475       |
|    time_elapsed         | 4149      |
|    total_timesteps      | 972800    |
| train/                  |           |
|    approx_kl            | 0.0360675 |
|    clip_fraction        | 0.0674    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.0775   |
|    explained_variance   | 0.796     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.0791    |
|    n_updates            | 17860     |
|    policy_gradient_loss | -0.013    |
|    value_loss           | 0.246     |
---------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.4       |
|    ep_rew_mean          | 7.98       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 476        |
|    time_elapsed         | 4157       |
|    total_timesteps      | 974848     |
| train/                  |            |
|    approx_kl            | 0.04282473 |
|    clip_fraction        | 0.059      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0662    |
|    explained_variance   | 0.829      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0275     |
|    n_updates            | 17870      |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 0.187      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 477        |
|    time_elapsed         | 4165       |
|    total_timesteps      | 976896     |
| train/                  |            |
|    approx_kl            | 0.06865412 |
|    clip_fraction        | 0.126      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.114     |
|    explained_variance   | 0.524      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.195      |
|    n_updates            | 17880      |
|    policy_gradient_loss | -0.0314    |
|    value_loss           | 0.499      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 7.95        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 478         |
|    time_elapsed         | 4174        |
|    total_timesteps      | 978944      |
| train/                  |             |
|    approx_kl            | 0.034771793 |
|    clip_fraction        | 0.0831      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.103      |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0938      |
|    n_updates            | 17890       |
|    policy_gradient_loss | -0.0176     |
|    value_loss           | 0.322       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=980000, episode_reward=1.60 +/- 8.24
Episode length: 27.67 +/- 15.80
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.7        |
|    mean_reward          | 1.6         |
| time/                   |             |
|    total_timesteps      | 980000      |
| train/                  |             |
|    approx_kl            | 0.034944303 |
|    clip_fraction        | 0.0686      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0771     |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.025       |
|    n_updates            | 17900       |
|    policy_gradient_loss | -0.0143     |
|    value_loss           | 0.302       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 7.87     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 479      |
|    time_elapsed    | 4183     |
|    total_timesteps | 980992   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.2        |
|    ep_rew_mean          | 7.75        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 480         |
|    time_elapsed         | 4191        |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.042860016 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.149       |
|    n_updates            | 17910       |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 0.598       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 7.6         |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 481         |
|    time_elapsed         | 4199        |
|    total_timesteps      | 985088      |
| train/                  |             |
|    approx_kl            | 0.056794353 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0968     |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 17920       |
|    policy_gradient_loss | -0.00882    |
|    value_loss           | 0.603       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.8         |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 482         |
|    time_elapsed         | 4207        |
|    total_timesteps      | 987136      |
| train/                  |             |
|    approx_kl            | 0.048764147 |
|    clip_fraction        | 0.0802      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.105      |
|    explained_variance   | 0.688       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.101       |
|    n_updates            | 17930       |
|    policy_gradient_loss | -0.0103     |
|    value_loss           | 0.534       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 8.06       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 483        |
|    time_elapsed         | 4215       |
|    total_timesteps      | 989184     |
| train/                  |            |
|    approx_kl            | 0.03875024 |
|    clip_fraction        | 0.086      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.101     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.157      |
|    n_updates            | 17940      |
|    policy_gradient_loss | -0.0202    |
|    value_loss           | 0.457      |
----------------------------------------
reached max steps=100
Eval num_timesteps=990000, episode_reward=1.76 +/- 8.33
Episode length: 30.00 +/- 14.35
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30          |
|    mean_reward          | 1.76        |
| time/                   |             |
|    total_timesteps      | 990000      |
| train/                  |             |
|    approx_kl            | 0.031590723 |
|    clip_fraction        | 0.074       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0815     |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0691      |
|    n_updates            | 17950       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 0.26        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.27     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 484      |
|    time_elapsed    | 4223     |
|    total_timesteps | 991232   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 7.99        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 485         |
|    time_elapsed         | 4231        |
|    total_timesteps      | 993280      |
| train/                  |             |
|    approx_kl            | 0.026080294 |
|    clip_fraction        | 0.0543      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0665     |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0261      |
|    n_updates            | 17960       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 0.221       |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.8        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 486         |
|    time_elapsed         | 4239        |
|    total_timesteps      | 995328      |
| train/                  |             |
|    approx_kl            | 0.051351808 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0995     |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0382      |
|    n_updates            | 17970       |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.429       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 487         |
|    time_elapsed         | 4246        |
|    total_timesteps      | 997376      |
| train/                  |             |
|    approx_kl            | 0.049615264 |
|    clip_fraction        | 0.106       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.665       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.369       |
|    n_updates            | 17980       |
|    policy_gradient_loss | -0.0237     |
|    value_loss           | 0.633       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.5       |
|    ep_rew_mean          | 8.37       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 488        |
|    time_elapsed         | 4254       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.06379197 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0997    |
|    explained_variance   | 0.542      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.155      |
|    n_updates            | 17990      |
|    policy_gradient_loss | 0.0262     |
|    value_loss           | 0.575      |
----------------------------------------
Eval num_timesteps=1000000, episode_reward=7.42 +/- 0.84
Episode length: 21.00 +/- 5.66
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 21          |
|    mean_reward          | 7.42        |
| time/                   |             |
|    total_timesteps      | 1000000     |
| train/                  |             |
|    approx_kl            | 0.028586933 |
|    clip_fraction        | 0.0637      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0672     |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0362      |
|    n_updates            | 18000       |
|    policy_gradient_loss | -0.014      |
|    value_loss           | 0.217       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 8.16     |
| time/              |          |
|    fps             | 234      |
|    iterations      | 489      |
|    time_elapsed    | 4262     |
|    total_timesteps | 1001472  |
---------------------------------
