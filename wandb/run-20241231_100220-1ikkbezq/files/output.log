Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Loaded model from models\orig_easy8_20241111\iter_1000000_steps. Continuing training.
cuda:0
Logging to ./logs/ppo/minigrid_custom_tensorboard/20241231_2
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x00000247128FC6A0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x000002470C381480>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 28.1     |
|    ep_rew_mean     | 3.94     |
| time/              |          |
|    fps             | 272      |
|    iterations      | 1        |
|    time_elapsed    | 7        |
|    total_timesteps | 2048     |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.8       |
|    ep_rew_mean          | 3.2        |
| time/                   |            |
|    fps                  | 283        |
|    iterations           | 2          |
|    time_elapsed         | 14         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.22254875 |
|    clip_fraction        | 0.272      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.328     |
|    explained_variance   | -0.041     |
|    learning_rate        | 0.0003     |
|    loss                 | 2.51       |
|    n_updates            | 9770       |
|    policy_gradient_loss | -0.00201   |
|    value_loss           | 5.4        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.5       |
|    ep_rew_mean          | 3.04       |
| time/                   |            |
|    fps                  | 282        |
|    iterations           | 3          |
|    time_elapsed         | 21         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.08248544 |
|    clip_fraction        | 0.214      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.336     |
|    explained_variance   | 0.211      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.76       |
|    n_updates            | 9780       |
|    policy_gradient_loss | 0.000809   |
|    value_loss           | 4.54       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.6       |
|    ep_rew_mean          | 3.15       |
| time/                   |            |
|    fps                  | 278        |
|    iterations           | 4          |
|    time_elapsed         | 29         |
|    total_timesteps      | 8192       |
| train/                  |            |
|    approx_kl            | 0.07847955 |
|    clip_fraction        | 0.224      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.377     |
|    explained_variance   | 0.295      |
|    learning_rate        | 0.0003     |
|    loss                 | 2.84       |
|    n_updates            | 9790       |
|    policy_gradient_loss | 0.000239   |
|    value_loss           | 5.39       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=10000, episode_reward=-7.19 +/- 3.97
Episode length: 40.33 +/- 13.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 40.3      |
|    mean_reward          | -7.19     |
| time/                   |           |
|    total_timesteps      | 10000     |
| train/                  |           |
|    approx_kl            | 0.0635704 |
|    clip_fraction        | 0.202     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.409    |
|    explained_variance   | 0.207     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.46      |
|    n_updates            | 9800      |
|    policy_gradient_loss | 0.00333   |
|    value_loss           | 4.86      |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.1     |
|    ep_rew_mean     | 3.59     |
| time/              |          |
|    fps             | 268      |
|    iterations      | 5        |
|    time_elapsed    | 38       |
|    total_timesteps | 10240    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.8        |
|    ep_rew_mean          | 3.17        |
| time/                   |             |
|    fps                  | 265         |
|    iterations           | 6           |
|    time_elapsed         | 46          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.091941744 |
|    clip_fraction        | 0.235       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.411      |
|    explained_variance   | 0.241       |
|    learning_rate        | 0.0003      |
|    loss                 | 2.46        |
|    n_updates            | 9810        |
|    policy_gradient_loss | 0.0059      |
|    value_loss           | 5.53        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 29.3        |
|    ep_rew_mean          | 2.21        |
| time/                   |             |
|    fps                  | 261         |
|    iterations           | 7           |
|    time_elapsed         | 54          |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.079334795 |
|    clip_fraction        | 0.213       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.416      |
|    explained_variance   | 0.183       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 9820        |
|    policy_gradient_loss | 0.0108      |
|    value_loss           | 5.27        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 29.7       |
|    ep_rew_mean          | 2.08       |
| time/                   |            |
|    fps                  | 258        |
|    iterations           | 8          |
|    time_elapsed         | 63         |
|    total_timesteps      | 16384      |
| train/                  |            |
|    approx_kl            | 0.06262023 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.444     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.53       |
|    n_updates            | 9830       |
|    policy_gradient_loss | -0.0011    |
|    value_loss           | 4.59       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 28.6       |
|    ep_rew_mean          | 2.38       |
| time/                   |            |
|    fps                  | 256        |
|    iterations           | 9          |
|    time_elapsed         | 71         |
|    total_timesteps      | 18432      |
| train/                  |            |
|    approx_kl            | 0.05852037 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.45      |
|    explained_variance   | 0.276      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.37       |
|    n_updates            | 9840       |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 3.8        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=20000, episode_reward=3.52 +/- 2.49
Episode length: 19.00 +/- 5.35
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 19       |
|    mean_reward          | 3.52     |
| time/                   |          |
|    total_timesteps      | 20000    |
| train/                  |          |
|    approx_kl            | 0.056181 |
|    clip_fraction        | 0.209    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.403   |
|    explained_variance   | 0.199    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.86     |
|    n_updates            | 9850     |
|    policy_gradient_loss | -0.00938 |
|    value_loss           | 4.02     |
--------------------------------------
New best mean reward!
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.4     |
|    ep_rew_mean     | 2.44     |
| time/              |          |
|    fps             | 255      |
|    iterations      | 10       |
|    time_elapsed    | 80       |
|    total_timesteps | 20480    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 27.9       |
|    ep_rew_mean          | 2.35       |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 11         |
|    time_elapsed         | 89         |
|    total_timesteps      | 22528      |
| train/                  |            |
|    approx_kl            | 0.04953365 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.461     |
|    explained_variance   | 0.3        |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18       |
|    n_updates            | 9860       |
|    policy_gradient_loss | -0.00664   |
|    value_loss           | 4.71       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 25.5       |
|    ep_rew_mean          | 3.31       |
| time/                   |            |
|    fps                  | 254        |
|    iterations           | 12         |
|    time_elapsed         | 96         |
|    total_timesteps      | 24576      |
| train/                  |            |
|    approx_kl            | 0.08230168 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.438     |
|    explained_variance   | 0.151      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.38       |
|    n_updates            | 9870       |
|    policy_gradient_loss | -0.00693   |
|    value_loss           | 3.93       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.7       |
|    ep_rew_mean          | 3.45       |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 13         |
|    time_elapsed         | 104        |
|    total_timesteps      | 26624      |
| train/                  |            |
|    approx_kl            | 0.05777197 |
|    clip_fraction        | 0.171      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.435     |
|    explained_variance   | 0.255      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 9880       |
|    policy_gradient_loss | -0.00339   |
|    value_loss           | 3.69       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 26.4        |
|    ep_rew_mean          | 3.36        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 14          |
|    time_elapsed         | 113         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.046130665 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.434      |
|    explained_variance   | 0.205       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.98        |
|    n_updates            | 9890        |
|    policy_gradient_loss | -0.00387    |
|    value_loss           | 4.42        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=30000, episode_reward=1.02 +/- 7.82
Episode length: 30.33 +/- 14.20
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 30.3       |
|    mean_reward          | 1.02       |
| time/                   |            |
|    total_timesteps      | 30000      |
| train/                  |            |
|    approx_kl            | 0.08325625 |
|    clip_fraction        | 0.217      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.445     |
|    explained_variance   | 0.0779     |
|    learning_rate        | 0.0003     |
|    loss                 | 1.87       |
|    n_updates            | 9900       |
|    policy_gradient_loss | -0.00553   |
|    value_loss           | 4.41       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.7     |
|    ep_rew_mean     | 2.58     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 15       |
|    time_elapsed    | 122      |
|    total_timesteps | 30720    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.6        |
|    ep_rew_mean          | 2.84        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 16          |
|    time_elapsed         | 130         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.057049993 |
|    clip_fraction        | 0.219       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.443      |
|    explained_variance   | 0.196       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 9910        |
|    policy_gradient_loss | -0.00287    |
|    value_loss           | 3.82        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.7       |
|    ep_rew_mean          | 2.86       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 17         |
|    time_elapsed         | 139        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.06974086 |
|    clip_fraction        | 0.231      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.439     |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.29       |
|    n_updates            | 9920       |
|    policy_gradient_loss | -0.00143   |
|    value_loss           | 3.89       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.1        |
|    ep_rew_mean          | 3.35        |
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 18          |
|    time_elapsed         | 147         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.056732986 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.404      |
|    explained_variance   | 0.133       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.59        |
|    n_updates            | 9930        |
|    policy_gradient_loss | 0.0118      |
|    value_loss           | 3.99        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 24.4      |
|    ep_rew_mean          | 2.88      |
| time/                   |           |
|    fps                  | 248       |
|    iterations           | 19        |
|    time_elapsed         | 156       |
|    total_timesteps      | 38912     |
| train/                  |           |
|    approx_kl            | 0.0645799 |
|    clip_fraction        | 0.195     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.359    |
|    explained_variance   | 0.0687    |
|    learning_rate        | 0.0003    |
|    loss                 | 0.85      |
|    n_updates            | 9940      |
|    policy_gradient_loss | 0.00183   |
|    value_loss           | 4.17      |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=40000, episode_reward=-6.19 +/- 5.38
Episode length: 40.33 +/- 13.67
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 40.3        |
|    mean_reward          | -6.19       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.050143167 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.362      |
|    explained_variance   | 0.228       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 9950        |
|    policy_gradient_loss | -4.34e-05   |
|    value_loss           | 3.73        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 25.4     |
|    ep_rew_mean     | 2.41     |
| time/              |          |
|    fps             | 248      |
|    iterations      | 20       |
|    time_elapsed    | 164      |
|    total_timesteps | 40960    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24          |
|    ep_rew_mean          | 3           |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 21          |
|    time_elapsed         | 173         |
|    total_timesteps      | 43008       |
| train/                  |             |
|    approx_kl            | 0.067052856 |
|    clip_fraction        | 0.211       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.355      |
|    explained_variance   | -0.0569     |
|    learning_rate        | 0.0003      |
|    loss                 | 1.2         |
|    n_updates            | 9960        |
|    policy_gradient_loss | 0.00591     |
|    value_loss           | 3.31        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.5        |
|    ep_rew_mean          | 3.53        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 22          |
|    time_elapsed         | 182         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.074712455 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.328      |
|    explained_variance   | 0.0604      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.7         |
|    n_updates            | 9970        |
|    policy_gradient_loss | 9.7e-05     |
|    value_loss           | 3.67        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.5       |
|    ep_rew_mean          | 3.93       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 23         |
|    time_elapsed         | 191        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.05212304 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.331     |
|    explained_variance   | 0.2        |
|    learning_rate        | 0.0003     |
|    loss                 | 0.884      |
|    n_updates            | 9980       |
|    policy_gradient_loss | -0.00373   |
|    value_loss           | 3.62       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 24.5        |
|    ep_rew_mean          | 2.99        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 24          |
|    time_elapsed         | 199         |
|    total_timesteps      | 49152       |
| train/                  |             |
|    approx_kl            | 0.047608197 |
|    clip_fraction        | 0.167       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.345      |
|    explained_variance   | 0.354       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.33        |
|    n_updates            | 9990        |
|    policy_gradient_loss | -0.00151    |
|    value_loss           | 4.12        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=50000, episode_reward=3.27 +/- 3.53
Episode length: 27.67 +/- 16.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.7        |
|    mean_reward          | 3.27        |
| time/                   |             |
|    total_timesteps      | 50000       |
| train/                  |             |
|    approx_kl            | 0.057787888 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.32       |
|    explained_variance   | 0.0389      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 10000       |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 3.38        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.9     |
|    ep_rew_mean     | 3.45     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 25       |
|    time_elapsed    | 207      |
|    total_timesteps | 51200    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26         |
|    ep_rew_mean          | 3.85       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 26         |
|    time_elapsed         | 216        |
|    total_timesteps      | 53248      |
| train/                  |            |
|    approx_kl            | 0.07008798 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.301     |
|    explained_variance   | 0.16       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.23       |
|    n_updates            | 10010      |
|    policy_gradient_loss | -0.00287   |
|    value_loss           | 4.11       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.1        |
|    ep_rew_mean          | 3.73        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 27          |
|    time_elapsed         | 224         |
|    total_timesteps      | 55296       |
| train/                  |             |
|    approx_kl            | 0.056547984 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.319      |
|    explained_variance   | 0.0922      |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 10020       |
|    policy_gradient_loss | 0.00319     |
|    value_loss           | 3.48        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.5        |
|    ep_rew_mean          | 3.33        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 28          |
|    time_elapsed         | 233         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.060253896 |
|    clip_fraction        | 0.164       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.114       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.908       |
|    n_updates            | 10030       |
|    policy_gradient_loss | -0.00528    |
|    value_loss           | 2.9         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 27.1       |
|    ep_rew_mean          | 2.54       |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 29         |
|    time_elapsed         | 241        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.10623659 |
|    clip_fraction        | 0.22       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.364     |
|    explained_variance   | 0.183      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.34       |
|    n_updates            | 10040      |
|    policy_gradient_loss | -0.00814   |
|    value_loss           | 4.48       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=60000, episode_reward=-7.14 +/- 4.05
Episode length: 44.67 +/- 7.54
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 44.7        |
|    mean_reward          | -7.14       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.062282175 |
|    clip_fraction        | 0.186       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.321      |
|    explained_variance   | 0.104       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.57        |
|    n_updates            | 10050       |
|    policy_gradient_loss | -0.000198   |
|    value_loss           | 3.76        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 24.6     |
|    ep_rew_mean     | 4.09     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 30       |
|    time_elapsed    | 250      |
|    total_timesteps | 61440    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 23.1      |
|    ep_rew_mean          | 3.69      |
| time/                   |           |
|    fps                  | 244       |
|    iterations           | 31        |
|    time_elapsed         | 259       |
|    total_timesteps      | 63488     |
| train/                  |           |
|    approx_kl            | 0.0627424 |
|    clip_fraction        | 0.166     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.301    |
|    explained_variance   | 0.133     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.46      |
|    n_updates            | 10060     |
|    policy_gradient_loss | 0.00221   |
|    value_loss           | 4.44      |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.5        |
|    ep_rew_mean          | 4.39        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 32          |
|    time_elapsed         | 268         |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.055805765 |
|    clip_fraction        | 0.182       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.346      |
|    explained_variance   | 0.102       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 10070       |
|    policy_gradient_loss | 0.00242     |
|    value_loss           | 3.4         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.9        |
|    ep_rew_mean          | 3.57        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 33          |
|    time_elapsed         | 277         |
|    total_timesteps      | 67584       |
| train/                  |             |
|    approx_kl            | 0.041851334 |
|    clip_fraction        | 0.171       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.363      |
|    explained_variance   | 0.313       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.28        |
|    n_updates            | 10080       |
|    policy_gradient_loss | -0.00354    |
|    value_loss           | 3.28        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.3       |
|    ep_rew_mean          | 3.41       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 34         |
|    time_elapsed         | 285        |
|    total_timesteps      | 69632      |
| train/                  |            |
|    approx_kl            | 0.08246946 |
|    clip_fraction        | 0.198      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.4       |
|    explained_variance   | 0.162      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.84       |
|    n_updates            | 10090      |
|    policy_gradient_loss | -0.00279   |
|    value_loss           | 3.79       |
----------------------------------------
Eval num_timesteps=70000, episode_reward=5.70 +/- 4.82
Episode length: 16.67 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | 5.7        |
| time/                   |            |
|    total_timesteps      | 70000      |
| train/                  |            |
|    approx_kl            | 0.04159542 |
|    clip_fraction        | 0.203      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.361     |
|    explained_variance   | 0.114      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 10100      |
|    policy_gradient_loss | -0.00522   |
|    value_loss           | 2.92       |
----------------------------------------
New best mean reward!
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 3.99     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 35       |
|    time_elapsed    | 294      |
|    total_timesteps | 71680    |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.3        |
|    ep_rew_mean          | 3.85        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 36          |
|    time_elapsed         | 302         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.042440556 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.364      |
|    explained_variance   | 0.175       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.19        |
|    n_updates            | 10110       |
|    policy_gradient_loss | -0.00465    |
|    value_loss           | 3.51        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 3.76       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 37         |
|    time_elapsed         | 310        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.07237077 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.16       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.868      |
|    n_updates            | 10120      |
|    policy_gradient_loss | 0.0264     |
|    value_loss           | 2.81       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.3        |
|    ep_rew_mean          | 3.46        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 38          |
|    time_elapsed         | 318         |
|    total_timesteps      | 77824       |
| train/                  |             |
|    approx_kl            | 0.048233934 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.406      |
|    explained_variance   | 0.185       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.989       |
|    n_updates            | 10130       |
|    policy_gradient_loss | -0.0123     |
|    value_loss           | 2.94        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.5       |
|    ep_rew_mean          | 3.27       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 39         |
|    time_elapsed         | 327        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.05370976 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.378     |
|    explained_variance   | 0.25       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 10140      |
|    policy_gradient_loss | -0.0122    |
|    value_loss           | 2.5        |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=80000, episode_reward=-2.50 +/- 7.37
Episode length: 38.67 +/- 16.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.7        |
|    mean_reward          | -2.5        |
| time/                   |             |
|    total_timesteps      | 80000       |
| train/                  |             |
|    approx_kl            | 0.029666161 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.393      |
|    explained_variance   | 0.272       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 10150       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 2.37        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.6     |
|    ep_rew_mean     | 3.69     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 40       |
|    time_elapsed    | 336      |
|    total_timesteps | 81920    |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 3.91       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 41         |
|    time_elapsed         | 344        |
|    total_timesteps      | 83968      |
| train/                  |            |
|    approx_kl            | 0.05243565 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.422     |
|    explained_variance   | 0.296      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.01       |
|    n_updates            | 10160      |
|    policy_gradient_loss | -0.00463   |
|    value_loss           | 3.38       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 3.44        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 42          |
|    time_elapsed         | 352         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.038249265 |
|    clip_fraction        | 0.163       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.428      |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.712       |
|    n_updates            | 10170       |
|    policy_gradient_loss | -0.00778    |
|    value_loss           | 2.7         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 3.17       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 43         |
|    time_elapsed         | 361        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.05237773 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.407     |
|    explained_variance   | 0.202      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.454      |
|    n_updates            | 10180      |
|    policy_gradient_loss | -6.49e-05  |
|    value_loss           | 2.48       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=90000, episode_reward=6.50 +/- 2.20
Episode length: 13.00 +/- 0.82
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13         |
|    mean_reward          | 6.5        |
| time/                   |            |
|    total_timesteps      | 90000      |
| train/                  |            |
|    approx_kl            | 0.11265123 |
|    clip_fraction        | 0.174      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.381     |
|    explained_variance   | 0.219      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.22       |
|    n_updates            | 10190      |
|    policy_gradient_loss | -0.00134   |
|    value_loss           | 3.11       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 4.18     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 44       |
|    time_elapsed    | 369      |
|    total_timesteps | 90112    |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 3.89        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 45          |
|    time_elapsed         | 377         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.044356707 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.388      |
|    explained_variance   | 0.303       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.02        |
|    n_updates            | 10200       |
|    policy_gradient_loss | -0.00711    |
|    value_loss           | 3.3         |
-----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.6      |
|    ep_rew_mean          | 3.91      |
| time/                   |           |
|    fps                  | 243       |
|    iterations           | 46        |
|    time_elapsed         | 386       |
|    total_timesteps      | 94208     |
| train/                  |           |
|    approx_kl            | 0.0394475 |
|    clip_fraction        | 0.157     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.358    |
|    explained_variance   | 0.219     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.21      |
|    n_updates            | 10210     |
|    policy_gradient_loss | -0.00654  |
|    value_loss           | 2.73      |
---------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 4.06        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 47          |
|    time_elapsed         | 394         |
|    total_timesteps      | 96256       |
| train/                  |             |
|    approx_kl            | 0.036840647 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.335      |
|    explained_variance   | 0.278       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.555       |
|    n_updates            | 10220       |
|    policy_gradient_loss | -0.0064     |
|    value_loss           | 2.02        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 3.49        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 48          |
|    time_elapsed         | 402         |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.047314644 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.348      |
|    explained_variance   | 0.287       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.846       |
|    n_updates            | 10230       |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 3.37        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=100000, episode_reward=0.37 +/- 7.55
Episode length: 25.67 +/- 17.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.7       |
|    mean_reward          | 0.371      |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.04632585 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.359     |
|    explained_variance   | 0.28       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.18       |
|    n_updates            | 10240      |
|    policy_gradient_loss | -0.00499   |
|    value_loss           | 3.33       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 4.15     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 49       |
|    time_elapsed    | 411      |
|    total_timesteps | 100352   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 4.35       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 50         |
|    time_elapsed         | 419        |
|    total_timesteps      | 102400     |
| train/                  |            |
|    approx_kl            | 0.07253549 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.281      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.02       |
|    n_updates            | 10250      |
|    policy_gradient_loss | -0.0132    |
|    value_loss           | 3.05       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | 4.5         |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 51          |
|    time_elapsed         | 429         |
|    total_timesteps      | 104448      |
| train/                  |             |
|    approx_kl            | 0.041511744 |
|    clip_fraction        | 0.174       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.361      |
|    explained_variance   | 0.376       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.697       |
|    n_updates            | 10260       |
|    policy_gradient_loss | -0.0139     |
|    value_loss           | 2.93        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 4.33        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 52          |
|    time_elapsed         | 438         |
|    total_timesteps      | 106496      |
| train/                  |             |
|    approx_kl            | 0.048148863 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.328      |
|    explained_variance   | 0.33        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.805       |
|    n_updates            | 10270       |
|    policy_gradient_loss | 0.00944     |
|    value_loss           | 2.97        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 4.37        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 53          |
|    time_elapsed         | 446         |
|    total_timesteps      | 108544      |
| train/                  |             |
|    approx_kl            | 0.045644797 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.321      |
|    explained_variance   | 0.11        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 10280       |
|    policy_gradient_loss | -0.00379    |
|    value_loss           | 2.61        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=110000, episode_reward=-0.41 +/- 9.09
Episode length: 24.67 +/- 17.93
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 24.7        |
|    mean_reward          | -0.411      |
| time/                   |             |
|    total_timesteps      | 110000      |
| train/                  |             |
|    approx_kl            | 0.048149712 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.307      |
|    explained_variance   | 0.227       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.97        |
|    n_updates            | 10290       |
|    policy_gradient_loss | -0.0117     |
|    value_loss           | 2.87        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.3     |
|    ep_rew_mean     | 4.28     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 54       |
|    time_elapsed    | 455      |
|    total_timesteps | 110592   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 3.89       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 55         |
|    time_elapsed         | 464        |
|    total_timesteps      | 112640     |
| train/                  |            |
|    approx_kl            | 0.05367441 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.276     |
|    explained_variance   | 0.169      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.04       |
|    n_updates            | 10300      |
|    policy_gradient_loss | -0.0082    |
|    value_loss           | 3.22       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.2        |
|    ep_rew_mean          | 3.99        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 56          |
|    time_elapsed         | 472         |
|    total_timesteps      | 114688      |
| train/                  |             |
|    approx_kl            | 0.041418917 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | 0.252       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.17        |
|    n_updates            | 10310       |
|    policy_gradient_loss | -0.00131    |
|    value_loss           | 3.3         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 4.79       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 57         |
|    time_elapsed         | 481        |
|    total_timesteps      | 116736     |
| train/                  |            |
|    approx_kl            | 0.04894703 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.31      |
|    explained_variance   | 0.308      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.26       |
|    n_updates            | 10320      |
|    policy_gradient_loss | -0.00952   |
|    value_loss           | 3.22       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 4.27        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 58          |
|    time_elapsed         | 489         |
|    total_timesteps      | 118784      |
| train/                  |             |
|    approx_kl            | 0.045251723 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.324      |
|    explained_variance   | 0.176       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 10330       |
|    policy_gradient_loss | -0.00799    |
|    value_loss           | 3.21        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=120000, episode_reward=9.85 +/- 5.39
Episode length: 16.00 +/- 0.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 16        |
|    mean_reward          | 9.85      |
| time/                   |           |
|    total_timesteps      | 120000    |
| train/                  |           |
|    approx_kl            | 0.0678017 |
|    clip_fraction        | 0.151     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.298    |
|    explained_variance   | 0.188     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.12      |
|    n_updates            | 10340     |
|    policy_gradient_loss | -0.0062   |
|    value_loss           | 3.08      |
---------------------------------------
New best mean reward!
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.1     |
|    ep_rew_mean     | 4.29     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 59       |
|    time_elapsed    | 497      |
|    total_timesteps | 120832   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.8        |
|    ep_rew_mean          | 3.93        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 60          |
|    time_elapsed         | 506         |
|    total_timesteps      | 122880      |
| train/                  |             |
|    approx_kl            | 0.053175885 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.297      |
|    explained_variance   | 0.332       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 10350       |
|    policy_gradient_loss | -0.00101    |
|    value_loss           | 3.27        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 4.97        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 61          |
|    time_elapsed         | 515         |
|    total_timesteps      | 124928      |
| train/                  |             |
|    approx_kl            | 0.059987046 |
|    clip_fraction        | 0.169       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.304      |
|    explained_variance   | 0.231       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.43        |
|    n_updates            | 10360       |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 3.28        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 4.28       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 62         |
|    time_elapsed         | 523        |
|    total_timesteps      | 126976     |
| train/                  |            |
|    approx_kl            | 0.08995119 |
|    clip_fraction        | 0.187      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.28      |
|    explained_variance   | 0.103      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.79       |
|    n_updates            | 10370      |
|    policy_gradient_loss | -0.00336   |
|    value_loss           | 2.96       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 4.04        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 63          |
|    time_elapsed         | 532         |
|    total_timesteps      | 129024      |
| train/                  |             |
|    approx_kl            | 0.050044302 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.307      |
|    explained_variance   | 0.158       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.809       |
|    n_updates            | 10380       |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 2.81        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=130000, episode_reward=2.31 +/- 1.37
Episode length: 12.33 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 12.3        |
|    mean_reward          | 2.31        |
| time/                   |             |
|    total_timesteps      | 130000      |
| train/                  |             |
|    approx_kl            | 0.035785124 |
|    clip_fraction        | 0.156       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.31       |
|    explained_variance   | 0.362       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.3         |
|    n_updates            | 10390       |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 2.6         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.9     |
|    ep_rew_mean     | 3.03     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 64       |
|    time_elapsed    | 540      |
|    total_timesteps | 131072   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | 4.81        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 65          |
|    time_elapsed         | 548         |
|    total_timesteps      | 133120      |
| train/                  |             |
|    approx_kl            | 0.040903486 |
|    clip_fraction        | 0.159       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.316      |
|    explained_variance   | 0.277       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 10400       |
|    policy_gradient_loss | -0.00889    |
|    value_loss           | 2.87        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 4.69        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 66          |
|    time_elapsed         | 556         |
|    total_timesteps      | 135168      |
| train/                  |             |
|    approx_kl            | 0.050504506 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.281      |
|    explained_variance   | 0.224       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 10410       |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 2.92        |
-----------------------------------------
reached max steps=100
--------------------------------------
| rollout/                |          |
|    ep_len_mean          | 17.5     |
|    ep_rew_mean          | 4.84     |
| time/                   |          |
|    fps                  | 242      |
|    iterations           | 67       |
|    time_elapsed         | 565      |
|    total_timesteps      | 137216   |
| train/                  |          |
|    approx_kl            | 0.038771 |
|    clip_fraction        | 0.113    |
|    clip_range           | 0.2      |
|    entropy_loss         | -0.244   |
|    explained_variance   | 0.257    |
|    learning_rate        | 0.0003   |
|    loss                 | 1.68     |
|    n_updates            | 10420    |
|    policy_gradient_loss | -0.00273 |
|    value_loss           | 2.69     |
--------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 3.31       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 68         |
|    time_elapsed         | 573        |
|    total_timesteps      | 139264     |
| train/                  |            |
|    approx_kl            | 0.03422005 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.237     |
|    explained_variance   | 0.288      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.784      |
|    n_updates            | 10430      |
|    policy_gradient_loss | -0.00513   |
|    value_loss           | 2.64       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=140000, episode_reward=2.69 +/- 10.52
Episode length: 30.33 +/- 14.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 30.3        |
|    mean_reward          | 2.69        |
| time/                   |             |
|    total_timesteps      | 140000      |
| train/                  |             |
|    approx_kl            | 0.047744796 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 0.197       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.04        |
|    n_updates            | 10440       |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 3.06        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 4.07     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 69       |
|    time_elapsed    | 582      |
|    total_timesteps | 141312   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 4.62       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 70         |
|    time_elapsed         | 591        |
|    total_timesteps      | 143360     |
| train/                  |            |
|    approx_kl            | 0.04150289 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.27      |
|    explained_variance   | 0.33       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.715      |
|    n_updates            | 10450      |
|    policy_gradient_loss | -0.000259  |
|    value_loss           | 2.43       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.3        |
|    ep_rew_mean          | 4.73        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 71          |
|    time_elapsed         | 599         |
|    total_timesteps      | 145408      |
| train/                  |             |
|    approx_kl            | 0.047024935 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.283      |
|    explained_variance   | 0.186       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 10460       |
|    policy_gradient_loss | -0.00767    |
|    value_loss           | 2.99        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 4.48        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 72          |
|    time_elapsed         | 607         |
|    total_timesteps      | 147456      |
| train/                  |             |
|    approx_kl            | 0.054242946 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.243       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.713       |
|    n_updates            | 10470       |
|    policy_gradient_loss | -0.00225    |
|    value_loss           | 2.3         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 3.8        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 73         |
|    time_elapsed         | 616        |
|    total_timesteps      | 149504     |
| train/                  |            |
|    approx_kl            | 0.04094808 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.274     |
|    explained_variance   | 0.272      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 10480      |
|    policy_gradient_loss | -0.00509   |
|    value_loss           | 2.26       |
----------------------------------------
Eval num_timesteps=150000, episode_reward=4.54 +/- 1.81
Episode length: 14.33 +/- 2.87
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 4.54        |
| time/                   |             |
|    total_timesteps      | 150000      |
| train/                  |             |
|    approx_kl            | 0.039212413 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.278      |
|    explained_variance   | 0.383       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 10490       |
|    policy_gradient_loss | -0.00323    |
|    value_loss           | 2.14        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 5.12     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 74       |
|    time_elapsed    | 625      |
|    total_timesteps | 151552   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 4.91        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 75          |
|    time_elapsed         | 633         |
|    total_timesteps      | 153600      |
| train/                  |             |
|    approx_kl            | 0.048368707 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.242       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.82        |
|    n_updates            | 10500       |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 2.49        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 4.49        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 76          |
|    time_elapsed         | 642         |
|    total_timesteps      | 155648      |
| train/                  |             |
|    approx_kl            | 0.027147774 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.302      |
|    explained_variance   | 0.428       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.8         |
|    n_updates            | 10510       |
|    policy_gradient_loss | -0.00723    |
|    value_loss           | 2.74        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 4.25        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 77          |
|    time_elapsed         | 651         |
|    total_timesteps      | 157696      |
| train/                  |             |
|    approx_kl            | 0.045160238 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.298      |
|    explained_variance   | 0.424       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 10520       |
|    policy_gradient_loss | -0.00211    |
|    value_loss           | 2.86        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 4.32        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 78          |
|    time_elapsed         | 659         |
|    total_timesteps      | 159744      |
| train/                  |             |
|    approx_kl            | 0.039449006 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.275      |
|    explained_variance   | 0.214       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 10530       |
|    policy_gradient_loss | -0.00759    |
|    value_loss           | 2.62        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=160000, episode_reward=1.89 +/- 5.32
Episode length: 26.33 +/- 16.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26.3       |
|    mean_reward          | 1.89       |
| time/                   |            |
|    total_timesteps      | 160000     |
| train/                  |            |
|    approx_kl            | 0.05283752 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.261     |
|    explained_variance   | 0.304      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.447      |
|    n_updates            | 10540      |
|    policy_gradient_loss | -0.00399   |
|    value_loss           | 2.12       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 5.28     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 79       |
|    time_elapsed    | 667      |
|    total_timesteps | 161792   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 5.11        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 80          |
|    time_elapsed         | 675         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.044844437 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.275      |
|    explained_variance   | 0.34        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.927       |
|    n_updates            | 10550       |
|    policy_gradient_loss | -0.00352    |
|    value_loss           | 2.92        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 4.53       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 81         |
|    time_elapsed         | 684        |
|    total_timesteps      | 165888     |
| train/                  |            |
|    approx_kl            | 0.05131176 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.369      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.816      |
|    n_updates            | 10560      |
|    policy_gradient_loss | 0.00563    |
|    value_loss           | 2.79       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 4.18        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 82          |
|    time_elapsed         | 692         |
|    total_timesteps      | 167936      |
| train/                  |             |
|    approx_kl            | 0.030090969 |
|    clip_fraction        | 0.142       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.313      |
|    explained_variance   | 0.326       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.69        |
|    n_updates            | 10570       |
|    policy_gradient_loss | -0.0108     |
|    value_loss           | 2.38        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 5.7         |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 83          |
|    time_elapsed         | 701         |
|    total_timesteps      | 169984      |
| train/                  |             |
|    approx_kl            | 0.035688758 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.296      |
|    explained_variance   | 0.474       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.978       |
|    n_updates            | 10580       |
|    policy_gradient_loss | -0.00867    |
|    value_loss           | 2.52        |
-----------------------------------------
Eval num_timesteps=170000, episode_reward=6.50 +/- 4.25
Episode length: 13.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13          |
|    mean_reward          | 6.5         |
| time/                   |             |
|    total_timesteps      | 170000      |
| train/                  |             |
|    approx_kl            | 0.061103158 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.261      |
|    explained_variance   | 0.355       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.18        |
|    n_updates            | 10590       |
|    policy_gradient_loss | -0.00173    |
|    value_loss           | 2.21        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 4.74     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 84       |
|    time_elapsed    | 710      |
|    total_timesteps | 172032   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.6        |
|    ep_rew_mean          | 4.76        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 85          |
|    time_elapsed         | 718         |
|    total_timesteps      | 174080      |
| train/                  |             |
|    approx_kl            | 0.052555136 |
|    clip_fraction        | 0.155       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.304      |
|    explained_variance   | 0.329       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.72        |
|    n_updates            | 10600       |
|    policy_gradient_loss | -0.00717    |
|    value_loss           | 2.58        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 5.14        |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 86          |
|    time_elapsed         | 728         |
|    total_timesteps      | 176128      |
| train/                  |             |
|    approx_kl            | 0.047348533 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.332      |
|    explained_variance   | 0.357       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.06        |
|    n_updates            | 10610       |
|    policy_gradient_loss | -0.00328    |
|    value_loss           | 2.72        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 4.78        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 87          |
|    time_elapsed         | 735         |
|    total_timesteps      | 178176      |
| train/                  |             |
|    approx_kl            | 0.033668064 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.373       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.901       |
|    n_updates            | 10620       |
|    policy_gradient_loss | -0.00577    |
|    value_loss           | 2.39        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=180000, episode_reward=-5.61 +/- 6.21
Episode length: 37.67 +/- 17.44
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.7        |
|    mean_reward          | -5.61       |
| time/                   |             |
|    total_timesteps      | 180000      |
| train/                  |             |
|    approx_kl            | 0.056960225 |
|    clip_fraction        | 0.173       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.347      |
|    explained_variance   | 0.294       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.1         |
|    n_updates            | 10630       |
|    policy_gradient_loss | -0.0111     |
|    value_loss           | 2.55        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.9     |
|    ep_rew_mean     | 4.93     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 88       |
|    time_elapsed    | 744      |
|    total_timesteps | 180224   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 5.09       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 89         |
|    time_elapsed         | 753        |
|    total_timesteps      | 182272     |
| train/                  |            |
|    approx_kl            | 0.03518571 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.358     |
|    explained_variance   | 0.47       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.25       |
|    n_updates            | 10640      |
|    policy_gradient_loss | -0.00963   |
|    value_loss           | 3.01       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.2       |
|    ep_rew_mean          | 5.03       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 90         |
|    time_elapsed         | 760        |
|    total_timesteps      | 184320     |
| train/                  |            |
|    approx_kl            | 0.04649119 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.342     |
|    explained_variance   | 0.297      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.09       |
|    n_updates            | 10650      |
|    policy_gradient_loss | -0.00258   |
|    value_loss           | 2.82       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 5.27       |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 91         |
|    time_elapsed         | 770        |
|    total_timesteps      | 186368     |
| train/                  |            |
|    approx_kl            | 0.07374808 |
|    clip_fraction        | 0.208      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.35      |
|    explained_variance   | 0.431      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.858      |
|    n_updates            | 10660      |
|    policy_gradient_loss | -0.00727   |
|    value_loss           | 2.64       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | 6.06       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 92         |
|    time_elapsed         | 778        |
|    total_timesteps      | 188416     |
| train/                  |            |
|    approx_kl            | 0.03316178 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.314     |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.62       |
|    n_updates            | 10670      |
|    policy_gradient_loss | -0.00432   |
|    value_loss           | 2.15       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=190000, episode_reward=-5.68 +/- 6.10
Episode length: 38.00 +/- 16.97
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 38         |
|    mean_reward          | -5.68      |
| time/                   |            |
|    total_timesteps      | 190000     |
| train/                  |            |
|    approx_kl            | 0.03411781 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.28      |
|    explained_variance   | 0.449      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.5        |
|    n_updates            | 10680      |
|    policy_gradient_loss | -0.00522   |
|    value_loss           | 2.88       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 5.61     |
| time/              |          |
|    fps             | 241      |
|    iterations      | 93       |
|    time_elapsed    | 787      |
|    total_timesteps | 190464   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.1       |
|    ep_rew_mean          | 4.5        |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 94         |
|    time_elapsed         | 795        |
|    total_timesteps      | 192512     |
| train/                  |            |
|    approx_kl            | 0.04418831 |
|    clip_fraction        | 0.149      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.285     |
|    explained_variance   | 0.362      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.26       |
|    n_updates            | 10690      |
|    policy_gradient_loss | -0.00531   |
|    value_loss           | 2.16       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.5       |
|    ep_rew_mean          | 5.22       |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 95         |
|    time_elapsed         | 805        |
|    total_timesteps      | 194560     |
| train/                  |            |
|    approx_kl            | 0.08427042 |
|    clip_fraction        | 0.236      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.326     |
|    explained_variance   | 0.39       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.877      |
|    n_updates            | 10700      |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 2.45       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 5.24        |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 96          |
|    time_elapsed         | 815         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.047059786 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.285      |
|    explained_variance   | 0.46        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.928       |
|    n_updates            | 10710       |
|    policy_gradient_loss | -0.00493    |
|    value_loss           | 2.07        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 4.93        |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 97          |
|    time_elapsed         | 824         |
|    total_timesteps      | 198656      |
| train/                  |             |
|    approx_kl            | 0.035527613 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.247      |
|    explained_variance   | 0.483       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.879       |
|    n_updates            | 10720       |
|    policy_gradient_loss | -0.00212    |
|    value_loss           | 2.27        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=200000, episode_reward=5.99 +/- 2.63
Episode length: 15.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 5.99        |
| time/                   |             |
|    total_timesteps      | 200000      |
| train/                  |             |
|    approx_kl            | 0.041019887 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.294      |
|    explained_variance   | 0.422       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.21        |
|    n_updates            | 10730       |
|    policy_gradient_loss | -0.00932    |
|    value_loss           | 2.84        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 5.38     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 98       |
|    time_elapsed    | 833      |
|    total_timesteps | 200704   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 4.42        |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 99          |
|    time_elapsed         | 841         |
|    total_timesteps      | 202752      |
| train/                  |             |
|    approx_kl            | 0.052381083 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.248      |
|    explained_variance   | 0.349       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.803       |
|    n_updates            | 10740       |
|    policy_gradient_loss | -0.00685    |
|    value_loss           | 2.47        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 4.91       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 100        |
|    time_elapsed         | 850        |
|    total_timesteps      | 204800     |
| train/                  |            |
|    approx_kl            | 0.05382612 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.277     |
|    explained_variance   | 0.426      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.952      |
|    n_updates            | 10750      |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 2.86       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 4.47       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 101        |
|    time_elapsed         | 859        |
|    total_timesteps      | 206848     |
| train/                  |            |
|    approx_kl            | 0.04254573 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.267     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.864      |
|    n_updates            | 10760      |
|    policy_gradient_loss | -0.0136    |
|    value_loss           | 2.24       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 6.94        |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 102         |
|    time_elapsed         | 869         |
|    total_timesteps      | 208896      |
| train/                  |             |
|    approx_kl            | 0.056045055 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.244      |
|    explained_variance   | 0.436       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.839       |
|    n_updates            | 10770       |
|    policy_gradient_loss | -0.00425    |
|    value_loss           | 2.62        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=210000, episode_reward=-2.21 +/- 5.68
Episode length: 37.33 +/- 17.91
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 37.3        |
|    mean_reward          | -2.21       |
| time/                   |             |
|    total_timesteps      | 210000      |
| train/                  |             |
|    approx_kl            | 0.057269365 |
|    clip_fraction        | 0.132       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.524       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.12        |
|    n_updates            | 10780       |
|    policy_gradient_loss | -0.00654    |
|    value_loss           | 2.61        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 4.97     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 103      |
|    time_elapsed    | 877      |
|    total_timesteps | 210944   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 5.72       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 104        |
|    time_elapsed         | 885        |
|    total_timesteps      | 212992     |
| train/                  |            |
|    approx_kl            | 0.07593595 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.488      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.05       |
|    n_updates            | 10790      |
|    policy_gradient_loss | 0.00235    |
|    value_loss           | 2.32       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 6.22       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 105        |
|    time_elapsed         | 892        |
|    total_timesteps      | 215040     |
| train/                  |            |
|    approx_kl            | 0.06945396 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.267     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.57       |
|    n_updates            | 10800      |
|    policy_gradient_loss | -0.0114    |
|    value_loss           | 1.99       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 5.45        |
| time/                   |             |
|    fps                  | 240         |
|    iterations           | 106         |
|    time_elapsed         | 901         |
|    total_timesteps      | 217088      |
| train/                  |             |
|    approx_kl            | 0.044832878 |
|    clip_fraction        | 0.148       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.237      |
|    explained_variance   | 0.5         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.83        |
|    n_updates            | 10810       |
|    policy_gradient_loss | -0.00224    |
|    value_loss           | 2.6         |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 6.11       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 107        |
|    time_elapsed         | 910        |
|    total_timesteps      | 219136     |
| train/                  |            |
|    approx_kl            | 0.03861795 |
|    clip_fraction        | 0.159      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.281     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.13       |
|    n_updates            | 10820      |
|    policy_gradient_loss | -0.00592   |
|    value_loss           | 2.31       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=220000, episode_reward=-4.16 +/- 8.26
Episode length: 38.67 +/- 16.03
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.7        |
|    mean_reward          | -4.16       |
| time/                   |             |
|    total_timesteps      | 220000      |
| train/                  |             |
|    approx_kl            | 0.037015125 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.276      |
|    explained_variance   | 0.516       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.09        |
|    n_updates            | 10830       |
|    policy_gradient_loss | -0.0039     |
|    value_loss           | 2.24        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 4.91     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 108      |
|    time_elapsed    | 918      |
|    total_timesteps | 221184   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 5.63        |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 109         |
|    time_elapsed         | 924         |
|    total_timesteps      | 223232      |
| train/                  |             |
|    approx_kl            | 0.048487093 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.317      |
|    explained_variance   | 0.471       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.01        |
|    n_updates            | 10840       |
|    policy_gradient_loss | -0.00921    |
|    value_loss           | 2.75        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.8       |
|    ep_rew_mean          | 5.65       |
| time/                   |            |
|    fps                  | 241        |
|    iterations           | 110        |
|    time_elapsed         | 931        |
|    total_timesteps      | 225280     |
| train/                  |            |
|    approx_kl            | 0.03921339 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.447      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.41       |
|    n_updates            | 10850      |
|    policy_gradient_loss | -0.00397   |
|    value_loss           | 2.79       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 5.38       |
| time/                   |            |
|    fps                  | 242        |
|    iterations           | 111        |
|    time_elapsed         | 938        |
|    total_timesteps      | 227328     |
| train/                  |            |
|    approx_kl            | 0.02812622 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.291     |
|    explained_variance   | 0.495      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.71       |
|    n_updates            | 10860      |
|    policy_gradient_loss | -0.0021    |
|    value_loss           | 2.32       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 5.02        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 112         |
|    time_elapsed         | 944         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.037306905 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.283      |
|    explained_variance   | 0.628       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.483       |
|    n_updates            | 10870       |
|    policy_gradient_loss | -0.00601    |
|    value_loss           | 2.01        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=230000, episode_reward=8.24 +/- 3.91
Episode length: 12.67 +/- 0.94
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 12.7       |
|    mean_reward          | 8.24       |
| time/                   |            |
|    total_timesteps      | 230000     |
| train/                  |            |
|    approx_kl            | 0.06249972 |
|    clip_fraction        | 0.15       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.491      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.675      |
|    n_updates            | 10880      |
|    policy_gradient_loss | -0.00433   |
|    value_loss           | 2.14       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 5.43     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 113      |
|    time_elapsed    | 951      |
|    total_timesteps | 231424   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 6.1        |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 114        |
|    time_elapsed         | 958        |
|    total_timesteps      | 233472     |
| train/                  |            |
|    approx_kl            | 0.03238575 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.288     |
|    explained_variance   | 0.593      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.61       |
|    n_updates            | 10890      |
|    policy_gradient_loss | -0.0105    |
|    value_loss           | 1.84       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 6.73        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 115         |
|    time_elapsed         | 964         |
|    total_timesteps      | 235520      |
| train/                  |             |
|    approx_kl            | 0.031438705 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.221      |
|    explained_variance   | 0.616       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.703       |
|    n_updates            | 10900       |
|    policy_gradient_loss | -0.00596    |
|    value_loss           | 1.64        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.6        |
|    ep_rew_mean          | 5.58        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 116         |
|    time_elapsed         | 971         |
|    total_timesteps      | 237568      |
| train/                  |             |
|    approx_kl            | 0.050455116 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.582       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.74        |
|    n_updates            | 10910       |
|    policy_gradient_loss | 0.00114     |
|    value_loss           | 1.89        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.9        |
|    ep_rew_mean          | 5.22        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 117         |
|    time_elapsed         | 977         |
|    total_timesteps      | 239616      |
| train/                  |             |
|    approx_kl            | 0.046315745 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.325      |
|    explained_variance   | 0.545       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.788       |
|    n_updates            | 10920       |
|    policy_gradient_loss | -0.00539    |
|    value_loss           | 2.37        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=240000, episode_reward=1.89 +/- 8.41
Episode length: 26.33 +/- 16.74
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26.3       |
|    mean_reward          | 1.89       |
| time/                   |            |
|    total_timesteps      | 240000     |
| train/                  |            |
|    approx_kl            | 0.06902829 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.354     |
|    explained_variance   | 0.522      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.618      |
|    n_updates            | 10930      |
|    policy_gradient_loss | -0.0086    |
|    value_loss           | 2.22       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20       |
|    ep_rew_mean     | 6.06     |
| time/              |          |
|    fps             | 245      |
|    iterations      | 118      |
|    time_elapsed    | 985      |
|    total_timesteps | 241664   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 5.09        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 119         |
|    time_elapsed         | 992         |
|    total_timesteps      | 243712      |
| train/                  |             |
|    approx_kl            | 0.049834542 |
|    clip_fraction        | 0.175       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.322      |
|    explained_variance   | 0.25        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.695       |
|    n_updates            | 10940       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 2.26        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 6.51        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 120         |
|    time_elapsed         | 1000        |
|    total_timesteps      | 245760      |
| train/                  |             |
|    approx_kl            | 0.060497742 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.311      |
|    explained_variance   | 0.589       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.583       |
|    n_updates            | 10950       |
|    policy_gradient_loss | -0.00429    |
|    value_loss           | 1.72        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.2       |
|    ep_rew_mean          | 6.54       |
| time/                   |            |
|    fps                  | 245        |
|    iterations           | 121        |
|    time_elapsed         | 1007       |
|    total_timesteps      | 247808     |
| train/                  |            |
|    approx_kl            | 0.05512529 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.281     |
|    explained_variance   | 0.564      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.612      |
|    n_updates            | 10960      |
|    policy_gradient_loss | -0.0102    |
|    value_loss           | 1.81       |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 20        |
|    ep_rew_mean          | 5.92      |
| time/                   |           |
|    fps                  | 246       |
|    iterations           | 122       |
|    time_elapsed         | 1014      |
|    total_timesteps      | 249856    |
| train/                  |           |
|    approx_kl            | 0.0494406 |
|    clip_fraction        | 0.143     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.241    |
|    explained_variance   | 0.573     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.42      |
|    n_updates            | 10970     |
|    policy_gradient_loss | -0.0123   |
|    value_loss           | 1.79      |
---------------------------------------
reached max steps=100
Eval num_timesteps=250000, episode_reward=9.50 +/- 10.26
Episode length: 29.67 +/- 14.52
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 29.7        |
|    mean_reward          | 9.5         |
| time/                   |             |
|    total_timesteps      | 250000      |
| train/                  |             |
|    approx_kl            | 0.054319043 |
|    clip_fraction        | 0.146       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.29       |
|    explained_variance   | 0.579       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.571       |
|    n_updates            | 10980       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 1.51        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 6.14     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 123      |
|    time_elapsed    | 1021     |
|    total_timesteps | 251904   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 5.78       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 124        |
|    time_elapsed         | 1028       |
|    total_timesteps      | 253952     |
| train/                  |            |
|    approx_kl            | 0.06533114 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.309     |
|    explained_variance   | 0.503      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.951      |
|    n_updates            | 10990      |
|    policy_gradient_loss | -0.000532  |
|    value_loss           | 2.55       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 5.84       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 125        |
|    time_elapsed         | 1035       |
|    total_timesteps      | 256000     |
| train/                  |            |
|    approx_kl            | 0.10152869 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.597      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.651      |
|    n_updates            | 11000      |
|    policy_gradient_loss | -0.00298   |
|    value_loss           | 1.97       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 5.63       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 126        |
|    time_elapsed         | 1042       |
|    total_timesteps      | 258048     |
| train/                  |            |
|    approx_kl            | 0.04903711 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.291     |
|    explained_variance   | 0.552      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.682      |
|    n_updates            | 11010      |
|    policy_gradient_loss | 0.00222    |
|    value_loss           | 2.35       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=260000, episode_reward=-1.15 +/- 6.26
Episode length: 25.00 +/- 17.68
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25         |
|    mean_reward          | -1.15      |
| time/                   |            |
|    total_timesteps      | 260000     |
| train/                  |            |
|    approx_kl            | 0.03590333 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.267     |
|    explained_variance   | 0.582      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.03       |
|    n_updates            | 11020      |
|    policy_gradient_loss | -0.0079    |
|    value_loss           | 2.04       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21       |
|    ep_rew_mean     | 5.08     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 127      |
|    time_elapsed    | 1049     |
|    total_timesteps | 260096   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 5.46       |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 128        |
|    time_elapsed         | 1056       |
|    total_timesteps      | 262144     |
| train/                  |            |
|    approx_kl            | 0.05404634 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.29      |
|    explained_variance   | 0.506      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.825      |
|    n_updates            | 11030      |
|    policy_gradient_loss | -0.00326   |
|    value_loss           | 2.56       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.7        |
|    ep_rew_mean          | 5.45        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 129         |
|    time_elapsed         | 1063        |
|    total_timesteps      | 264192      |
| train/                  |             |
|    approx_kl            | 0.037146587 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.266      |
|    explained_variance   | 0.576       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.569       |
|    n_updates            | 11040       |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 1.83        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 6.6         |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 130         |
|    time_elapsed         | 1070        |
|    total_timesteps      | 266240      |
| train/                  |             |
|    approx_kl            | 0.054883156 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.236      |
|    explained_variance   | 0.371       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.565       |
|    n_updates            | 11050       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 1.71        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18         |
|    ep_rew_mean          | 6.51       |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 131        |
|    time_elapsed         | 1077       |
|    total_timesteps      | 268288     |
| train/                  |            |
|    approx_kl            | 0.06672463 |
|    clip_fraction        | 0.156      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.216     |
|    explained_variance   | 0.623      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.2        |
|    n_updates            | 11060      |
|    policy_gradient_loss | -0.00604   |
|    value_loss           | 2.17       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=270000, episode_reward=3.41 +/- 9.68
Episode length: 27.00 +/- 16.27
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27          |
|    mean_reward          | 3.41        |
| time/                   |             |
|    total_timesteps      | 270000      |
| train/                  |             |
|    approx_kl            | 0.033048503 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.625       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.585       |
|    n_updates            | 11070       |
|    policy_gradient_loss | -0.00284    |
|    value_loss           | 1.66        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.8     |
|    ep_rew_mean     | 6.8      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 132      |
|    time_elapsed    | 1084     |
|    total_timesteps | 270336   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.8       |
|    ep_rew_mean          | 5.07       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 133        |
|    time_elapsed         | 1091       |
|    total_timesteps      | 272384     |
| train/                  |            |
|    approx_kl            | 0.12574553 |
|    clip_fraction        | 0.184      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.204     |
|    explained_variance   | 0.536      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.14       |
|    n_updates            | 11080      |
|    policy_gradient_loss | -0.00636   |
|    value_loss           | 1.9        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.9       |
|    ep_rew_mean          | 6.59       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 134        |
|    time_elapsed         | 1099       |
|    total_timesteps      | 274432     |
| train/                  |            |
|    approx_kl            | 0.13434136 |
|    clip_fraction        | 0.16       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.237     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.54       |
|    n_updates            | 11090      |
|    policy_gradient_loss | -0.000569  |
|    value_loss           | 3.51       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 5.17       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 135        |
|    time_elapsed         | 1106       |
|    total_timesteps      | 276480     |
| train/                  |            |
|    approx_kl            | 0.14557171 |
|    clip_fraction        | 0.168      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.225     |
|    explained_variance   | 0.512      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.894      |
|    n_updates            | 11100      |
|    policy_gradient_loss | -0.00246   |
|    value_loss           | 2.58       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21.5       |
|    ep_rew_mean          | 5.52       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 136        |
|    time_elapsed         | 1113       |
|    total_timesteps      | 278528     |
| train/                  |            |
|    approx_kl            | 0.05494191 |
|    clip_fraction        | 0.164      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.247     |
|    explained_variance   | 0.56       |
|    learning_rate        | 0.0003     |
|    loss                 | 1.38       |
|    n_updates            | 11110      |
|    policy_gradient_loss | -0.00416   |
|    value_loss           | 2.54       |
----------------------------------------
reached max steps=100
Eval num_timesteps=280000, episode_reward=2.04 +/- 8.51
Episode length: 25.67 +/- 17.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.7       |
|    mean_reward          | 2.04       |
| time/                   |            |
|    total_timesteps      | 280000     |
| train/                  |            |
|    approx_kl            | 0.08593352 |
|    clip_fraction        | 0.204      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.295     |
|    explained_variance   | 0.516      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.19       |
|    n_updates            | 11120      |
|    policy_gradient_loss | -0.0161    |
|    value_loss           | 2.69       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 6.2      |
| time/              |          |
|    fps             | 250      |
|    iterations      | 137      |
|    time_elapsed    | 1121     |
|    total_timesteps | 280576   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 6.33       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 138        |
|    time_elapsed         | 1129       |
|    total_timesteps      | 282624     |
| train/                  |            |
|    approx_kl            | 0.05132515 |
|    clip_fraction        | 0.169      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.251     |
|    explained_variance   | 0.493      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.755      |
|    n_updates            | 11130      |
|    policy_gradient_loss | -0.00298   |
|    value_loss           | 2.01       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 6.48        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 139         |
|    time_elapsed         | 1136        |
|    total_timesteps      | 284672      |
| train/                  |             |
|    approx_kl            | 0.051213205 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.264      |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 11140       |
|    policy_gradient_loss | -0.00351    |
|    value_loss           | 2.12        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 6.24        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 140         |
|    time_elapsed         | 1143        |
|    total_timesteps      | 286720      |
| train/                  |             |
|    approx_kl            | 0.032238998 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.712       |
|    n_updates            | 11150       |
|    policy_gradient_loss | -0.00445    |
|    value_loss           | 2.12        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 6.71        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 141         |
|    time_elapsed         | 1151        |
|    total_timesteps      | 288768      |
| train/                  |             |
|    approx_kl            | 0.026870426 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | 0.655       |
|    learning_rate        | 0.0003      |
|    loss                 | 1           |
|    n_updates            | 11160       |
|    policy_gradient_loss | -0.00366    |
|    value_loss           | 1.88        |
-----------------------------------------
Eval num_timesteps=290000, episode_reward=9.03 +/- 1.23
Episode length: 16.67 +/- 5.19
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | 9.03        |
| time/                   |             |
|    total_timesteps      | 290000      |
| train/                  |             |
|    approx_kl            | 0.047452383 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | 0.552       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.769       |
|    n_updates            | 11170       |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 2.05        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 6.57     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 142      |
|    time_elapsed    | 1158     |
|    total_timesteps | 290816   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 6.05       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 143        |
|    time_elapsed         | 1165       |
|    total_timesteps      | 292864     |
| train/                  |            |
|    approx_kl            | 0.03510483 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.217     |
|    explained_variance   | 0.565      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.646      |
|    n_updates            | 11180      |
|    policy_gradient_loss | -0.0038    |
|    value_loss           | 1.78       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 6.18       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 144        |
|    time_elapsed         | 1172       |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.04906196 |
|    clip_fraction        | 0.161      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.267     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.763      |
|    n_updates            | 11190      |
|    policy_gradient_loss | -0.00663   |
|    value_loss           | 1.83       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 6.54        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 145         |
|    time_elapsed         | 1180        |
|    total_timesteps      | 296960      |
| train/                  |             |
|    approx_kl            | 0.053152155 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.437       |
|    n_updates            | 11200       |
|    policy_gradient_loss | -0.00305    |
|    value_loss           | 1.83        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 6.61        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 146         |
|    time_elapsed         | 1187        |
|    total_timesteps      | 299008      |
| train/                  |             |
|    approx_kl            | 0.046828426 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.184      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.496       |
|    n_updates            | 11210       |
|    policy_gradient_loss | -0.0025     |
|    value_loss           | 1.44        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=300000, episode_reward=5.15 +/- 7.46
Episode length: 26.67 +/- 16.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26.7       |
|    mean_reward          | 5.15       |
| time/                   |            |
|    total_timesteps      | 300000     |
| train/                  |            |
|    approx_kl            | 0.07036198 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.223     |
|    explained_variance   | 0.515      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.381      |
|    n_updates            | 11220      |
|    policy_gradient_loss | 0.0208     |
|    value_loss           | 1.69       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 7.43     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 147      |
|    time_elapsed    | 1196     |
|    total_timesteps | 301056   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 6.52        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 148         |
|    time_elapsed         | 1205        |
|    total_timesteps      | 303104      |
| train/                  |             |
|    approx_kl            | 0.064762555 |
|    clip_fraction        | 0.145       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.403       |
|    n_updates            | 11230       |
|    policy_gradient_loss | -0.00168    |
|    value_loss           | 1.9         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 6.96        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 149         |
|    time_elapsed         | 1216        |
|    total_timesteps      | 305152      |
| train/                  |             |
|    approx_kl            | 0.031662293 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.803       |
|    n_updates            | 11240       |
|    policy_gradient_loss | -0.00626    |
|    value_loss           | 2.1         |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 7.23       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 150        |
|    time_elapsed         | 1227       |
|    total_timesteps      | 307200     |
| train/                  |            |
|    approx_kl            | 0.04243239 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.197     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.663      |
|    n_updates            | 11250      |
|    policy_gradient_loss | -0.00983   |
|    value_loss           | 1.62       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 5.61        |
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 151         |
|    time_elapsed         | 1239        |
|    total_timesteps      | 309248      |
| train/                  |             |
|    approx_kl            | 0.046971094 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.213      |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.467       |
|    n_updates            | 11260       |
|    policy_gradient_loss | -0.0119     |
|    value_loss           | 1.68        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=310000, episode_reward=5.37 +/- 3.80
Episode length: 25.67 +/- 17.21
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 25.7       |
|    mean_reward          | 5.37       |
| time/                   |            |
|    total_timesteps      | 310000     |
| train/                  |            |
|    approx_kl            | 0.09518378 |
|    clip_fraction        | 0.18       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.241     |
|    explained_variance   | 0.606      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.304      |
|    n_updates            | 11270      |
|    policy_gradient_loss | -0.00564   |
|    value_loss           | 1.24       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.3     |
|    ep_rew_mean     | 7.1      |
| time/              |          |
|    fps             | 249      |
|    iterations      | 152      |
|    time_elapsed    | 1246     |
|    total_timesteps | 311296   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 6.09        |
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 153         |
|    time_elapsed         | 1256        |
|    total_timesteps      | 313344      |
| train/                  |             |
|    approx_kl            | 0.040766045 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.228      |
|    explained_variance   | 0.698       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.763       |
|    n_updates            | 11280       |
|    policy_gradient_loss | -0.00721    |
|    value_loss           | 1.75        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 6.66        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 154         |
|    time_elapsed         | 1267        |
|    total_timesteps      | 315392      |
| train/                  |             |
|    approx_kl            | 0.036229797 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.273      |
|    explained_variance   | 0.615       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.827       |
|    n_updates            | 11290       |
|    policy_gradient_loss | -0.00292    |
|    value_loss           | 1.92        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 6.22       |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 155        |
|    time_elapsed         | 1276       |
|    total_timesteps      | 317440     |
| train/                  |            |
|    approx_kl            | 0.07475269 |
|    clip_fraction        | 0.162      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.245     |
|    explained_variance   | 0.457      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.569      |
|    n_updates            | 11300      |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 1.84       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 6.63        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 156         |
|    time_elapsed         | 1283        |
|    total_timesteps      | 319488      |
| train/                  |             |
|    approx_kl            | 0.053604387 |
|    clip_fraction        | 0.162       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.268      |
|    explained_variance   | 0.507       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.43        |
|    n_updates            | 11310       |
|    policy_gradient_loss | -0.00202    |
|    value_loss           | 1.74        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=320000, episode_reward=6.21 +/- 2.00
Episode length: 14.33 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 6.21        |
| time/                   |             |
|    total_timesteps      | 320000      |
| train/                  |             |
|    approx_kl            | 0.058708772 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.259      |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.631       |
|    n_updates            | 11320       |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 1.53        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 6.4      |
| time/              |          |
|    fps             | 248      |
|    iterations      | 157      |
|    time_elapsed    | 1291     |
|    total_timesteps | 321536   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 6.61        |
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 158         |
|    time_elapsed         | 1298        |
|    total_timesteps      | 323584      |
| train/                  |             |
|    approx_kl            | 0.028821379 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | 0.583       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.519       |
|    n_updates            | 11330       |
|    policy_gradient_loss | -0.00709    |
|    value_loss           | 1.68        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 6.26        |
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 159         |
|    time_elapsed         | 1306        |
|    total_timesteps      | 325632      |
| train/                  |             |
|    approx_kl            | 0.040456697 |
|    clip_fraction        | 0.153       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.635       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.54        |
|    n_updates            | 11340       |
|    policy_gradient_loss | 0.00568     |
|    value_loss           | 1.7         |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.5       |
|    ep_rew_mean          | 5.49       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 160        |
|    time_elapsed         | 1314       |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.04868763 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.275     |
|    explained_variance   | 0.656      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.436      |
|    n_updates            | 11350      |
|    policy_gradient_loss | -0.014     |
|    value_loss           | 1.82       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18         |
|    ep_rew_mean          | 7.05       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 161        |
|    time_elapsed         | 1321       |
|    total_timesteps      | 329728     |
| train/                  |            |
|    approx_kl            | 0.03473696 |
|    clip_fraction        | 0.144      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.243     |
|    explained_variance   | 0.612      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.5        |
|    n_updates            | 11360      |
|    policy_gradient_loss | -0.00447   |
|    value_loss           | 1.65       |
----------------------------------------
Eval num_timesteps=330000, episode_reward=7.44 +/- 3.75
Episode length: 16.33 +/- 2.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 7.44       |
| time/                   |            |
|    total_timesteps      | 330000     |
| train/                  |            |
|    approx_kl            | 0.03863611 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.698      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.124      |
|    n_updates            | 11370      |
|    policy_gradient_loss | -0.00298   |
|    value_loss           | 1.22       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 6.15     |
| time/              |          |
|    fps             | 249      |
|    iterations      | 162      |
|    time_elapsed    | 1329     |
|    total_timesteps | 331776   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 7.23        |
| time/                   |             |
|    fps                  | 249         |
|    iterations           | 163         |
|    time_elapsed         | 1336        |
|    total_timesteps      | 333824      |
| train/                  |             |
|    approx_kl            | 0.028411314 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.246      |
|    explained_variance   | 0.594       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.741       |
|    n_updates            | 11380       |
|    policy_gradient_loss | -0.0089     |
|    value_loss           | 1.97        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.9       |
|    ep_rew_mean          | 6.97       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 164        |
|    time_elapsed         | 1342       |
|    total_timesteps      | 335872     |
| train/                  |            |
|    approx_kl            | 0.04201443 |
|    clip_fraction        | 0.132      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.251     |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.727      |
|    n_updates            | 11390      |
|    policy_gradient_loss | 0.00127    |
|    value_loss           | 2.09       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 6.72       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 165        |
|    time_elapsed         | 1349       |
|    total_timesteps      | 337920     |
| train/                  |            |
|    approx_kl            | 0.04204473 |
|    clip_fraction        | 0.139      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.269     |
|    explained_variance   | 0.647      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.705      |
|    n_updates            | 11400      |
|    policy_gradient_loss | -0.00659   |
|    value_loss           | 2.2        |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 6.78        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 166         |
|    time_elapsed         | 1356        |
|    total_timesteps      | 339968      |
| train/                  |             |
|    approx_kl            | 0.045505784 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.235      |
|    explained_variance   | 0.643       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.495       |
|    n_updates            | 11410       |
|    policy_gradient_loss | -0.00704    |
|    value_loss           | 1.94        |
-----------------------------------------
Eval num_timesteps=340000, episode_reward=10.70 +/- 4.68
Episode length: 16.67 +/- 2.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.7       |
|    mean_reward          | 10.7       |
| time/                   |            |
|    total_timesteps      | 340000     |
| train/                  |            |
|    approx_kl            | 0.06910792 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.249     |
|    explained_variance   | 0.666      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.552      |
|    n_updates            | 11420      |
|    policy_gradient_loss | -0.0153    |
|    value_loss           | 1.58       |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 6.83     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 167      |
|    time_elapsed    | 1365     |
|    total_timesteps | 342016   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.1        |
|    ep_rew_mean          | 5.96        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 168         |
|    time_elapsed         | 1373        |
|    total_timesteps      | 344064      |
| train/                  |             |
|    approx_kl            | 0.046874985 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.229      |
|    explained_variance   | 0.639       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.517       |
|    n_updates            | 11430       |
|    policy_gradient_loss | -0.00266    |
|    value_loss           | 1.37        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 6.02        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 169         |
|    time_elapsed         | 1380        |
|    total_timesteps      | 346112      |
| train/                  |             |
|    approx_kl            | 0.066579595 |
|    clip_fraction        | 0.224       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.301      |
|    explained_variance   | 0.55        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.88        |
|    n_updates            | 11440       |
|    policy_gradient_loss | -0.0197     |
|    value_loss           | 2.44        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 6.61        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 170         |
|    time_elapsed         | 1388        |
|    total_timesteps      | 348160      |
| train/                  |             |
|    approx_kl            | 0.045891598 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.231      |
|    explained_variance   | 0.618       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.44        |
|    n_updates            | 11450       |
|    policy_gradient_loss | -0.00622    |
|    value_loss           | 1.89        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=350000, episode_reward=-10.00 +/- 0.00
Episode length: 50.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 50         |
|    mean_reward          | -10        |
| time/                   |            |
|    total_timesteps      | 350000     |
| train/                  |            |
|    approx_kl            | 0.04435981 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.22      |
|    explained_variance   | 0.727      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.604      |
|    n_updates            | 11460      |
|    policy_gradient_loss | -0.00269   |
|    value_loss           | 1.51       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 22.4     |
|    ep_rew_mean     | 5.14     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 171      |
|    time_elapsed    | 1396     |
|    total_timesteps | 350208   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.2       |
|    ep_rew_mean          | 6.58       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 172        |
|    time_elapsed         | 1405       |
|    total_timesteps      | 352256     |
| train/                  |            |
|    approx_kl            | 0.07618438 |
|    clip_fraction        | 0.196      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.286     |
|    explained_variance   | 0.575      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.27       |
|    n_updates            | 11470      |
|    policy_gradient_loss | -0.0033    |
|    value_loss           | 3          |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 6.94       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 173        |
|    time_elapsed         | 1414       |
|    total_timesteps      | 354304     |
| train/                  |            |
|    approx_kl            | 0.08704831 |
|    clip_fraction        | 0.17       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.296     |
|    explained_variance   | 0.621      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.711      |
|    n_updates            | 11480      |
|    policy_gradient_loss | -0.0128    |
|    value_loss           | 1.86       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 5.92       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 174        |
|    time_elapsed         | 1425       |
|    total_timesteps      | 356352     |
| train/                  |            |
|    approx_kl            | 0.03187699 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.271     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.429      |
|    n_updates            | 11490      |
|    policy_gradient_loss | -0.00481   |
|    value_loss           | 2.1        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 6.32       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 175        |
|    time_elapsed         | 1434       |
|    total_timesteps      | 358400     |
| train/                  |            |
|    approx_kl            | 0.04420677 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.302     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.606      |
|    n_updates            | 11500      |
|    policy_gradient_loss | -0.00237   |
|    value_loss           | 1.9        |
----------------------------------------
reached max steps=100
Eval num_timesteps=360000, episode_reward=3.41 +/- 9.62
Episode length: 27.00 +/- 16.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27          |
|    mean_reward          | 3.41        |
| time/                   |             |
|    total_timesteps      | 360000      |
| train/                  |             |
|    approx_kl            | 0.054659426 |
|    clip_fraction        | 0.17        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.286      |
|    explained_variance   | 0.621       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.14        |
|    n_updates            | 11510       |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 2.08        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 7.37     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 176      |
|    time_elapsed    | 1441     |
|    total_timesteps | 360448   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 6.68        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 177         |
|    time_elapsed         | 1448        |
|    total_timesteps      | 362496      |
| train/                  |             |
|    approx_kl            | 0.046793357 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.253      |
|    explained_variance   | 0.656       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.488       |
|    n_updates            | 11520       |
|    policy_gradient_loss | -0.00274    |
|    value_loss           | 1.43        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 6.81       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 178        |
|    time_elapsed         | 1454       |
|    total_timesteps      | 364544     |
| train/                  |            |
|    approx_kl            | 0.04968856 |
|    clip_fraction        | 0.157      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.253     |
|    explained_variance   | 0.654      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.891      |
|    n_updates            | 11530      |
|    policy_gradient_loss | -0.00743   |
|    value_loss           | 1.62       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21          |
|    ep_rew_mean          | 6.12        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 179         |
|    time_elapsed         | 1461        |
|    total_timesteps      | 366592      |
| train/                  |             |
|    approx_kl            | 0.055222563 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.282      |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.759       |
|    n_updates            | 11540       |
|    policy_gradient_loss | -0.0164     |
|    value_loss           | 2.05        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.6       |
|    ep_rew_mean          | 6.22       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 180        |
|    time_elapsed         | 1469       |
|    total_timesteps      | 368640     |
| train/                  |            |
|    approx_kl            | 0.11118515 |
|    clip_fraction        | 0.173      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.27      |
|    explained_variance   | 0.568      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.862      |
|    n_updates            | 11550      |
|    policy_gradient_loss | -0.0109    |
|    value_loss           | 1.74       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=370000, episode_reward=3.49 +/- 9.68
Episode length: 26.67 +/- 16.58
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26.7       |
|    mean_reward          | 3.49       |
| time/                   |            |
|    total_timesteps      | 370000     |
| train/                  |            |
|    approx_kl            | 0.06669107 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.296     |
|    explained_variance   | 0.504      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.876      |
|    n_updates            | 11560      |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 2.13       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.6     |
|    ep_rew_mean     | 6.09     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 181      |
|    time_elapsed    | 1475     |
|    total_timesteps | 370688   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.3       |
|    ep_rew_mean          | 6.66       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 182        |
|    time_elapsed         | 1482       |
|    total_timesteps      | 372736     |
| train/                  |            |
|    approx_kl            | 0.07443467 |
|    clip_fraction        | 0.167      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.237     |
|    explained_variance   | 0.554      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.54       |
|    n_updates            | 11570      |
|    policy_gradient_loss | -0.00227   |
|    value_loss           | 2.15       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 6.82       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 183        |
|    time_elapsed         | 1489       |
|    total_timesteps      | 374784     |
| train/                  |            |
|    approx_kl            | 0.05221279 |
|    clip_fraction        | 0.145      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.228     |
|    explained_variance   | 0.681      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.406      |
|    n_updates            | 11580      |
|    policy_gradient_loss | -0.0053    |
|    value_loss           | 1.6        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.7       |
|    ep_rew_mean          | 6.17       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 184        |
|    time_elapsed         | 1496       |
|    total_timesteps      | 376832     |
| train/                  |            |
|    approx_kl            | 0.10547748 |
|    clip_fraction        | 0.166      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.234     |
|    explained_variance   | 0.64       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.729      |
|    n_updates            | 11590      |
|    policy_gradient_loss | -0.00417   |
|    value_loss           | 2.05       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.1       |
|    ep_rew_mean          | 6.79       |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 185        |
|    time_elapsed         | 1503       |
|    total_timesteps      | 378880     |
| train/                  |            |
|    approx_kl            | 0.08661838 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.276     |
|    explained_variance   | 0.615      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.621      |
|    n_updates            | 11600      |
|    policy_gradient_loss | -0.0088    |
|    value_loss           | 2.17       |
----------------------------------------
reached max steps=100
Eval num_timesteps=380000, episode_reward=9.47 +/- 2.16
Episode length: 14.67 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.7        |
|    mean_reward          | 9.47        |
| time/                   |             |
|    total_timesteps      | 380000      |
| train/                  |             |
|    approx_kl            | 0.090511784 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.687       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.803       |
|    n_updates            | 11610       |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 1.6         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 6.94     |
| time/              |          |
|    fps             | 252      |
|    iterations      | 186      |
|    time_elapsed    | 1510     |
|    total_timesteps | 380928   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 6.29       |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 187        |
|    time_elapsed         | 1517       |
|    total_timesteps      | 382976     |
| train/                  |            |
|    approx_kl            | 0.03624957 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.209     |
|    explained_variance   | 0.643      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.491      |
|    n_updates            | 11620      |
|    policy_gradient_loss | -0.00512   |
|    value_loss           | 1.79       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 7.1         |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 188         |
|    time_elapsed         | 1524        |
|    total_timesteps      | 385024      |
| train/                  |             |
|    approx_kl            | 0.057957157 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.224      |
|    explained_variance   | 0.712       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.275       |
|    n_updates            | 11630       |
|    policy_gradient_loss | 0.00358     |
|    value_loss           | 1.56        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 21.3        |
|    ep_rew_mean          | 5.97        |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 189         |
|    time_elapsed         | 1532        |
|    total_timesteps      | 387072      |
| train/                  |             |
|    approx_kl            | 0.042019933 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.24       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.624       |
|    n_updates            | 11640       |
|    policy_gradient_loss | -0.00365    |
|    value_loss           | 1.78        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 6.37        |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 190         |
|    time_elapsed         | 1541        |
|    total_timesteps      | 389120      |
| train/                  |             |
|    approx_kl            | 0.043143295 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.306      |
|    explained_variance   | 0.641       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.723       |
|    n_updates            | 11650       |
|    policy_gradient_loss | -0.00223    |
|    value_loss           | 2.23        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=390000, episode_reward=12.15 +/- 3.91
Episode length: 17.67 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.7        |
|    mean_reward          | 12.1        |
| time/                   |             |
|    total_timesteps      | 390000      |
| train/                  |             |
|    approx_kl            | 0.039119273 |
|    clip_fraction        | 0.19        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.304      |
|    explained_variance   | 0.61        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.865       |
|    n_updates            | 11660       |
|    policy_gradient_loss | -0.00798    |
|    value_loss           | 1.88        |
-----------------------------------------
New best mean reward!
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.8     |
|    ep_rew_mean     | 5.76     |
| time/              |          |
|    fps             | 252      |
|    iterations      | 191      |
|    time_elapsed    | 1550     |
|    total_timesteps | 391168   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.1       |
|    ep_rew_mean          | 6.08       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 192        |
|    time_elapsed         | 1563       |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.06404409 |
|    clip_fraction        | 0.207      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.33      |
|    explained_variance   | 0.451      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.978      |
|    n_updates            | 11670      |
|    policy_gradient_loss | -0.0203    |
|    value_loss           | 2.39       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 5.73       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 193        |
|    time_elapsed         | 1572       |
|    total_timesteps      | 395264     |
| train/                  |            |
|    approx_kl            | 0.06288868 |
|    clip_fraction        | 0.151      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.281     |
|    explained_variance   | 0.638      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.536      |
|    n_updates            | 11680      |
|    policy_gradient_loss | -0.00385   |
|    value_loss           | 1.93       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 6.68       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 194        |
|    time_elapsed         | 1581       |
|    total_timesteps      | 397312     |
| train/                  |            |
|    approx_kl            | 0.04408329 |
|    clip_fraction        | 0.177      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.327     |
|    explained_variance   | 0.657      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.443      |
|    n_updates            | 11690      |
|    policy_gradient_loss | -0.00289   |
|    value_loss           | 1.56       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 22.6       |
|    ep_rew_mean          | 5.47       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 195        |
|    time_elapsed         | 1589       |
|    total_timesteps      | 399360     |
| train/                  |            |
|    approx_kl            | 0.04122793 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.282     |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.482      |
|    n_updates            | 11700      |
|    policy_gradient_loss | -0.00624   |
|    value_loss           | 1.45       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=400000, episode_reward=8.74 +/- 5.31
Episode length: 18.00 +/- 4.55
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 18         |
|    mean_reward          | 8.74       |
| time/                   |            |
|    total_timesteps      | 400000     |
| train/                  |            |
|    approx_kl            | 0.08184324 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.394     |
|    explained_variance   | 0.636      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.445      |
|    n_updates            | 11710      |
|    policy_gradient_loss | -0.00361   |
|    value_loss           | 2.55       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 6.61     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 196      |
|    time_elapsed    | 1599     |
|    total_timesteps | 401408   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 6.5         |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 197         |
|    time_elapsed         | 1609        |
|    total_timesteps      | 403456      |
| train/                  |             |
|    approx_kl            | 0.041471995 |
|    clip_fraction        | 0.183       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.312      |
|    explained_variance   | 0.622       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.915       |
|    n_updates            | 11720       |
|    policy_gradient_loss | -0.00381    |
|    value_loss           | 2.12        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.8       |
|    ep_rew_mean          | 6.2        |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 198        |
|    time_elapsed         | 1617       |
|    total_timesteps      | 405504     |
| train/                  |            |
|    approx_kl            | 0.03717927 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.293     |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.16       |
|    n_updates            | 11730      |
|    policy_gradient_loss | -0.00494   |
|    value_loss           | 1.52       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | 6.84       |
| time/                   |            |
|    fps                  | 250        |
|    iterations           | 199        |
|    time_elapsed         | 1625       |
|    total_timesteps      | 407552     |
| train/                  |            |
|    approx_kl            | 0.03279459 |
|    clip_fraction        | 0.163      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.302     |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.951      |
|    n_updates            | 11740      |
|    policy_gradient_loss | -0.00476   |
|    value_loss           | 1.42       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 6.08        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 200         |
|    time_elapsed         | 1635        |
|    total_timesteps      | 409600      |
| train/                  |             |
|    approx_kl            | 0.037232563 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.287      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.705       |
|    n_updates            | 11750       |
|    policy_gradient_loss | -0.00563    |
|    value_loss           | 2.11        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=410000, episode_reward=4.32 +/- 1.98
Episode length: 15.33 +/- 2.49
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.3        |
|    mean_reward          | 4.32        |
| time/                   |             |
|    total_timesteps      | 410000      |
| train/                  |             |
|    approx_kl            | 0.034337364 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.246      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.855       |
|    n_updates            | 11760       |
|    policy_gradient_loss | -0.000778   |
|    value_loss           | 1.46        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 5.99     |
| time/              |          |
|    fps             | 250      |
|    iterations      | 201      |
|    time_elapsed    | 1641     |
|    total_timesteps | 411648   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20          |
|    ep_rew_mean          | 6.62        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 202         |
|    time_elapsed         | 1649        |
|    total_timesteps      | 413696      |
| train/                  |             |
|    approx_kl            | 0.029696798 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.258      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.539       |
|    n_updates            | 11770       |
|    policy_gradient_loss | -0.00729    |
|    value_loss           | 1.82        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 6.81        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 203         |
|    time_elapsed         | 1656        |
|    total_timesteps      | 415744      |
| train/                  |             |
|    approx_kl            | 0.060664102 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.234      |
|    explained_variance   | 0.651       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.29        |
|    n_updates            | 11780       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 1.97        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 6.57        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 204         |
|    time_elapsed         | 1662        |
|    total_timesteps      | 417792      |
| train/                  |             |
|    approx_kl            | 0.037098646 |
|    clip_fraction        | 0.123       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.711       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.62        |
|    n_updates            | 11790       |
|    policy_gradient_loss | -0.00356    |
|    value_loss           | 1.47        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 6.22       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 205        |
|    time_elapsed         | 1669       |
|    total_timesteps      | 419840     |
| train/                  |            |
|    approx_kl            | 0.06957105 |
|    clip_fraction        | 0.175      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.227     |
|    explained_variance   | 0.59       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.46       |
|    n_updates            | 11800      |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 1.8        |
----------------------------------------
reached max steps=100
Eval num_timesteps=420000, episode_reward=2.04 +/- 5.35
Episode length: 25.67 +/- 17.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 25.7      |
|    mean_reward          | 2.04      |
| time/                   |           |
|    total_timesteps      | 420000    |
| train/                  |           |
|    approx_kl            | 0.0828182 |
|    clip_fraction        | 0.149     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.251    |
|    explained_variance   | 0.721     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.15      |
|    n_updates            | 11810     |
|    policy_gradient_loss | -0.00716  |
|    value_loss           | 1.56      |
---------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 7.04     |
| time/              |          |
|    fps             | 251      |
|    iterations      | 206      |
|    time_elapsed    | 1677     |
|    total_timesteps | 421888   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.6        |
|    ep_rew_mean          | 7.25        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 207         |
|    time_elapsed         | 1684        |
|    total_timesteps      | 423936      |
| train/                  |             |
|    approx_kl            | 0.037837587 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.237      |
|    explained_variance   | 0.691       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.344       |
|    n_updates            | 11820       |
|    policy_gradient_loss | -0.00446    |
|    value_loss           | 1.37        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 7.26        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 208         |
|    time_elapsed         | 1691        |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.037615865 |
|    clip_fraction        | 0.15        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.242      |
|    explained_variance   | 0.689       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.652       |
|    n_updates            | 11830       |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 1.61        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.4       |
|    ep_rew_mean          | 6.97       |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 209        |
|    time_elapsed         | 1697       |
|    total_timesteps      | 428032     |
| train/                  |            |
|    approx_kl            | 0.06194011 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.212     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.602      |
|    n_updates            | 11840      |
|    policy_gradient_loss | -0.0134    |
|    value_loss           | 1.37       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=430000, episode_reward=0.53 +/- 7.70
Episode length: 28.00 +/- 16.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28          |
|    mean_reward          | 0.529       |
| time/                   |             |
|    total_timesteps      | 430000      |
| train/                  |             |
|    approx_kl            | 0.078537315 |
|    clip_fraction        | 0.154       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.222      |
|    explained_variance   | 0.695       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.317       |
|    n_updates            | 11850       |
|    policy_gradient_loss | -0.00906    |
|    value_loss           | 1.63        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18       |
|    ep_rew_mean     | 6.52     |
| time/              |          |
|    fps             | 252      |
|    iterations      | 210      |
|    time_elapsed    | 1704     |
|    total_timesteps | 430080   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 6.04        |
| time/                   |             |
|    fps                  | 252         |
|    iterations           | 211         |
|    time_elapsed         | 1711        |
|    total_timesteps      | 432128      |
| train/                  |             |
|    approx_kl            | 0.031729713 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.194      |
|    explained_variance   | 0.67        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.727       |
|    n_updates            | 11860       |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 1.64        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.2       |
|    ep_rew_mean          | 6.55       |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 212        |
|    time_elapsed         | 1717       |
|    total_timesteps      | 434176     |
| train/                  |            |
|    approx_kl            | 0.08509414 |
|    clip_fraction        | 0.155      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.222     |
|    explained_variance   | 0.631      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.676      |
|    n_updates            | 11870      |
|    policy_gradient_loss | -0.0115    |
|    value_loss           | 1.26       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 213        |
|    time_elapsed         | 1724       |
|    total_timesteps      | 436224     |
| train/                  |            |
|    approx_kl            | 0.07060539 |
|    clip_fraction        | 0.127      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.191     |
|    explained_variance   | 0.614      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.833      |
|    n_updates            | 11880      |
|    policy_gradient_loss | -0.00576   |
|    value_loss           | 1.76       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.5       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 253        |
|    iterations           | 214        |
|    time_elapsed         | 1731       |
|    total_timesteps      | 438272     |
| train/                  |            |
|    approx_kl            | 0.07432835 |
|    clip_fraction        | 0.0979     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.711      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.437      |
|    n_updates            | 11890      |
|    policy_gradient_loss | -0.00718   |
|    value_loss           | 1.33       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=440000, episode_reward=1.97 +/- 9.25
Episode length: 26.00 +/- 17.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26          |
|    mean_reward          | 1.97        |
| time/                   |             |
|    total_timesteps      | 440000      |
| train/                  |             |
|    approx_kl            | 0.057672057 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.763       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.414       |
|    n_updates            | 11900       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 1.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 6.41     |
| time/              |          |
|    fps             | 253      |
|    iterations      | 215      |
|    time_elapsed    | 1738     |
|    total_timesteps | 440320   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.3        |
|    ep_rew_mean          | 7.34        |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 216         |
|    time_elapsed         | 1745        |
|    total_timesteps      | 442368      |
| train/                  |             |
|    approx_kl            | 0.040991567 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.19       |
|    explained_variance   | 0.646       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.357       |
|    n_updates            | 11910       |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 1.34        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.4        |
|    ep_rew_mean          | 7.32        |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 217         |
|    time_elapsed         | 1752        |
|    total_timesteps      | 444416      |
| train/                  |             |
|    approx_kl            | 0.058136266 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.776       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.947       |
|    n_updates            | 11920       |
|    policy_gradient_loss | -0.00135    |
|    value_loss           | 1.02        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 6.88        |
| time/                   |             |
|    fps                  | 253         |
|    iterations           | 218         |
|    time_elapsed         | 1761        |
|    total_timesteps      | 446464      |
| train/                  |             |
|    approx_kl            | 0.049858525 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.54        |
|    n_updates            | 11930       |
|    policy_gradient_loss | -0.00361    |
|    value_loss           | 1.47        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | 6.55       |
| time/                   |            |
|    fps                  | 252        |
|    iterations           | 219        |
|    time_elapsed         | 1774       |
|    total_timesteps      | 448512     |
| train/                  |            |
|    approx_kl            | 0.03447859 |
|    clip_fraction        | 0.108      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.191     |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.373      |
|    n_updates            | 11940      |
|    policy_gradient_loss | -0.00684   |
|    value_loss           | 1.45       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=450000, episode_reward=-2.64 +/- 10.41
Episode length: 39.33 +/- 15.08
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.3        |
|    mean_reward          | -2.64       |
| time/                   |             |
|    total_timesteps      | 450000      |
| train/                  |             |
|    approx_kl            | 0.064452134 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.659       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.515       |
|    n_updates            | 11950       |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 1.32        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 6.76     |
| time/              |          |
|    fps             | 252      |
|    iterations      | 220      |
|    time_elapsed    | 1785     |
|    total_timesteps | 450560   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 6.91        |
| time/                   |             |
|    fps                  | 251         |
|    iterations           | 221         |
|    time_elapsed         | 1797        |
|    total_timesteps      | 452608      |
| train/                  |             |
|    approx_kl            | 0.056857772 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.225      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.15        |
|    n_updates            | 11960       |
|    policy_gradient_loss | -0.0129     |
|    value_loss           | 1.87        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18         |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 251        |
|    iterations           | 222        |
|    time_elapsed         | 1809       |
|    total_timesteps      | 454656     |
| train/                  |            |
|    approx_kl            | 0.04554695 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.218     |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.343      |
|    n_updates            | 11970      |
|    policy_gradient_loss | -0.00503   |
|    value_loss           | 1.53       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 7.45        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 223         |
|    time_elapsed         | 1821        |
|    total_timesteps      | 456704      |
| train/                  |             |
|    approx_kl            | 0.031127669 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.756       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.312       |
|    n_updates            | 11980       |
|    policy_gradient_loss | -0.00503    |
|    value_loss           | 1.21        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.4        |
|    ep_rew_mean          | 6.88        |
| time/                   |             |
|    fps                  | 250         |
|    iterations           | 224         |
|    time_elapsed         | 1831        |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.037127916 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.671       |
|    n_updates            | 11990       |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 1.23        |
-----------------------------------------
Eval num_timesteps=460000, episode_reward=8.17 +/- 6.76
Episode length: 13.00 +/- 1.41
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13          |
|    mean_reward          | 8.17        |
| time/                   |             |
|    total_timesteps      | 460000      |
| train/                  |             |
|    approx_kl            | 0.069655955 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.254      |
|    explained_variance   | 0.723       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.79        |
|    n_updates            | 12000       |
|    policy_gradient_loss | -0.00441    |
|    value_loss           | 1.71        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 6.43     |
| time/              |          |
|    fps             | 249      |
|    iterations      | 225      |
|    time_elapsed    | 1844     |
|    total_timesteps | 460800   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 6.55       |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 226        |
|    time_elapsed         | 1853       |
|    total_timesteps      | 462848     |
| train/                  |            |
|    approx_kl            | 0.04745378 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.238     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.993      |
|    n_updates            | 12010      |
|    policy_gradient_loss | -0.00444   |
|    value_loss           | 1.7        |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 6.7        |
| time/                   |            |
|    fps                  | 249        |
|    iterations           | 227        |
|    time_elapsed         | 1864       |
|    total_timesteps      | 464896     |
| train/                  |            |
|    approx_kl            | 0.04151719 |
|    clip_fraction        | 0.128      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.201     |
|    explained_variance   | 0.707      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.377      |
|    n_updates            | 12020      |
|    policy_gradient_loss | -0.00249   |
|    value_loss           | 1.16       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 6.66       |
| time/                   |            |
|    fps                  | 248        |
|    iterations           | 228        |
|    time_elapsed         | 1875       |
|    total_timesteps      | 466944     |
| train/                  |            |
|    approx_kl            | 0.17674865 |
|    clip_fraction        | 0.165      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.198     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.297      |
|    n_updates            | 12030      |
|    policy_gradient_loss | -0.00428   |
|    value_loss           | 1.77       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 6.78        |
| time/                   |             |
|    fps                  | 248         |
|    iterations           | 229         |
|    time_elapsed         | 1886        |
|    total_timesteps      | 468992      |
| train/                  |             |
|    approx_kl            | 0.060847025 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.649       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.609       |
|    n_updates            | 12040       |
|    policy_gradient_loss | -0.00646    |
|    value_loss           | 1.37        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=470000, episode_reward=6.35 +/- 2.12
Episode length: 13.67 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.7        |
|    mean_reward          | 6.35        |
| time/                   |             |
|    total_timesteps      | 470000      |
| train/                  |             |
|    approx_kl            | 0.052760158 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.661       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.455       |
|    n_updates            | 12050       |
|    policy_gradient_loss | -0.00698    |
|    value_loss           | 1.8         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 6.39     |
| time/              |          |
|    fps             | 248      |
|    iterations      | 230      |
|    time_elapsed    | 1897     |
|    total_timesteps | 471040   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.37        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 231         |
|    time_elapsed         | 1908        |
|    total_timesteps      | 473088      |
| train/                  |             |
|    approx_kl            | 0.037434965 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.227      |
|    explained_variance   | 0.735       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.506       |
|    n_updates            | 12060       |
|    policy_gradient_loss | -0.00681    |
|    value_loss           | 1.17        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 6.51        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 232         |
|    time_elapsed         | 1922        |
|    total_timesteps      | 475136      |
| train/                  |             |
|    approx_kl            | 0.042566195 |
|    clip_fraction        | 0.136       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.236      |
|    explained_variance   | 0.601       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.97        |
|    n_updates            | 12070       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 2.08        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 233        |
|    time_elapsed         | 1931       |
|    total_timesteps      | 477184     |
| train/                  |            |
|    approx_kl            | 0.05824723 |
|    clip_fraction        | 0.147      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.249     |
|    explained_variance   | 0.689      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.397      |
|    n_updates            | 12080      |
|    policy_gradient_loss | -0.00438   |
|    value_loss           | 1.71       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 6.97       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 234        |
|    time_elapsed         | 1940       |
|    total_timesteps      | 479232     |
| train/                  |            |
|    approx_kl            | 0.04846764 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.205     |
|    explained_variance   | 0.743      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.409      |
|    n_updates            | 12090      |
|    policy_gradient_loss | -0.0072    |
|    value_loss           | 1.35       |
----------------------------------------
reached max steps=100
Eval num_timesteps=480000, episode_reward=-2.88 +/- 5.25
Episode length: 28.33 +/- 15.37
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 28.3        |
|    mean_reward          | -2.88       |
| time/                   |             |
|    total_timesteps      | 480000      |
| train/                  |             |
|    approx_kl            | 0.041265473 |
|    clip_fraction        | 0.112       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.195      |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.221       |
|    n_updates            | 12100       |
|    policy_gradient_loss | -0.00468    |
|    value_loss           | 1.23        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.6     |
|    ep_rew_mean     | 6.15     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 235      |
|    time_elapsed    | 1950     |
|    total_timesteps | 481280   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 6.85        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 236         |
|    time_elapsed         | 1959        |
|    total_timesteps      | 483328      |
| train/                  |             |
|    approx_kl            | 0.046339422 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.197      |
|    explained_variance   | 0.666       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.645       |
|    n_updates            | 12110       |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 1.07        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 7.2         |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 237         |
|    time_elapsed         | 1967        |
|    total_timesteps      | 485376      |
| train/                  |             |
|    approx_kl            | 0.043428775 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.66        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.381       |
|    n_updates            | 12120       |
|    policy_gradient_loss | -0.00991    |
|    value_loss           | 1.31        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | 6.35        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 238         |
|    time_elapsed         | 1974        |
|    total_timesteps      | 487424      |
| train/                  |             |
|    approx_kl            | 0.034774248 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.177      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.38        |
|    n_updates            | 12130       |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 1.39        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.7       |
|    ep_rew_mean          | 7.03       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 239        |
|    time_elapsed         | 1982       |
|    total_timesteps      | 489472     |
| train/                  |            |
|    approx_kl            | 0.08665733 |
|    clip_fraction        | 0.133      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.196     |
|    explained_variance   | 0.729      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.547      |
|    n_updates            | 12140      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 1.11       |
----------------------------------------
reached max steps=100
Eval num_timesteps=490000, episode_reward=3.12 +/- 9.42
Episode length: 28.33 +/- 15.46
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.3       |
|    mean_reward          | 3.12       |
| time/                   |            |
|    total_timesteps      | 490000     |
| train/                  |            |
|    approx_kl            | 0.07571943 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.207     |
|    explained_variance   | 0.726      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.555      |
|    n_updates            | 12150      |
|    policy_gradient_loss | -0.00428   |
|    value_loss           | 1.63       |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.1     |
|    ep_rew_mean     | 6.35     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 240      |
|    time_elapsed    | 1990     |
|    total_timesteps | 491520   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 6.69        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 241         |
|    time_elapsed         | 1997        |
|    total_timesteps      | 493568      |
| train/                  |             |
|    approx_kl            | 0.069015436 |
|    clip_fraction        | 0.134       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.624       |
|    n_updates            | 12160       |
|    policy_gradient_loss | -0.00874    |
|    value_loss           | 2.03        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.7      |
|    ep_rew_mean          | 6.69      |
| time/                   |           |
|    fps                  | 247       |
|    iterations           | 242       |
|    time_elapsed         | 2004      |
|    total_timesteps      | 495616    |
| train/                  |           |
|    approx_kl            | 0.0415417 |
|    clip_fraction        | 0.123     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.205    |
|    explained_variance   | 0.608     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.348     |
|    n_updates            | 12170     |
|    policy_gradient_loss | -0.0164   |
|    value_loss           | 1.29      |
---------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.2       |
|    ep_rew_mean          | 7.33       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 243        |
|    time_elapsed         | 2012       |
|    total_timesteps      | 497664     |
| train/                  |            |
|    approx_kl            | 0.06622621 |
|    clip_fraction        | 0.141      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.228     |
|    explained_variance   | 0.607      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.867      |
|    n_updates            | 12180      |
|    policy_gradient_loss | -0.0191    |
|    value_loss           | 1.53       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.62        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 244         |
|    time_elapsed         | 2020        |
|    total_timesteps      | 499712      |
| train/                  |             |
|    approx_kl            | 0.027634814 |
|    clip_fraction        | 0.0892      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.401       |
|    n_updates            | 12190       |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 1.32        |
-----------------------------------------
Eval num_timesteps=500000, episode_reward=9.47 +/- 5.93
Episode length: 14.67 +/- 1.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 14.7       |
|    mean_reward          | 9.47       |
| time/                   |            |
|    total_timesteps      | 500000     |
| train/                  |            |
|    approx_kl            | 0.03558326 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.209     |
|    explained_variance   | 0.718      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.924      |
|    n_updates            | 12200      |
|    policy_gradient_loss | -0.0027    |
|    value_loss           | 1.76       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 7.32     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 245      |
|    time_elapsed    | 2027     |
|    total_timesteps | 501760   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 6.73        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 246         |
|    time_elapsed         | 2034        |
|    total_timesteps      | 503808      |
| train/                  |             |
|    approx_kl            | 0.060618144 |
|    clip_fraction        | 0.152       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.193      |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.284       |
|    n_updates            | 12210       |
|    policy_gradient_loss | -0.00745    |
|    value_loss           | 1.3         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.6       |
|    ep_rew_mean          | 6.69       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 247        |
|    time_elapsed         | 2041       |
|    total_timesteps      | 505856     |
| train/                  |            |
|    approx_kl            | 0.05220229 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.189     |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 1.06       |
|    n_updates            | 12220      |
|    policy_gradient_loss | -0.0111    |
|    value_loss           | 1.16       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 7.01        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 248         |
|    time_elapsed         | 2048        |
|    total_timesteps      | 507904      |
| train/                  |             |
|    approx_kl            | 0.061810408 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.741       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.519       |
|    n_updates            | 12230       |
|    policy_gradient_loss | -0.00521    |
|    value_loss           | 1.16        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 6.91        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 249         |
|    time_elapsed         | 2056        |
|    total_timesteps      | 509952      |
| train/                  |             |
|    approx_kl            | 0.049851082 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.23       |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.188       |
|    n_updates            | 12240       |
|    policy_gradient_loss | -0.0221     |
|    value_loss           | 1.17        |
-----------------------------------------
Eval num_timesteps=510000, episode_reward=10.63 +/- 4.72
Episode length: 17.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 10.6        |
| time/                   |             |
|    total_timesteps      | 510000      |
| train/                  |             |
|    approx_kl            | 0.035542447 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.239      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.329       |
|    n_updates            | 12250       |
|    policy_gradient_loss | -0.0146     |
|    value_loss           | 1.38        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | 7.6      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 250      |
|    time_elapsed    | 2064     |
|    total_timesteps | 512000   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 6.25        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 251         |
|    time_elapsed         | 2073        |
|    total_timesteps      | 514048      |
| train/                  |             |
|    approx_kl            | 0.041079927 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.198      |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.416       |
|    n_updates            | 12260       |
|    policy_gradient_loss | -0.00348    |
|    value_loss           | 1.26        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 6.78       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 252        |
|    time_elapsed         | 2083       |
|    total_timesteps      | 516096     |
| train/                  |            |
|    approx_kl            | 0.04770704 |
|    clip_fraction        | 0.154      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.236     |
|    explained_variance   | 0.724      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.154      |
|    n_updates            | 12270      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 1.28       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.6       |
|    ep_rew_mean          | 6.56       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 253        |
|    time_elapsed         | 2091       |
|    total_timesteps      | 518144     |
| train/                  |            |
|    approx_kl            | 0.06500067 |
|    clip_fraction        | 0.148      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.233     |
|    explained_variance   | 0.694      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.564      |
|    n_updates            | 12280      |
|    policy_gradient_loss | -0.00597   |
|    value_loss           | 1.75       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=520000, episode_reward=13.82 +/- 5.23
Episode length: 17.67 +/- 4.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 17.7      |
|    mean_reward          | 13.8      |
| time/                   |           |
|    total_timesteps      | 520000    |
| train/                  |           |
|    approx_kl            | 0.0970169 |
|    clip_fraction        | 0.141     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.226    |
|    explained_variance   | 0.649     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.436     |
|    n_updates            | 12290     |
|    policy_gradient_loss | -0.0157   |
|    value_loss           | 1.61      |
---------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.2     |
|    ep_rew_mean     | 6.5      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 254      |
|    time_elapsed    | 2100     |
|    total_timesteps | 520192   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 6.57        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 255         |
|    time_elapsed         | 2110        |
|    total_timesteps      | 522240      |
| train/                  |             |
|    approx_kl            | 0.033703186 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.214      |
|    explained_variance   | 0.567       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.594       |
|    n_updates            | 12300       |
|    policy_gradient_loss | -0.0137     |
|    value_loss           | 1.56        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.4        |
|    ep_rew_mean          | 6.29        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 256         |
|    time_elapsed         | 2119        |
|    total_timesteps      | 524288      |
| train/                  |             |
|    approx_kl            | 0.045091085 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.751       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.481       |
|    n_updates            | 12310       |
|    policy_gradient_loss | -0.00964    |
|    value_loss           | 1.11        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 7.4        |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 257        |
|    time_elapsed         | 2127       |
|    total_timesteps      | 526336     |
| train/                  |            |
|    approx_kl            | 0.04618574 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.226     |
|    explained_variance   | 0.624      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.294      |
|    n_updates            | 12320      |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 1.35       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 7.03        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 258         |
|    time_elapsed         | 2135        |
|    total_timesteps      | 528384      |
| train/                  |             |
|    approx_kl            | 0.035980627 |
|    clip_fraction        | 0.095       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.718       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.421       |
|    n_updates            | 12330       |
|    policy_gradient_loss | -0.00145    |
|    value_loss           | 1.24        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=530000, episode_reward=5.43 +/- 3.72
Episode length: 13.33 +/- 0.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 13.3      |
|    mean_reward          | 5.43      |
| time/                   |           |
|    total_timesteps      | 530000    |
| train/                  |           |
|    approx_kl            | 0.0314223 |
|    clip_fraction        | 0.118     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.199    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.38      |
|    n_updates            | 12340     |
|    policy_gradient_loss | -0.00801  |
|    value_loss           | 1.29      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 7.31     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 259      |
|    time_elapsed    | 2144     |
|    total_timesteps | 530432   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 7.12       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 260        |
|    time_elapsed         | 2152       |
|    total_timesteps      | 532480     |
| train/                  |            |
|    approx_kl            | 0.04412909 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.182     |
|    explained_variance   | 0.74       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.256      |
|    n_updates            | 12350      |
|    policy_gradient_loss | -0.00894   |
|    value_loss           | 1.27       |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.6      |
|    ep_rew_mean          | 7.72      |
| time/                   |           |
|    fps                  | 247       |
|    iterations           | 261       |
|    time_elapsed         | 2160      |
|    total_timesteps      | 534528    |
| train/                  |           |
|    approx_kl            | 0.0531883 |
|    clip_fraction        | 0.114     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.181    |
|    explained_variance   | 0.773     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.393     |
|    n_updates            | 12360     |
|    policy_gradient_loss | -0.0106   |
|    value_loss           | 1.41      |
---------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.2      |
|    ep_rew_mean          | 7.07      |
| time/                   |           |
|    fps                  | 247       |
|    iterations           | 262       |
|    time_elapsed         | 2167      |
|    total_timesteps      | 536576    |
| train/                  |           |
|    approx_kl            | 0.0640626 |
|    clip_fraction        | 0.122     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.195    |
|    explained_variance   | 0.738     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.754     |
|    n_updates            | 12370     |
|    policy_gradient_loss | -0.00263  |
|    value_loss           | 1.51      |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 7.25        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 263         |
|    time_elapsed         | 2175        |
|    total_timesteps      | 538624      |
| train/                  |             |
|    approx_kl            | 0.051395085 |
|    clip_fraction        | 0.137       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 1.23        |
|    n_updates            | 12380       |
|    policy_gradient_loss | -0.00683    |
|    value_loss           | 1.8         |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=540000, episode_reward=8.82 +/- 1.71
Episode length: 17.67 +/- 3.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.82       |
| time/                   |            |
|    total_timesteps      | 540000     |
| train/                  |            |
|    approx_kl            | 0.04323955 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.168     |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.484      |
|    n_updates            | 12390      |
|    policy_gradient_loss | -0.00664   |
|    value_loss           | 1.49       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 6.42     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 264      |
|    time_elapsed    | 2184     |
|    total_timesteps | 540672   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | 7.15       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 265        |
|    time_elapsed         | 2191       |
|    total_timesteps      | 542720     |
| train/                  |            |
|    approx_kl            | 0.04729689 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.196     |
|    explained_variance   | 0.722      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.244      |
|    n_updates            | 12400      |
|    policy_gradient_loss | -0.00458   |
|    value_loss           | 1.51       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 7.52        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 266         |
|    time_elapsed         | 2199        |
|    total_timesteps      | 544768      |
| train/                  |             |
|    approx_kl            | 0.038363896 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 12410       |
|    policy_gradient_loss | -0.0131     |
|    value_loss           | 1.88        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.3       |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 267        |
|    time_elapsed         | 2208       |
|    total_timesteps      | 546816     |
| train/                  |            |
|    approx_kl            | 0.03441818 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.198     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.393      |
|    n_updates            | 12420      |
|    policy_gradient_loss | -0.00529   |
|    value_loss           | 1.23       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 6.95        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 268         |
|    time_elapsed         | 2219        |
|    total_timesteps      | 548864      |
| train/                  |             |
|    approx_kl            | 0.051321838 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.693       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.243       |
|    n_updates            | 12430       |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 1.19        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=550000, episode_reward=9.25 +/- 2.00
Episode length: 15.67 +/- 1.70
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15.7        |
|    mean_reward          | 9.25        |
| time/                   |             |
|    total_timesteps      | 550000      |
| train/                  |             |
|    approx_kl            | 0.078089185 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.165      |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.323       |
|    n_updates            | 12440       |
|    policy_gradient_loss | -0.00781    |
|    value_loss           | 1.31        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.6     |
|    ep_rew_mean     | 6.38     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 269      |
|    time_elapsed    | 2227     |
|    total_timesteps | 550912   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.3        |
|    ep_rew_mean          | 5.93        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 270         |
|    time_elapsed         | 2237        |
|    total_timesteps      | 552960      |
| train/                  |             |
|    approx_kl            | 0.046961576 |
|    clip_fraction        | 0.157       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.207      |
|    explained_variance   | 0.672       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.421       |
|    n_updates            | 12450       |
|    policy_gradient_loss | -0.0069     |
|    value_loss           | 1.39        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 6.95       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 271        |
|    time_elapsed         | 2247       |
|    total_timesteps      | 555008     |
| train/                  |            |
|    approx_kl            | 0.06332019 |
|    clip_fraction        | 0.153      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.218     |
|    explained_variance   | 0.675      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.62       |
|    n_updates            | 12460      |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 1.9        |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 7.23       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 272        |
|    time_elapsed         | 2257       |
|    total_timesteps      | 557056     |
| train/                  |            |
|    approx_kl            | 0.13361286 |
|    clip_fraction        | 0.13       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.195     |
|    explained_variance   | 0.731      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.683      |
|    n_updates            | 12470      |
|    policy_gradient_loss | -0.00771   |
|    value_loss           | 1.51       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 6.57        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 273         |
|    time_elapsed         | 2265        |
|    total_timesteps      | 559104      |
| train/                  |             |
|    approx_kl            | 0.073086396 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.701       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.566       |
|    n_updates            | 12480       |
|    policy_gradient_loss | -0.0125     |
|    value_loss           | 1.36        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=560000, episode_reward=-5.09 +/- 9.15
Episode length: 38.33 +/- 16.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38.3        |
|    mean_reward          | -5.09       |
| time/                   |             |
|    total_timesteps      | 560000      |
| train/                  |             |
|    approx_kl            | 0.037398532 |
|    clip_fraction        | 0.0982      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.171      |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.59        |
|    n_updates            | 12490       |
|    policy_gradient_loss | -0.0011     |
|    value_loss           | 1.47        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | 6.53     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 274      |
|    time_elapsed    | 2273     |
|    total_timesteps | 561152   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 6.99        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 275         |
|    time_elapsed         | 2281        |
|    total_timesteps      | 563200      |
| train/                  |             |
|    approx_kl            | 0.049242683 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.707       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.311       |
|    n_updates            | 12500       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 1.01        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.6       |
|    ep_rew_mean          | 7.11       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 276        |
|    time_elapsed         | 2290       |
|    total_timesteps      | 565248     |
| train/                  |            |
|    approx_kl            | 0.03475283 |
|    clip_fraction        | 0.0983     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.16       |
|    n_updates            | 12510      |
|    policy_gradient_loss | -0.00414   |
|    value_loss           | 1          |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 6.64        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 277         |
|    time_elapsed         | 2297        |
|    total_timesteps      | 567296      |
| train/                  |             |
|    approx_kl            | 0.078203484 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.904       |
|    n_updates            | 12520       |
|    policy_gradient_loss | -0.0112     |
|    value_loss           | 1.34        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 6.95        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 278         |
|    time_elapsed         | 2305        |
|    total_timesteps      | 569344      |
| train/                  |             |
|    approx_kl            | 0.064618155 |
|    clip_fraction        | 0.109       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.716       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.175       |
|    n_updates            | 12530       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.4         |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=570000, episode_reward=8.89 +/- 5.70
Episode length: 17.33 +/- 3.68
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17.3        |
|    mean_reward          | 8.89        |
| time/                   |             |
|    total_timesteps      | 570000      |
| train/                  |             |
|    approx_kl            | 0.036771208 |
|    clip_fraction        | 0.0893      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.284       |
|    n_updates            | 12540       |
|    policy_gradient_loss | -0.00544    |
|    value_loss           | 1.4         |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.3     |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 247      |
|    iterations      | 279      |
|    time_elapsed    | 2311     |
|    total_timesteps | 571392   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 6.81        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 280         |
|    time_elapsed         | 2318        |
|    total_timesteps      | 573440      |
| train/                  |             |
|    approx_kl            | 0.048066713 |
|    clip_fraction        | 0.0972      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.681       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.385       |
|    n_updates            | 12550       |
|    policy_gradient_loss | -0.0013     |
|    value_loss           | 1.47        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 281        |
|    time_elapsed         | 2327       |
|    total_timesteps      | 575488     |
| train/                  |            |
|    approx_kl            | 0.03808528 |
|    clip_fraction        | 0.0856     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.162     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.36       |
|    n_updates            | 12560      |
|    policy_gradient_loss | -0.00643   |
|    value_loss           | 1.14       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 6.85        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 282         |
|    time_elapsed         | 2334        |
|    total_timesteps      | 577536      |
| train/                  |             |
|    approx_kl            | 0.034043364 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.679       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.876       |
|    n_updates            | 12570       |
|    policy_gradient_loss | 0.000813    |
|    value_loss           | 1.34        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.05        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 283         |
|    time_elapsed         | 2342        |
|    total_timesteps      | 579584      |
| train/                  |             |
|    approx_kl            | 0.040795688 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.317       |
|    n_updates            | 12580       |
|    policy_gradient_loss | -0.0106     |
|    value_loss           | 1.1         |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=580000, episode_reward=-3.35 +/- 8.03
Episode length: 38.00 +/- 16.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 38        |
|    mean_reward          | -3.35     |
| time/                   |           |
|    total_timesteps      | 580000    |
| train/                  |           |
|    approx_kl            | 0.1715094 |
|    clip_fraction        | 0.138     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.183    |
|    explained_variance   | 0.701     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.317     |
|    n_updates            | 12590     |
|    policy_gradient_loss | -0.0151   |
|    value_loss           | 1.31      |
---------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 284      |
|    time_elapsed    | 2350     |
|    total_timesteps | 581632   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 7.25        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 285         |
|    time_elapsed         | 2357        |
|    total_timesteps      | 583680      |
| train/                  |             |
|    approx_kl            | 0.045404106 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.619       |
|    n_updates            | 12600       |
|    policy_gradient_loss | -0.00628    |
|    value_loss           | 1.82        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.7       |
|    ep_rew_mean          | 7.14       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 286        |
|    time_elapsed         | 2365       |
|    total_timesteps      | 585728     |
| train/                  |            |
|    approx_kl            | 0.08675431 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.238      |
|    n_updates            | 12610      |
|    policy_gradient_loss | -0.00718   |
|    value_loss           | 1.38       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 6.73        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 287         |
|    time_elapsed         | 2375        |
|    total_timesteps      | 587776      |
| train/                  |             |
|    approx_kl            | 0.051370393 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.175      |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.409       |
|    n_updates            | 12620       |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 1.09        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 6.57        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 288         |
|    time_elapsed         | 2385        |
|    total_timesteps      | 589824      |
| train/                  |             |
|    approx_kl            | 0.028540611 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.34        |
|    n_updates            | 12630       |
|    policy_gradient_loss | -0.00583    |
|    value_loss           | 0.975       |
-----------------------------------------
Eval num_timesteps=590000, episode_reward=7.88 +/- 6.35
Episode length: 14.33 +/- 3.30
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.3        |
|    mean_reward          | 7.88        |
| time/                   |             |
|    total_timesteps      | 590000      |
| train/                  |             |
|    approx_kl            | 0.061364762 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.179      |
|    explained_variance   | 0.543       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.276       |
|    n_updates            | 12640       |
|    policy_gradient_loss | -0.0163     |
|    value_loss           | 1.4         |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17       |
|    ep_rew_mean     | 7.4      |
| time/              |          |
|    fps             | 247      |
|    iterations      | 289      |
|    time_elapsed    | 2395     |
|    total_timesteps | 591872   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.6       |
|    ep_rew_mean          | 7.31       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 290        |
|    time_elapsed         | 2405       |
|    total_timesteps      | 593920     |
| train/                  |            |
|    approx_kl            | 0.03995614 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.469      |
|    n_updates            | 12650      |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 1.01       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 6.83       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 291        |
|    time_elapsed         | 2413       |
|    total_timesteps      | 595968     |
| train/                  |            |
|    approx_kl            | 0.04870049 |
|    clip_fraction        | 0.123      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.191     |
|    explained_variance   | 0.779      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.106      |
|    n_updates            | 12660      |
|    policy_gradient_loss | -0.0124    |
|    value_loss           | 1.04       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.13        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 292         |
|    time_elapsed         | 2423        |
|    total_timesteps      | 598016      |
| train/                  |             |
|    approx_kl            | 0.050554022 |
|    clip_fraction        | 0.144       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.212      |
|    explained_variance   | 0.577       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.456       |
|    n_updates            | 12670       |
|    policy_gradient_loss | -0.0216     |
|    value_loss           | 1.26        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=600000, episode_reward=7.22 +/- 3.68
Episode length: 17.33 +/- 3.30
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.3       |
|    mean_reward          | 7.22       |
| time/                   |            |
|    total_timesteps      | 600000     |
| train/                  |            |
|    approx_kl            | 0.05654841 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.155     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.469      |
|    n_updates            | 12680      |
|    policy_gradient_loss | -0.00362   |
|    value_loss           | 1.32       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 7.24     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 293      |
|    time_elapsed    | 2434     |
|    total_timesteps | 600064   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 6.23        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 294         |
|    time_elapsed         | 2444        |
|    total_timesteps      | 602112      |
| train/                  |             |
|    approx_kl            | 0.048987653 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.178      |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.394       |
|    n_updates            | 12690       |
|    policy_gradient_loss | -0.0169     |
|    value_loss           | 1.32        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.9        |
|    ep_rew_mean          | 6.16        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 295         |
|    time_elapsed         | 2453        |
|    total_timesteps      | 604160      |
| train/                  |             |
|    approx_kl            | 0.044193216 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.191      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.96        |
|    n_updates            | 12700       |
|    policy_gradient_loss | -0.00604    |
|    value_loss           | 1.62        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.2       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 296        |
|    time_elapsed         | 2461       |
|    total_timesteps      | 606208     |
| train/                  |            |
|    approx_kl            | 0.07907541 |
|    clip_fraction        | 0.2        |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.221     |
|    explained_variance   | 0.546      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.482      |
|    n_updates            | 12710      |
|    policy_gradient_loss | -0.035     |
|    value_loss           | 1.92       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 6.62       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 297        |
|    time_elapsed         | 2468       |
|    total_timesteps      | 608256     |
| train/                  |            |
|    approx_kl            | 0.04922813 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.171     |
|    explained_variance   | 0.704      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.667      |
|    n_updates            | 12720      |
|    policy_gradient_loss | -0.00972   |
|    value_loss           | 1.83       |
----------------------------------------
reached max steps=100
Eval num_timesteps=610000, episode_reward=12.00 +/- 4.26
Episode length: 18.33 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 18.3        |
|    mean_reward          | 12          |
| time/                   |             |
|    total_timesteps      | 610000      |
| train/                  |             |
|    approx_kl            | 0.058953505 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.568       |
|    n_updates            | 12730       |
|    policy_gradient_loss | -0.00496    |
|    value_loss           | 1.13        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 6.82     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 298      |
|    time_elapsed    | 2476     |
|    total_timesteps | 610304   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 6.74       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 299        |
|    time_elapsed         | 2484       |
|    total_timesteps      | 612352     |
| train/                  |            |
|    approx_kl            | 0.06960842 |
|    clip_fraction        | 0.116      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.72       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.454      |
|    n_updates            | 12740      |
|    policy_gradient_loss | -0.00509   |
|    value_loss           | 1.4        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 7.31       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 300        |
|    time_elapsed         | 2492       |
|    total_timesteps      | 614400     |
| train/                  |            |
|    approx_kl            | 0.03746787 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.236     |
|    explained_variance   | 0.712      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.333      |
|    n_updates            | 12750      |
|    policy_gradient_loss | -0.0123    |
|    value_loss           | 1.44       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 7.31        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 301         |
|    time_elapsed         | 2499        |
|    total_timesteps      | 616448      |
| train/                  |             |
|    approx_kl            | 0.041343212 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.2        |
|    explained_variance   | 0.71        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.364       |
|    n_updates            | 12760       |
|    policy_gradient_loss | -0.00573    |
|    value_loss           | 1.38        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | 6.81       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 302        |
|    time_elapsed         | 2509       |
|    total_timesteps      | 618496     |
| train/                  |            |
|    approx_kl            | 0.03329289 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.198     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.433      |
|    n_updates            | 12770      |
|    policy_gradient_loss | -0.00625   |
|    value_loss           | 1.22       |
----------------------------------------
reached max steps=100
Eval num_timesteps=620000, episode_reward=5.01 +/- 10.61
Episode length: 27.33 +/- 16.05
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27.3       |
|    mean_reward          | 5.01       |
| time/                   |            |
|    total_timesteps      | 620000     |
| train/                  |            |
|    approx_kl            | 0.02657573 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.235     |
|    explained_variance   | 0.68       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.282      |
|    n_updates            | 12780      |
|    policy_gradient_loss | -0.00795   |
|    value_loss           | 1.4        |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 7.26     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 303      |
|    time_elapsed    | 2519     |
|    total_timesteps | 620544   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.2        |
|    ep_rew_mean          | 7.42        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 304         |
|    time_elapsed         | 2530        |
|    total_timesteps      | 622592      |
| train/                  |             |
|    approx_kl            | 0.027194478 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.223      |
|    explained_variance   | 0.778       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.294       |
|    n_updates            | 12790       |
|    policy_gradient_loss | -0.00757    |
|    value_loss           | 1.24        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | 8.02        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 305         |
|    time_elapsed         | 2539        |
|    total_timesteps      | 624640      |
| train/                  |             |
|    approx_kl            | 0.051277515 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.233      |
|    explained_variance   | 0.704       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.543       |
|    n_updates            | 12800       |
|    policy_gradient_loss | -0.00891    |
|    value_loss           | 1.75        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 7.19        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 306         |
|    time_elapsed         | 2546        |
|    total_timesteps      | 626688      |
| train/                  |             |
|    approx_kl            | 0.038216613 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.21       |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.18        |
|    n_updates            | 12810       |
|    policy_gradient_loss | -0.0145     |
|    value_loss           | 1.17        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 7.06        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 307         |
|    time_elapsed         | 2554        |
|    total_timesteps      | 628736      |
| train/                  |             |
|    approx_kl            | 0.032846723 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.197      |
|    explained_variance   | 0.777       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.33        |
|    n_updates            | 12820       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 1.02        |
-----------------------------------------
Eval num_timesteps=630000, episode_reward=10.63 +/- 2.82
Episode length: 17.00 +/- 2.16
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17         |
|    mean_reward          | 10.6       |
| time/                   |            |
|    total_timesteps      | 630000     |
| train/                  |            |
|    approx_kl            | 0.05496954 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.788      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.505      |
|    n_updates            | 12830      |
|    policy_gradient_loss | -0.0127    |
|    value_loss           | 1.19       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.8     |
|    ep_rew_mean     | 6.61     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 308      |
|    time_elapsed    | 2563     |
|    total_timesteps | 630784   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 7.32        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 309         |
|    time_elapsed         | 2571        |
|    total_timesteps      | 632832      |
| train/                  |             |
|    approx_kl            | 0.052152097 |
|    clip_fraction        | 0.115       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.692       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.456       |
|    n_updates            | 12840       |
|    policy_gradient_loss | -0.00653    |
|    value_loss           | 1.69        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 6.92        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 310         |
|    time_elapsed         | 2578        |
|    total_timesteps      | 634880      |
| train/                  |             |
|    approx_kl            | 0.027421342 |
|    clip_fraction        | 0.0901      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.146      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.384       |
|    n_updates            | 12850       |
|    policy_gradient_loss | -0.00535    |
|    value_loss           | 1.45        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 7.73        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 311         |
|    time_elapsed         | 2585        |
|    total_timesteps      | 636928      |
| train/                  |             |
|    approx_kl            | 0.027842805 |
|    clip_fraction        | 0.0754      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.406       |
|    n_updates            | 12860       |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 1.22        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 6.85        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 312         |
|    time_elapsed         | 2592        |
|    total_timesteps      | 638976      |
| train/                  |             |
|    approx_kl            | 0.028104557 |
|    clip_fraction        | 0.0923      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.161      |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.229       |
|    n_updates            | 12870       |
|    policy_gradient_loss | -0.00561    |
|    value_loss           | 1.36        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=640000, episode_reward=0.01 +/- 7.20
Episode length: 27.33 +/- 16.36
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.3        |
|    mean_reward          | 0.008       |
| time/                   |             |
|    total_timesteps      | 640000      |
| train/                  |             |
|    approx_kl            | 0.062169317 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.16       |
|    explained_variance   | 0.74        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.524       |
|    n_updates            | 12880       |
|    policy_gradient_loss | -0.00931    |
|    value_loss           | 1.44        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 7.52     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 313      |
|    time_elapsed    | 2599     |
|    total_timesteps | 641024   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 314        |
|    time_elapsed         | 2606       |
|    total_timesteps      | 643072     |
| train/                  |            |
|    approx_kl            | 0.05617621 |
|    clip_fraction        | 0.112      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.138     |
|    explained_variance   | 0.717      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.674      |
|    n_updates            | 12890      |
|    policy_gradient_loss | -0.00759   |
|    value_loss           | 1.44       |
----------------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 18.1      |
|    ep_rew_mean          | 6.73      |
| time/                   |           |
|    fps                  | 246       |
|    iterations           | 315       |
|    time_elapsed         | 2613      |
|    total_timesteps      | 645120    |
| train/                  |           |
|    approx_kl            | 0.0359819 |
|    clip_fraction        | 0.089     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.131    |
|    explained_variance   | 0.746     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.335     |
|    n_updates            | 12900     |
|    policy_gradient_loss | -0.000461 |
|    value_loss           | 1.4       |
---------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 6.85        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 316         |
|    time_elapsed         | 2621        |
|    total_timesteps      | 647168      |
| train/                  |             |
|    approx_kl            | 0.042319737 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.272       |
|    n_updates            | 12910       |
|    policy_gradient_loss | -0.0128     |
|    value_loss           | 1.29        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 7.3         |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 317         |
|    time_elapsed         | 2629        |
|    total_timesteps      | 649216      |
| train/                  |             |
|    approx_kl            | 0.046602614 |
|    clip_fraction        | 0.107       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.155      |
|    explained_variance   | 0.734       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.343       |
|    n_updates            | 12920       |
|    policy_gradient_loss | -0.0149     |
|    value_loss           | 1.31        |
-----------------------------------------
Eval num_timesteps=650000, episode_reward=11.86 +/- 3.31
Episode length: 19.00 +/- 4.24
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19          |
|    mean_reward          | 11.9        |
| time/                   |             |
|    total_timesteps      | 650000      |
| train/                  |             |
|    approx_kl            | 0.062362634 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.138      |
|    explained_variance   | 0.561       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.318       |
|    n_updates            | 12930       |
|    policy_gradient_loss | -0.0195     |
|    value_loss           | 1.38        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.7     |
|    ep_rew_mean     | 7.45     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 318      |
|    time_elapsed    | 2636     |
|    total_timesteps | 651264   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.3       |
|    ep_rew_mean          | 7.41       |
| time/                   |            |
|    fps                  | 247        |
|    iterations           | 319        |
|    time_elapsed         | 2644       |
|    total_timesteps      | 653312     |
| train/                  |            |
|    approx_kl            | 0.06800603 |
|    clip_fraction        | 0.114      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.135     |
|    explained_variance   | 0.629      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.27       |
|    n_updates            | 12940      |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 1.32       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 7.94        |
| time/                   |             |
|    fps                  | 247         |
|    iterations           | 320         |
|    time_elapsed         | 2653        |
|    total_timesteps      | 655360      |
| train/                  |             |
|    approx_kl            | 0.054600015 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.133      |
|    explained_variance   | 0.68        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.413       |
|    n_updates            | 12950       |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 1.1         |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.7        |
|    ep_rew_mean          | 6.8         |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 321         |
|    time_elapsed         | 2661        |
|    total_timesteps      | 657408      |
| train/                  |             |
|    approx_kl            | 0.047404137 |
|    clip_fraction        | 0.0933      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.118      |
|    explained_variance   | 0.771       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.411       |
|    n_updates            | 12960       |
|    policy_gradient_loss | -0.00201    |
|    value_loss           | 1.36        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 6.99       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 322        |
|    time_elapsed         | 2670       |
|    total_timesteps      | 659456     |
| train/                  |            |
|    approx_kl            | 0.09838503 |
|    clip_fraction        | 0.14       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.16      |
|    explained_variance   | 0.497      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.337      |
|    n_updates            | 12970      |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 1.28       |
----------------------------------------
Eval num_timesteps=660000, episode_reward=6.06 +/- 1.91
Episode length: 15.00 +/- 2.45
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 6.06        |
| time/                   |             |
|    total_timesteps      | 660000      |
| train/                  |             |
|    approx_kl            | 0.070011616 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.702       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.36        |
|    n_updates            | 12980       |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 1.19        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.1     |
|    ep_rew_mean     | 7.97     |
| time/              |          |
|    fps             | 246      |
|    iterations      | 323      |
|    time_elapsed    | 2681     |
|    total_timesteps | 661504   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 8.34        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 324         |
|    time_elapsed         | 2689        |
|    total_timesteps      | 663552      |
| train/                  |             |
|    approx_kl            | 0.046457097 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.131      |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.253       |
|    n_updates            | 12990       |
|    policy_gradient_loss | -0.00625    |
|    value_loss           | 1.2         |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.2       |
|    ep_rew_mean          | 7.35       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 325        |
|    time_elapsed         | 2699       |
|    total_timesteps      | 665600     |
| train/                  |            |
|    approx_kl            | 0.07576804 |
|    clip_fraction        | 0.0872     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.108     |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.238      |
|    n_updates            | 13000      |
|    policy_gradient_loss | -0.00334   |
|    value_loss           | 1.21       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.5       |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 326        |
|    time_elapsed         | 2708       |
|    total_timesteps      | 667648     |
| train/                  |            |
|    approx_kl            | 0.06915616 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.338      |
|    n_updates            | 13010      |
|    policy_gradient_loss | -0.00649   |
|    value_loss           | 1.39       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 6.66       |
| time/                   |            |
|    fps                  | 246        |
|    iterations           | 327        |
|    time_elapsed         | 2716       |
|    total_timesteps      | 669696     |
| train/                  |            |
|    approx_kl            | 0.05366324 |
|    clip_fraction        | 0.0945     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.748      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.364      |
|    n_updates            | 13020      |
|    policy_gradient_loss | -0.0059    |
|    value_loss           | 1.36       |
----------------------------------------
reached max steps=100
Eval num_timesteps=670000, episode_reward=4.76 +/- 2.10
Episode length: 13.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.3        |
|    mean_reward          | 4.76        |
| time/                   |             |
|    total_timesteps      | 670000      |
| train/                  |             |
|    approx_kl            | 0.036733456 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.657       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.364       |
|    n_updates            | 13030       |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 1.48        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 7.3      |
| time/              |          |
|    fps             | 246      |
|    iterations      | 328      |
|    time_elapsed    | 2726     |
|    total_timesteps | 671744   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.7        |
|    ep_rew_mean          | 7.85        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 329         |
|    time_elapsed         | 2734        |
|    total_timesteps      | 673792      |
| train/                  |             |
|    approx_kl            | 0.040530078 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.812       |
|    n_updates            | 13040       |
|    policy_gradient_loss | -0.00678    |
|    value_loss           | 1.6         |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.4        |
|    ep_rew_mean          | 6.93        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 330         |
|    time_elapsed         | 2746        |
|    total_timesteps      | 675840      |
| train/                  |             |
|    approx_kl            | 0.047609128 |
|    clip_fraction        | 0.0694      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0979     |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.217       |
|    n_updates            | 13050       |
|    policy_gradient_loss | -0.00378    |
|    value_loss           | 0.867       |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 6.91        |
| time/                   |             |
|    fps                  | 246         |
|    iterations           | 331         |
|    time_elapsed         | 2755        |
|    total_timesteps      | 677888      |
| train/                  |             |
|    approx_kl            | 0.041545272 |
|    clip_fraction        | 0.129       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.49        |
|    n_updates            | 13060       |
|    policy_gradient_loss | -0.0066     |
|    value_loss           | 1.07        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 7.02        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 332         |
|    time_elapsed         | 2765        |
|    total_timesteps      | 679936      |
| train/                  |             |
|    approx_kl            | 0.061524503 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.728       |
|    n_updates            | 13070       |
|    policy_gradient_loss | -0.00797    |
|    value_loss           | 1.52        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=680000, episode_reward=1.82 +/- 9.15
Episode length: 26.67 +/- 16.58
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.7        |
|    mean_reward          | 1.82        |
| time/                   |             |
|    total_timesteps      | 680000      |
| train/                  |             |
|    approx_kl            | 0.039275944 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.151      |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.625       |
|    n_updates            | 13080       |
|    policy_gradient_loss | -0.00283    |
|    value_loss           | 1.45        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.1     |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 245      |
|    iterations      | 333      |
|    time_elapsed    | 2775     |
|    total_timesteps | 681984   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.7        |
|    ep_rew_mean          | 7.49        |
| time/                   |             |
|    fps                  | 245         |
|    iterations           | 334         |
|    time_elapsed         | 2787        |
|    total_timesteps      | 684032      |
| train/                  |             |
|    approx_kl            | 0.045308914 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.147      |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.425       |
|    n_updates            | 13090       |
|    policy_gradient_loss | 0.00181     |
|    value_loss           | 1.33        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.9        |
|    ep_rew_mean          | 7.05        |
| time/                   |             |
|    fps                  | 244         |
|    iterations           | 335         |
|    time_elapsed         | 2800        |
|    total_timesteps      | 686080      |
| train/                  |             |
|    approx_kl            | 0.021136953 |
|    clip_fraction        | 0.101       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.775       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.501       |
|    n_updates            | 13100       |
|    policy_gradient_loss | -0.00846    |
|    value_loss           | 0.889       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.6       |
|    ep_rew_mean          | 7.45       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 336        |
|    time_elapsed         | 2814       |
|    total_timesteps      | 688128     |
| train/                  |            |
|    approx_kl            | 0.05079993 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.799      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.363      |
|    n_updates            | 13110      |
|    policy_gradient_loss | -0.0138    |
|    value_loss           | 0.802      |
----------------------------------------
reached max steps=100
Eval num_timesteps=690000, episode_reward=3.41 +/- 6.97
Episode length: 27.00 +/- 16.39
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27         |
|    mean_reward          | 3.41       |
| time/                   |            |
|    total_timesteps      | 690000     |
| train/                  |            |
|    approx_kl            | 0.04814089 |
|    clip_fraction        | 0.0883     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.381      |
|    n_updates            | 13120      |
|    policy_gradient_loss | -0.0059    |
|    value_loss           | 1.35       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 7.99     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 337      |
|    time_elapsed    | 2823     |
|    total_timesteps | 690176   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.9       |
|    ep_rew_mean          | 7.24       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 338        |
|    time_elapsed         | 2831       |
|    total_timesteps      | 692224     |
| train/                  |            |
|    approx_kl            | 0.06310043 |
|    clip_fraction        | 0.0993     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.123     |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.855      |
|    n_updates            | 13130      |
|    policy_gradient_loss | -0.00499   |
|    value_loss           | 1.4        |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.5       |
|    ep_rew_mean          | 7.82       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 339        |
|    time_elapsed         | 2840       |
|    total_timesteps      | 694272     |
| train/                  |            |
|    approx_kl            | 0.13811713 |
|    clip_fraction        | 0.0854     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.214      |
|    n_updates            | 13140      |
|    policy_gradient_loss | -0.0042    |
|    value_loss           | 1.09       |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 17.6      |
|    ep_rew_mean          | 7.39      |
| time/                   |           |
|    fps                  | 244       |
|    iterations           | 340       |
|    time_elapsed         | 2850      |
|    total_timesteps      | 696320    |
| train/                  |           |
|    approx_kl            | 0.0373403 |
|    clip_fraction        | 0.0732    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.112    |
|    explained_variance   | 0.791     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.504     |
|    n_updates            | 13150     |
|    policy_gradient_loss | -0.0042   |
|    value_loss           | 1.32      |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.1       |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 341        |
|    time_elapsed         | 2859       |
|    total_timesteps      | 698368     |
| train/                  |            |
|    approx_kl            | 0.04780298 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.674      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.251      |
|    n_updates            | 13160      |
|    policy_gradient_loss | -0.0149    |
|    value_loss           | 1.47       |
----------------------------------------
reached max steps=100
Eval num_timesteps=700000, episode_reward=5.71 +/- 3.39
Episode length: 19.67 +/- 4.50
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.7        |
|    mean_reward          | 5.71        |
| time/                   |             |
|    total_timesteps      | 700000      |
| train/                  |             |
|    approx_kl            | 0.045846604 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.159      |
|    explained_variance   | 0.782       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.303       |
|    n_updates            | 13170       |
|    policy_gradient_loss | -0.0093     |
|    value_loss           | 1.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 7.76     |
| time/              |          |
|    fps             | 244      |
|    iterations      | 342      |
|    time_elapsed    | 2868     |
|    total_timesteps | 700416   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 7.95       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 343        |
|    time_elapsed         | 2877       |
|    total_timesteps      | 702464     |
| train/                  |            |
|    approx_kl            | 0.04285911 |
|    clip_fraction        | 0.0796     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.126     |
|    explained_variance   | 0.78       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.659      |
|    n_updates            | 13180      |
|    policy_gradient_loss | -0.00686   |
|    value_loss           | 1.19       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.1       |
|    ep_rew_mean          | 6.65       |
| time/                   |            |
|    fps                  | 244        |
|    iterations           | 344        |
|    time_elapsed         | 2886       |
|    total_timesteps      | 704512     |
| train/                  |            |
|    approx_kl            | 0.04930961 |
|    clip_fraction        | 0.0965     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.131     |
|    explained_variance   | 0.728      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.6        |
|    n_updates            | 13190      |
|    policy_gradient_loss | -0.00815   |
|    value_loss           | 1.44       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.3        |
|    ep_rew_mean          | 7.61        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 345         |
|    time_elapsed         | 2897        |
|    total_timesteps      | 706560      |
| train/                  |             |
|    approx_kl            | 0.038728435 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.163      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.41        |
|    n_updates            | 13200       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 1.31        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.27        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 346         |
|    time_elapsed         | 2905        |
|    total_timesteps      | 708608      |
| train/                  |             |
|    approx_kl            | 0.052743196 |
|    clip_fraction        | 0.0883      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.709       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.521       |
|    n_updates            | 13210       |
|    policy_gradient_loss | -0.0148     |
|    value_loss           | 1.23        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=710000, episode_reward=7.51 +/- 3.38
Episode length: 16.00 +/- 3.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16         |
|    mean_reward          | 7.51       |
| time/                   |            |
|    total_timesteps      | 710000     |
| train/                  |            |
|    approx_kl            | 0.05389642 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.143     |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.317      |
|    n_updates            | 13220      |
|    policy_gradient_loss | -0.00829   |
|    value_loss           | 1.29       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 6.53     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 347      |
|    time_elapsed    | 2913     |
|    total_timesteps | 710656   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.29        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 348         |
|    time_elapsed         | 2921        |
|    total_timesteps      | 712704      |
| train/                  |             |
|    approx_kl            | 0.051630124 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.199       |
|    n_updates            | 13230       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 0.937       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.8       |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 349        |
|    time_elapsed         | 2929       |
|    total_timesteps      | 714752     |
| train/                  |            |
|    approx_kl            | 0.07967062 |
|    clip_fraction        | 0.103      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.152     |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.445      |
|    n_updates            | 13240      |
|    policy_gradient_loss | -0.0101    |
|    value_loss           | 1.21       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 6.29        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 350         |
|    time_elapsed         | 2941        |
|    total_timesteps      | 716800      |
| train/                  |             |
|    approx_kl            | 0.022672802 |
|    clip_fraction        | 0.0785      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.806       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.306       |
|    n_updates            | 13250       |
|    policy_gradient_loss | -0.00735    |
|    value_loss           | 1.05        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | 6.51        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 351         |
|    time_elapsed         | 2950        |
|    total_timesteps      | 718848      |
| train/                  |             |
|    approx_kl            | 0.065234706 |
|    clip_fraction        | 0.151       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.18       |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.367       |
|    n_updates            | 13260       |
|    policy_gradient_loss | 0.00782     |
|    value_loss           | 1.88        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=720000, episode_reward=5.04 +/- 1.33
Episode length: 27.33 +/- 13.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27.3        |
|    mean_reward          | 5.04        |
| time/                   |             |
|    total_timesteps      | 720000      |
| train/                  |             |
|    approx_kl            | 0.049375005 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.196      |
|    explained_variance   | 0.652       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.539       |
|    n_updates            | 13270       |
|    policy_gradient_loss | -0.0175     |
|    value_loss           | 1.24        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | 6.89     |
| time/              |          |
|    fps             | 243      |
|    iterations      | 352      |
|    time_elapsed    | 2958     |
|    total_timesteps | 720896   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.3       |
|    ep_rew_mean          | 8.49       |
| time/                   |            |
|    fps                  | 243        |
|    iterations           | 353        |
|    time_elapsed         | 2969       |
|    total_timesteps      | 722944     |
| train/                  |            |
|    approx_kl            | 0.04431788 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.181     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.29       |
|    n_updates            | 13280      |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 1.1        |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 6.08        |
| time/                   |             |
|    fps                  | 243         |
|    iterations           | 354         |
|    time_elapsed         | 2983        |
|    total_timesteps      | 724992      |
| train/                  |             |
|    approx_kl            | 0.030385338 |
|    clip_fraction        | 0.0863      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.672       |
|    n_updates            | 13290       |
|    policy_gradient_loss | -0.00575    |
|    value_loss           | 1.43        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | 6.46        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 355         |
|    time_elapsed         | 2997        |
|    total_timesteps      | 727040      |
| train/                  |             |
|    approx_kl            | 0.056350477 |
|    clip_fraction        | 0.135       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.192      |
|    explained_variance   | 0.757       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.289       |
|    n_updates            | 13300       |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.926       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 7.01        |
| time/                   |             |
|    fps                  | 242         |
|    iterations           | 356         |
|    time_elapsed         | 3008        |
|    total_timesteps      | 729088      |
| train/                  |             |
|    approx_kl            | 0.057780206 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.187      |
|    explained_variance   | 0.764       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.722       |
|    n_updates            | 13310       |
|    policy_gradient_loss | -0.00524    |
|    value_loss           | 1.24        |
-----------------------------------------
Eval num_timesteps=730000, episode_reward=3.96 +/- 1.82
Episode length: 17.00 +/- 3.74
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 17          |
|    mean_reward          | 3.96        |
| time/                   |             |
|    total_timesteps      | 730000      |
| train/                  |             |
|    approx_kl            | 0.030689124 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.667       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.217       |
|    n_updates            | 13320       |
|    policy_gradient_loss | -0.013      |
|    value_loss           | 1.28        |
-----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | 6.55     |
| time/              |          |
|    fps             | 242      |
|    iterations      | 357      |
|    time_elapsed    | 3021     |
|    total_timesteps | 731136   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.3        |
|    ep_rew_mean          | 7.7         |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 358         |
|    time_elapsed         | 3031        |
|    total_timesteps      | 733184      |
| train/                  |             |
|    approx_kl            | 0.062095672 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.162      |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.492       |
|    n_updates            | 13330       |
|    policy_gradient_loss | -0.0023     |
|    value_loss           | 1.27        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 359         |
|    time_elapsed         | 3043        |
|    total_timesteps      | 735232      |
| train/                  |             |
|    approx_kl            | 0.044915423 |
|    clip_fraction        | 0.0989      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.744       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.44        |
|    n_updates            | 13340       |
|    policy_gradient_loss | -0.00506    |
|    value_loss           | 1.29        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17          |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 360         |
|    time_elapsed         | 3053        |
|    total_timesteps      | 737280      |
| train/                  |             |
|    approx_kl            | 0.045727257 |
|    clip_fraction        | 0.0953      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.398       |
|    n_updates            | 13350       |
|    policy_gradient_loss | -0.00641    |
|    value_loss           | 1.1         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 7.35        |
| time/                   |             |
|    fps                  | 241         |
|    iterations           | 361         |
|    time_elapsed         | 3065        |
|    total_timesteps      | 739328      |
| train/                  |             |
|    approx_kl            | 0.027615882 |
|    clip_fraction        | 0.0824      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.292       |
|    n_updates            | 13360       |
|    policy_gradient_loss | -0.00715    |
|    value_loss           | 0.821       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=740000, episode_reward=3.56 +/- 11.14
Episode length: 26.33 +/- 16.86
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26.3       |
|    mean_reward          | 3.56       |
| time/                   |            |
|    total_timesteps      | 740000     |
| train/                  |            |
|    approx_kl            | 0.05322385 |
|    clip_fraction        | 0.0953     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.145     |
|    explained_variance   | 0.797      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.559      |
|    n_updates            | 13370      |
|    policy_gradient_loss | -0.00561   |
|    value_loss           | 1.04       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.2     |
|    ep_rew_mean     | 8.13     |
| time/              |          |
|    fps             | 240      |
|    iterations      | 362      |
|    time_elapsed    | 3077     |
|    total_timesteps | 741376   |
---------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 17.3      |
|    ep_rew_mean          | 7.12      |
| time/                   |           |
|    fps                  | 240       |
|    iterations           | 363       |
|    time_elapsed         | 3087      |
|    total_timesteps      | 743424    |
| train/                  |           |
|    approx_kl            | 0.1249271 |
|    clip_fraction        | 0.117     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.158    |
|    explained_variance   | 0.779     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.451     |
|    n_updates            | 13380     |
|    policy_gradient_loss | -0.00355  |
|    value_loss           | 1.44      |
---------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 6.99       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 364        |
|    time_elapsed         | 3099       |
|    total_timesteps      | 745472     |
| train/                  |            |
|    approx_kl            | 0.03342634 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.419      |
|    n_updates            | 13390      |
|    policy_gradient_loss | -0.00426   |
|    value_loss           | 1.26       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.7       |
|    ep_rew_mean          | 7.12       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 365        |
|    time_elapsed         | 3111       |
|    total_timesteps      | 747520     |
| train/                  |            |
|    approx_kl            | 0.05548688 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.161     |
|    explained_variance   | 0.767      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.921      |
|    n_updates            | 13400      |
|    policy_gradient_loss | -0.012     |
|    value_loss           | 1.41       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 7.17       |
| time/                   |            |
|    fps                  | 240        |
|    iterations           | 366        |
|    time_elapsed         | 3121       |
|    total_timesteps      | 749568     |
| train/                  |            |
|    approx_kl            | 0.16316622 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.463      |
|    n_updates            | 13410      |
|    policy_gradient_loss | -0.0155    |
|    value_loss           | 1.63       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=750000, episode_reward=-0.02 +/- 8.14
Episode length: 38.00 +/- 16.97
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 38          |
|    mean_reward          | -0.0173     |
| time/                   |             |
|    total_timesteps      | 750000      |
| train/                  |             |
|    approx_kl            | 0.061668918 |
|    clip_fraction        | 0.125       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.183      |
|    explained_variance   | 0.739       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.462       |
|    n_updates            | 13420       |
|    policy_gradient_loss | -0.00758    |
|    value_loss           | 1.2         |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.5     |
|    ep_rew_mean     | 6.87     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 367      |
|    time_elapsed    | 3133     |
|    total_timesteps | 751616   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.9        |
|    ep_rew_mean          | 7.48        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 368         |
|    time_elapsed         | 3141        |
|    total_timesteps      | 753664      |
| train/                  |             |
|    approx_kl            | 0.055450924 |
|    clip_fraction        | 0.14        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.201      |
|    explained_variance   | 0.548       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.579       |
|    n_updates            | 13430       |
|    policy_gradient_loss | -0.0187     |
|    value_loss           | 1.53        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.7       |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 369        |
|    time_elapsed         | 3150       |
|    total_timesteps      | 755712     |
| train/                  |            |
|    approx_kl            | 0.11321694 |
|    clip_fraction        | 0.135      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.177     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.451      |
|    n_updates            | 13440      |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 1.16       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 6.92        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 370         |
|    time_elapsed         | 3159        |
|    total_timesteps      | 757760      |
| train/                  |             |
|    approx_kl            | 0.035908233 |
|    clip_fraction        | 0.0946      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.173      |
|    explained_variance   | 0.769       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.203       |
|    n_updates            | 13450       |
|    policy_gradient_loss | -0.00993    |
|    value_loss           | 1.06        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.6       |
|    ep_rew_mean          | 6.6        |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 371        |
|    time_elapsed         | 3172       |
|    total_timesteps      | 759808     |
| train/                  |            |
|    approx_kl            | 0.05104076 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.181     |
|    explained_variance   | 0.766      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.183      |
|    n_updates            | 13460      |
|    policy_gradient_loss | -0.00848   |
|    value_loss           | 0.885      |
----------------------------------------
reached max steps=100
Eval num_timesteps=760000, episode_reward=4.79 +/- 10.46
Episode length: 28.33 +/- 15.33
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 28.3       |
|    mean_reward          | 4.79       |
| time/                   |            |
|    total_timesteps      | 760000     |
| train/                  |            |
|    approx_kl            | 0.03720296 |
|    clip_fraction        | 0.107      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.186     |
|    explained_variance   | 0.751      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.456      |
|    n_updates            | 13470      |
|    policy_gradient_loss | -0.00702   |
|    value_loss           | 0.994      |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 6.91     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 372      |
|    time_elapsed    | 3181     |
|    total_timesteps | 761856   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.6        |
|    ep_rew_mean          | 7.45        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 373         |
|    time_elapsed         | 3190        |
|    total_timesteps      | 763904      |
| train/                  |             |
|    approx_kl            | 0.054010674 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.197      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.223       |
|    n_updates            | 13480       |
|    policy_gradient_loss | -0.00555    |
|    value_loss           | 1.12        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.3        |
|    ep_rew_mean          | 6.91        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 374         |
|    time_elapsed         | 3199        |
|    total_timesteps      | 765952      |
| train/                  |             |
|    approx_kl            | 0.049009465 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.188      |
|    explained_variance   | 0.785       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.73        |
|    n_updates            | 13490       |
|    policy_gradient_loss | -0.0043     |
|    value_loss           | 1.35        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.9        |
|    ep_rew_mean          | 6.72        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 375         |
|    time_elapsed         | 3209        |
|    total_timesteps      | 768000      |
| train/                  |             |
|    approx_kl            | 0.038243644 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.186      |
|    explained_variance   | 0.722       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.289       |
|    n_updates            | 13500       |
|    policy_gradient_loss | -0.00464    |
|    value_loss           | 1.16        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=770000, episode_reward=7.44 +/- 4.00
Episode length: 16.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 16.3       |
|    mean_reward          | 7.44       |
| time/                   |            |
|    total_timesteps      | 770000     |
| train/                  |            |
|    approx_kl            | 0.06476992 |
|    clip_fraction        | 0.136      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.236     |
|    explained_variance   | 0.673      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.755      |
|    n_updates            | 13510      |
|    policy_gradient_loss | -0.0104    |
|    value_loss           | 1.82       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 20.5     |
|    ep_rew_mean     | 6.87     |
| time/              |          |
|    fps             | 239      |
|    iterations      | 376      |
|    time_elapsed    | 3218     |
|    total_timesteps | 770048   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19         |
|    ep_rew_mean          | 6.77       |
| time/                   |            |
|    fps                  | 239        |
|    iterations           | 377        |
|    time_elapsed         | 3227       |
|    total_timesteps      | 772096     |
| train/                  |            |
|    approx_kl            | 0.03998676 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.196     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.768      |
|    n_updates            | 13520      |
|    policy_gradient_loss | -0.00433   |
|    value_loss           | 1.44       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 20.1        |
|    ep_rew_mean          | 7.31        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 378         |
|    time_elapsed         | 3235        |
|    total_timesteps      | 774144      |
| train/                  |             |
|    approx_kl            | 0.075643696 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.189      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.546       |
|    n_updates            | 13530       |
|    policy_gradient_loss | -0.00383    |
|    value_loss           | 1.56        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | 7.37        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 379         |
|    time_elapsed         | 3245        |
|    total_timesteps      | 776192      |
| train/                  |             |
|    approx_kl            | 0.066554114 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.206      |
|    explained_variance   | 0.686       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.468       |
|    n_updates            | 13540       |
|    policy_gradient_loss | -0.00675    |
|    value_loss           | 1.71        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.5        |
|    ep_rew_mean          | 7.51        |
| time/                   |             |
|    fps                  | 239         |
|    iterations           | 380         |
|    time_elapsed         | 3253        |
|    total_timesteps      | 778240      |
| train/                  |             |
|    approx_kl            | 0.019294148 |
|    clip_fraction        | 0.083       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.152      |
|    explained_variance   | 0.743       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.183       |
|    n_updates            | 13550       |
|    policy_gradient_loss | -0.00313    |
|    value_loss           | 1.18        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=780000, episode_reward=1.53 +/- 8.15
Episode length: 28.00 +/- 15.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 28        |
|    mean_reward          | 1.53      |
| time/                   |           |
|    total_timesteps      | 780000    |
| train/                  |           |
|    approx_kl            | 0.0485558 |
|    clip_fraction        | 0.11      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.212    |
|    explained_variance   | 0.701     |
|    learning_rate        | 0.0003    |
|    loss                 | 1.51      |
|    n_updates            | 13560     |
|    policy_gradient_loss | -0.00519  |
|    value_loss           | 1.82      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.4     |
|    ep_rew_mean     | 6.68     |
| time/              |          |
|    fps             | 238      |
|    iterations      | 381      |
|    time_elapsed    | 3265     |
|    total_timesteps | 780288   |
---------------------------------
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 19.7      |
|    ep_rew_mean          | 6.45      |
| time/                   |           |
|    fps                  | 238       |
|    iterations           | 382       |
|    time_elapsed         | 3277      |
|    total_timesteps      | 782336    |
| train/                  |           |
|    approx_kl            | 0.1113308 |
|    clip_fraction        | 0.11      |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.23     |
|    explained_variance   | 0.73      |
|    learning_rate        | 0.0003    |
|    loss                 | 1.05      |
|    n_updates            | 13570     |
|    policy_gradient_loss | -0.00911  |
|    value_loss           | 1.96      |
---------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 6.53       |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 383        |
|    time_elapsed         | 3288       |
|    total_timesteps      | 784384     |
| train/                  |            |
|    approx_kl            | 0.06135919 |
|    clip_fraction        | 0.152      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.242     |
|    explained_variance   | 0.658      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.274      |
|    n_updates            | 13580      |
|    policy_gradient_loss | -0.00525   |
|    value_loss           | 1.5        |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | 6.7        |
| time/                   |            |
|    fps                  | 238        |
|    iterations           | 384        |
|    time_elapsed         | 3298       |
|    total_timesteps      | 786432     |
| train/                  |            |
|    approx_kl            | 0.07425877 |
|    clip_fraction        | 0.137      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.241     |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.424      |
|    n_updates            | 13590      |
|    policy_gradient_loss | -0.00989   |
|    value_loss           | 1.39       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 238         |
|    iterations           | 385         |
|    time_elapsed         | 3311        |
|    total_timesteps      | 788480      |
| train/                  |             |
|    approx_kl            | 0.036813557 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.22       |
|    explained_variance   | 0.779       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.285       |
|    n_updates            | 13600       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 1.51        |
-----------------------------------------
Eval num_timesteps=790000, episode_reward=11.13 +/- 2.11
Episode length: 22.33 +/- 10.50
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 22.3       |
|    mean_reward          | 11.1       |
| time/                   |            |
|    total_timesteps      | 790000     |
| train/                  |            |
|    approx_kl            | 0.04596516 |
|    clip_fraction        | 0.11       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.21      |
|    explained_variance   | 0.762      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.363      |
|    n_updates            | 13610      |
|    policy_gradient_loss | -0.00677   |
|    value_loss           | 1.35       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19       |
|    ep_rew_mean     | 6.75     |
| time/              |          |
|    fps             | 237      |
|    iterations      | 386      |
|    time_elapsed    | 3323     |
|    total_timesteps | 790528   |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 7.1        |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 387        |
|    time_elapsed         | 3334       |
|    total_timesteps      | 792576     |
| train/                  |            |
|    approx_kl            | 0.06946505 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.23      |
|    explained_variance   | 0.776      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.349      |
|    n_updates            | 13620      |
|    policy_gradient_loss | -0.013     |
|    value_loss           | 1.17       |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.5       |
|    ep_rew_mean          | 7.37       |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 388        |
|    time_elapsed         | 3344       |
|    total_timesteps      | 794624     |
| train/                  |            |
|    approx_kl            | 0.14594805 |
|    clip_fraction        | 0.0858     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.178     |
|    explained_variance   | 0.747      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.538      |
|    n_updates            | 13630      |
|    policy_gradient_loss | -0.00782   |
|    value_loss           | 1.39       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.9       |
|    ep_rew_mean          | 7.42       |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 389        |
|    time_elapsed         | 3356       |
|    total_timesteps      | 796672     |
| train/                  |            |
|    approx_kl            | 0.04604746 |
|    clip_fraction        | 0.105      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.169     |
|    explained_variance   | 0.785      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.551      |
|    n_updates            | 13640      |
|    policy_gradient_loss | -0.00469   |
|    value_loss           | 1.25       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.3       |
|    ep_rew_mean          | 7.57       |
| time/                   |            |
|    fps                  | 237        |
|    iterations           | 390        |
|    time_elapsed         | 3367       |
|    total_timesteps      | 798720     |
| train/                  |            |
|    approx_kl            | 0.06197708 |
|    clip_fraction        | 0.124      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.158     |
|    explained_variance   | 0.708      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.71       |
|    n_updates            | 13650      |
|    policy_gradient_loss | -0.0113    |
|    value_loss           | 1.57       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=800000, episode_reward=9.79 +/- 4.56
Episode length: 19.33 +/- 5.73
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 19.3        |
|    mean_reward          | 9.79        |
| time/                   |             |
|    total_timesteps      | 800000      |
| train/                  |             |
|    approx_kl            | 0.040659532 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.139      |
|    explained_variance   | 0.682       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.32        |
|    n_updates            | 13660       |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 1.23        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.4     |
|    ep_rew_mean     | 7.15     |
| time/              |          |
|    fps             | 236      |
|    iterations      | 391      |
|    time_elapsed    | 3383     |
|    total_timesteps | 800768   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 6.92       |
| time/                   |            |
|    fps                  | 236        |
|    iterations           | 392        |
|    time_elapsed         | 3395       |
|    total_timesteps      | 802816     |
| train/                  |            |
|    approx_kl            | 0.02672811 |
|    clip_fraction        | 0.0677     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.77       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.289      |
|    n_updates            | 13670      |
|    policy_gradient_loss | -0.00429   |
|    value_loss           | 1.15       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19          |
|    ep_rew_mean          | 7.35        |
| time/                   |             |
|    fps                  | 236         |
|    iterations           | 393         |
|    time_elapsed         | 3408        |
|    total_timesteps      | 804864      |
| train/                  |             |
|    approx_kl            | 0.112093054 |
|    clip_fraction        | 0.13        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.185      |
|    explained_variance   | 0.73        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.141       |
|    n_updates            | 13680       |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 1.36        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 235         |
|    iterations           | 394         |
|    time_elapsed         | 3422        |
|    total_timesteps      | 806912      |
| train/                  |             |
|    approx_kl            | 0.066668734 |
|    clip_fraction        | 0.126       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.153      |
|    explained_variance   | 0.636       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.481       |
|    n_updates            | 13690       |
|    policy_gradient_loss | -0.00844    |
|    value_loss           | 1.47        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.3       |
|    ep_rew_mean          | 7.77       |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 395        |
|    time_elapsed         | 3433       |
|    total_timesteps      | 808960     |
| train/                  |            |
|    approx_kl            | 0.05809458 |
|    clip_fraction        | 0.106      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.129     |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.468      |
|    n_updates            | 13700      |
|    policy_gradient_loss | -0.00909   |
|    value_loss           | 1.14       |
----------------------------------------
Eval num_timesteps=810000, episode_reward=5.70 +/- 2.45
Episode length: 16.67 +/- 2.05
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 16.7        |
|    mean_reward          | 5.7         |
| time/                   |             |
|    total_timesteps      | 810000      |
| train/                  |             |
|    approx_kl            | 0.036857765 |
|    clip_fraction        | 0.0925      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.126      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.419       |
|    n_updates            | 13710       |
|    policy_gradient_loss | -0.00176    |
|    value_loss           | 1.12        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.4     |
|    ep_rew_mean     | 6.83     |
| time/              |          |
|    fps             | 235      |
|    iterations      | 396      |
|    time_elapsed    | 3446     |
|    total_timesteps | 811008   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 8.12       |
| time/                   |            |
|    fps                  | 235        |
|    iterations           | 397        |
|    time_elapsed         | 3459       |
|    total_timesteps      | 813056     |
| train/                  |            |
|    approx_kl            | 0.10046529 |
|    clip_fraction        | 0.113      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.137     |
|    explained_variance   | 0.667      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.997      |
|    n_updates            | 13720      |
|    policy_gradient_loss | -0.0195    |
|    value_loss           | 1.6        |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.6        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 398         |
|    time_elapsed         | 3473        |
|    total_timesteps      | 815104      |
| train/                  |             |
|    approx_kl            | 0.039695013 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.262       |
|    n_updates            | 13730       |
|    policy_gradient_loss | -0.0116     |
|    value_loss           | 1.29        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.1       |
|    ep_rew_mean          | 7.32       |
| time/                   |            |
|    fps                  | 234        |
|    iterations           | 399        |
|    time_elapsed         | 3485       |
|    total_timesteps      | 817152     |
| train/                  |            |
|    approx_kl            | 0.16314855 |
|    clip_fraction        | 0.104      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.111     |
|    explained_variance   | 0.761      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 13740      |
|    policy_gradient_loss | -0.0179    |
|    value_loss           | 1.5        |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.06        |
| time/                   |             |
|    fps                  | 234         |
|    iterations           | 400         |
|    time_elapsed         | 3497        |
|    total_timesteps      | 819200      |
| train/                  |             |
|    approx_kl            | 0.039538197 |
|    clip_fraction        | 0.0578      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0888     |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.458       |
|    n_updates            | 13750       |
|    policy_gradient_loss | -0.00682    |
|    value_loss           | 0.998       |
-----------------------------------------
reached max steps=100
Eval num_timesteps=820000, episode_reward=1.75 +/- 8.31
Episode length: 27.00 +/- 16.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27         |
|    mean_reward          | 1.75       |
| time/                   |            |
|    total_timesteps      | 820000     |
| train/                  |            |
|    approx_kl            | 0.04914629 |
|    clip_fraction        | 0.0908     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.125     |
|    explained_variance   | 0.69       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.624      |
|    n_updates            | 13760      |
|    policy_gradient_loss | -0.00873   |
|    value_loss           | 1.61       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 7.5      |
| time/              |          |
|    fps             | 233      |
|    iterations      | 401      |
|    time_elapsed    | 3510     |
|    total_timesteps | 821248   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.1       |
|    ep_rew_mean          | 8.02       |
| time/                   |            |
|    fps                  | 233        |
|    iterations           | 402        |
|    time_elapsed         | 3522       |
|    total_timesteps      | 823296     |
| train/                  |            |
|    approx_kl            | 0.03718279 |
|    clip_fraction        | 0.0858     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.446      |
|    n_updates            | 13770      |
|    policy_gradient_loss | -0.00395   |
|    value_loss           | 1.47       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 7.28        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 403         |
|    time_elapsed         | 3535        |
|    total_timesteps      | 825344      |
| train/                  |             |
|    approx_kl            | 0.049943686 |
|    clip_fraction        | 0.102       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.611       |
|    n_updates            | 13780       |
|    policy_gradient_loss | 0.0127      |
|    value_loss           | 1.32        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 7.38        |
| time/                   |             |
|    fps                  | 233         |
|    iterations           | 404         |
|    time_elapsed         | 3548        |
|    total_timesteps      | 827392      |
| train/                  |             |
|    approx_kl            | 0.034833208 |
|    clip_fraction        | 0.0713      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.107      |
|    explained_variance   | 0.78        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.416       |
|    n_updates            | 13790       |
|    policy_gradient_loss | -0.00362    |
|    value_loss           | 1.29        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.4       |
|    ep_rew_mean          | 8.21       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 405        |
|    time_elapsed         | 3561       |
|    total_timesteps      | 829440     |
| train/                  |            |
|    approx_kl            | 0.08110646 |
|    clip_fraction        | 0.101      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.129     |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.605      |
|    n_updates            | 13800      |
|    policy_gradient_loss | -0.0112    |
|    value_loss           | 1.24       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=830000, episode_reward=0.30 +/- 7.52
Episode length: 26.00 +/- 16.99
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 26         |
|    mean_reward          | 0.299      |
| time/                   |            |
|    total_timesteps      | 830000     |
| train/                  |            |
|    approx_kl            | 0.05491412 |
|    clip_fraction        | 0.0909     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.782      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.529      |
|    n_updates            | 13810      |
|    policy_gradient_loss | -0.00816   |
|    value_loss           | 1.31       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.7     |
|    ep_rew_mean     | 7.09     |
| time/              |          |
|    fps             | 232      |
|    iterations      | 406      |
|    time_elapsed    | 3573     |
|    total_timesteps | 831488   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 19.2       |
|    ep_rew_mean          | 7.16       |
| time/                   |            |
|    fps                  | 232        |
|    iterations           | 407        |
|    time_elapsed         | 3586       |
|    total_timesteps      | 833536     |
| train/                  |            |
|    approx_kl            | 0.05762689 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.128     |
|    explained_variance   | 0.642      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.33       |
|    n_updates            | 13820      |
|    policy_gradient_loss | -0.00194   |
|    value_loss           | 1.85       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 6.57        |
| time/                   |             |
|    fps                  | 232         |
|    iterations           | 408         |
|    time_elapsed         | 3599        |
|    total_timesteps      | 835584      |
| train/                  |             |
|    approx_kl            | 0.042403534 |
|    clip_fraction        | 0.127       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.17       |
|    explained_variance   | 0.714       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.434       |
|    n_updates            | 13830       |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 1.38        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.1        |
|    ep_rew_mean          | 7.07        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 409         |
|    time_elapsed         | 3611        |
|    total_timesteps      | 837632      |
| train/                  |             |
|    approx_kl            | 0.062903546 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.218      |
|    explained_variance   | 0.683       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.509       |
|    n_updates            | 13840       |
|    policy_gradient_loss | -0.0241     |
|    value_loss           | 1.47        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 7.67        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 410         |
|    time_elapsed         | 3622        |
|    total_timesteps      | 839680      |
| train/                  |             |
|    approx_kl            | 0.036551576 |
|    clip_fraction        | 0.104       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.148      |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.43        |
|    n_updates            | 13850       |
|    policy_gradient_loss | -0.0161     |
|    value_loss           | 1.22        |
-----------------------------------------
reached max steps=100
Eval num_timesteps=840000, episode_reward=-1.44 +/- 6.07
Episode length: 26.33 +/- 16.86
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.3        |
|    mean_reward          | -1.44       |
| time/                   |             |
|    total_timesteps      | 840000      |
| train/                  |             |
|    approx_kl            | 0.038844675 |
|    clip_fraction        | 0.0907      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.72        |
|    learning_rate        | 0.0003      |
|    loss                 | 1.07        |
|    n_updates            | 13860       |
|    policy_gradient_loss | -0.0041     |
|    value_loss           | 1.51        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.6     |
|    ep_rew_mean     | 7.41     |
| time/              |          |
|    fps             | 231      |
|    iterations      | 411      |
|    time_elapsed    | 3635     |
|    total_timesteps | 841728   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 6.55        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 412         |
|    time_elapsed         | 3647        |
|    total_timesteps      | 843776      |
| train/                  |             |
|    approx_kl            | 0.048702806 |
|    clip_fraction        | 0.0866      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.113      |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.343       |
|    n_updates            | 13870       |
|    policy_gradient_loss | -0.00512    |
|    value_loss           | 0.842       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 7.09        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 413         |
|    time_elapsed         | 3657        |
|    total_timesteps      | 845824      |
| train/                  |             |
|    approx_kl            | 0.036217876 |
|    clip_fraction        | 0.139       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.172      |
|    explained_variance   | 0.752       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.278       |
|    n_updates            | 13880       |
|    policy_gradient_loss | -0.0133     |
|    value_loss           | 1.33        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.8        |
|    ep_rew_mean          | 7.49        |
| time/                   |             |
|    fps                  | 231         |
|    iterations           | 414         |
|    time_elapsed         | 3670        |
|    total_timesteps      | 847872      |
| train/                  |             |
|    approx_kl            | 0.044722717 |
|    clip_fraction        | 0.0932      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.266       |
|    n_updates            | 13890       |
|    policy_gradient_loss | -0.01       |
|    value_loss           | 0.923       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.6       |
|    ep_rew_mean          | 6.84       |
| time/                   |            |
|    fps                  | 230        |
|    iterations           | 415        |
|    time_elapsed         | 3682       |
|    total_timesteps      | 849920     |
| train/                  |            |
|    approx_kl            | 0.02793838 |
|    clip_fraction        | 0.0843     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.154     |
|    explained_variance   | 0.808      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.355      |
|    n_updates            | 13900      |
|    policy_gradient_loss | -0.00655   |
|    value_loss           | 0.992      |
----------------------------------------
Eval num_timesteps=850000, episode_reward=3.09 +/- 0.27
Episode length: 13.33 +/- 1.25
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.3        |
|    mean_reward          | 3.09        |
| time/                   |             |
|    total_timesteps      | 850000      |
| train/                  |             |
|    approx_kl            | 0.034856144 |
|    clip_fraction        | 0.0778      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0944      |
|    n_updates            | 13910       |
|    policy_gradient_loss | -0.00691    |
|    value_loss           | 1.06        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18.6     |
|    ep_rew_mean     | 7.22     |
| time/              |          |
|    fps             | 230      |
|    iterations      | 416      |
|    time_elapsed    | 3694     |
|    total_timesteps | 851968   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 7.65        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 417         |
|    time_elapsed         | 3707        |
|    total_timesteps      | 854016      |
| train/                  |             |
|    approx_kl            | 0.053944133 |
|    clip_fraction        | 0.0962      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.16       |
|    explained_variance   | 0.727       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.397       |
|    n_updates            | 13920       |
|    policy_gradient_loss | -0.00612    |
|    value_loss           | 1.56        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.9        |
|    ep_rew_mean          | 7.21        |
| time/                   |             |
|    fps                  | 230         |
|    iterations           | 418         |
|    time_elapsed         | 3720        |
|    total_timesteps      | 856064      |
| train/                  |             |
|    approx_kl            | 0.032177188 |
|    clip_fraction        | 0.0784      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.787       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.5         |
|    n_updates            | 13930       |
|    policy_gradient_loss | -0.00657    |
|    value_loss           | 1.16        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 6.95        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 419         |
|    time_elapsed         | 3733        |
|    total_timesteps      | 858112      |
| train/                  |             |
|    approx_kl            | 0.027739588 |
|    clip_fraction        | 0.0775      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.128      |
|    explained_variance   | 0.8         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.55        |
|    n_updates            | 13940       |
|    policy_gradient_loss | -0.00661    |
|    value_loss           | 1.02        |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=860000, episode_reward=12.15 +/- 3.82
Episode length: 17.67 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 12.1       |
| time/                   |            |
|    total_timesteps      | 860000     |
| train/                  |            |
|    approx_kl            | 0.10956675 |
|    clip_fraction        | 0.142      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.14      |
|    explained_variance   | 0.54       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.854      |
|    n_updates            | 13950      |
|    policy_gradient_loss | -0.00813   |
|    value_loss           | 1.92       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.4     |
|    ep_rew_mean     | 7.73     |
| time/              |          |
|    fps             | 229      |
|    iterations      | 420      |
|    time_elapsed    | 3745     |
|    total_timesteps | 860160   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 7.43        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 421         |
|    time_elapsed         | 3757        |
|    total_timesteps      | 862208      |
| train/                  |             |
|    approx_kl            | 0.030762529 |
|    clip_fraction        | 0.07        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.101      |
|    explained_variance   | 0.762       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.284       |
|    n_updates            | 13960       |
|    policy_gradient_loss | -0.00904    |
|    value_loss           | 1.35        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 7.28        |
| time/                   |             |
|    fps                  | 229         |
|    iterations           | 422         |
|    time_elapsed         | 3770        |
|    total_timesteps      | 864256      |
| train/                  |             |
|    approx_kl            | 0.039755538 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.715       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.295       |
|    n_updates            | 13970       |
|    policy_gradient_loss | -0.00545    |
|    value_loss           | 1.37        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 7.44        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 423         |
|    time_elapsed         | 3783        |
|    total_timesteps      | 866304      |
| train/                  |             |
|    approx_kl            | 0.042330522 |
|    clip_fraction        | 0.0882      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.12       |
|    explained_variance   | 0.755       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.41        |
|    n_updates            | 13980       |
|    policy_gradient_loss | -0.0118     |
|    value_loss           | 1.09        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.8       |
|    ep_rew_mean          | 7.34       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 424        |
|    time_elapsed         | 3795       |
|    total_timesteps      | 868352     |
| train/                  |            |
|    approx_kl            | 0.09781165 |
|    clip_fraction        | 0.109      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.115     |
|    explained_variance   | 0.719      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.586      |
|    n_updates            | 13990      |
|    policy_gradient_loss | -0.00832   |
|    value_loss           | 1.59       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=870000, episode_reward=5.08 +/- 7.30
Episode length: 27.00 +/- 16.39
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 27          |
|    mean_reward          | 5.08        |
| time/                   |             |
|    total_timesteps      | 870000      |
| train/                  |             |
|    approx_kl            | 0.030563759 |
|    clip_fraction        | 0.0841      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.132      |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.411       |
|    n_updates            | 14000       |
|    policy_gradient_loss | -0.0076     |
|    value_loss           | 0.902       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.6     |
|    ep_rew_mean     | 7.56     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 425      |
|    time_elapsed    | 3806     |
|    total_timesteps | 870400   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17         |
|    ep_rew_mean          | 7.06       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 426        |
|    time_elapsed         | 3816       |
|    total_timesteps      | 872448     |
| train/                  |            |
|    approx_kl            | 0.06333879 |
|    clip_fraction        | 0.084      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.787      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.403      |
|    n_updates            | 14010      |
|    policy_gradient_loss | -0.0072    |
|    value_loss           | 1.18       |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 17.3      |
|    ep_rew_mean          | 7.19      |
| time/                   |           |
|    fps                  | 228       |
|    iterations           | 427       |
|    time_elapsed         | 3826      |
|    total_timesteps      | 874496    |
| train/                  |           |
|    approx_kl            | 0.0382772 |
|    clip_fraction        | 0.0781    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.108    |
|    explained_variance   | 0.772     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.202     |
|    n_updates            | 14020     |
|    policy_gradient_loss | -0.00725  |
|    value_loss           | 0.978     |
---------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.7       |
|    ep_rew_mean          | 7.43       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 428        |
|    time_elapsed         | 3834       |
|    total_timesteps      | 876544     |
| train/                  |            |
|    approx_kl            | 0.05511228 |
|    clip_fraction        | 0.0857     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.12      |
|    explained_variance   | 0.805      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.712      |
|    n_updates            | 14030      |
|    policy_gradient_loss | -0.00449   |
|    value_loss           | 1.25       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.7       |
|    ep_rew_mean          | 7.28       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 429        |
|    time_elapsed         | 3844       |
|    total_timesteps      | 878592     |
| train/                  |            |
|    approx_kl            | 0.04045633 |
|    clip_fraction        | 0.0928     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.113     |
|    explained_variance   | 0.753      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.529      |
|    n_updates            | 14040      |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 1.01       |
----------------------------------------
reached max steps=100
Eval num_timesteps=880000, episode_reward=1.75 +/- 8.31
Episode length: 27.00 +/- 16.27
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 27         |
|    mean_reward          | 1.75       |
| time/                   |            |
|    total_timesteps      | 880000     |
| train/                  |            |
|    approx_kl            | 0.04703049 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.71       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.18       |
|    n_updates            | 14050      |
|    policy_gradient_loss | -0.0262    |
|    value_loss           | 0.899      |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 7.85     |
| time/              |          |
|    fps             | 228      |
|    iterations      | 430      |
|    time_elapsed    | 3856     |
|    total_timesteps | 880640   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.9       |
|    ep_rew_mean          | 7.87       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 431        |
|    time_elapsed         | 3866       |
|    total_timesteps      | 882688     |
| train/                  |            |
|    approx_kl            | 0.04236176 |
|    clip_fraction        | 0.0779     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0901    |
|    explained_variance   | 0.83       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.309      |
|    n_updates            | 14060      |
|    policy_gradient_loss | -0.00434   |
|    value_loss           | 0.876      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.1       |
|    ep_rew_mean          | 7.05       |
| time/                   |            |
|    fps                  | 228        |
|    iterations           | 432        |
|    time_elapsed         | 3876       |
|    total_timesteps      | 884736     |
| train/                  |            |
|    approx_kl            | 0.06965028 |
|    clip_fraction        | 0.0902     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.099     |
|    explained_variance   | 0.646      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.308      |
|    n_updates            | 14070      |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 0.95       |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 7.39        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 433         |
|    time_elapsed         | 3885        |
|    total_timesteps      | 886784      |
| train/                  |             |
|    approx_kl            | 0.061712988 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.122      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.288       |
|    n_updates            | 14080       |
|    policy_gradient_loss | -0.0109     |
|    value_loss           | 1.07        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.7        |
|    ep_rew_mean          | 7.28        |
| time/                   |             |
|    fps                  | 228         |
|    iterations           | 434         |
|    time_elapsed         | 3896        |
|    total_timesteps      | 888832      |
| train/                  |             |
|    approx_kl            | 0.041369334 |
|    clip_fraction        | 0.103       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.13       |
|    explained_variance   | 0.791       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.542       |
|    n_updates            | 14090       |
|    policy_gradient_loss | -0.00569    |
|    value_loss           | 0.966       |
-----------------------------------------
Eval num_timesteps=890000, episode_reward=10.92 +/- 2.42
Episode length: 15.67 +/- 1.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 15.7      |
|    mean_reward          | 10.9      |
| time/                   |           |
|    total_timesteps      | 890000    |
| train/                  |           |
|    approx_kl            | 0.0340314 |
|    clip_fraction        | 0.0974    |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.12     |
|    explained_variance   | 0.76      |
|    learning_rate        | 0.0003    |
|    loss                 | 0.373     |
|    n_updates            | 14100     |
|    policy_gradient_loss | -0.0128   |
|    value_loss           | 1.27      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.3     |
|    ep_rew_mean     | 8.11     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 435      |
|    time_elapsed    | 3907     |
|    total_timesteps | 890880   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.2       |
|    ep_rew_mean          | 7.59       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 436        |
|    time_elapsed         | 3917       |
|    total_timesteps      | 892928     |
| train/                  |            |
|    approx_kl            | 0.01739077 |
|    clip_fraction        | 0.0618     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.104     |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.347      |
|    n_updates            | 14110      |
|    policy_gradient_loss | -0.00578   |
|    value_loss           | 0.953      |
----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.4        |
|    ep_rew_mean          | 7.78        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 437         |
|    time_elapsed         | 3930        |
|    total_timesteps      | 894976      |
| train/                  |             |
|    approx_kl            | 0.052354883 |
|    clip_fraction        | 0.078       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.814       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.1         |
|    n_updates            | 14120       |
|    policy_gradient_loss | -0.0124     |
|    value_loss           | 0.959       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.4        |
|    ep_rew_mean          | 8.09        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 438         |
|    time_elapsed         | 3939        |
|    total_timesteps      | 897024      |
| train/                  |             |
|    approx_kl            | 0.042473488 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.15       |
|    explained_variance   | 0.798       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.469       |
|    n_updates            | 14130       |
|    policy_gradient_loss | -0.015      |
|    value_loss           | 1.16        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.1       |
|    ep_rew_mean          | 7.6        |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 439        |
|    time_elapsed         | 3948       |
|    total_timesteps      | 899072     |
| train/                  |            |
|    approx_kl            | 0.04018119 |
|    clip_fraction        | 0.0899     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.116     |
|    explained_variance   | 0.791      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.154      |
|    n_updates            | 14140      |
|    policy_gradient_loss | -0.00422   |
|    value_loss           | 0.989      |
----------------------------------------
Eval num_timesteps=900000, episode_reward=6.43 +/- 2.15
Episode length: 13.33 +/- 0.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.3        |
|    mean_reward          | 6.43        |
| time/                   |             |
|    total_timesteps      | 900000      |
| train/                  |             |
|    approx_kl            | 0.039620914 |
|    clip_fraction        | 0.0814      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.209       |
|    n_updates            | 14150       |
|    policy_gradient_loss | -0.00621    |
|    value_loss           | 0.919       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.6     |
|    ep_rew_mean     | 7.74     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 440      |
|    time_elapsed    | 3959     |
|    total_timesteps | 901120   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.1        |
|    ep_rew_mean          | 7.18        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 441         |
|    time_elapsed         | 3971        |
|    total_timesteps      | 903168      |
| train/                  |             |
|    approx_kl            | 0.052743826 |
|    clip_fraction        | 0.0812      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.116      |
|    explained_variance   | 0.766       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.344       |
|    n_updates            | 14160       |
|    policy_gradient_loss | 0.000439    |
|    value_loss           | 1.23        |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.9       |
|    ep_rew_mean          | 7.27       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 442        |
|    time_elapsed         | 3982       |
|    total_timesteps      | 905216     |
| train/                  |            |
|    approx_kl            | 0.03620123 |
|    clip_fraction        | 0.0813     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.107     |
|    explained_variance   | 0.835      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.369      |
|    n_updates            | 14170      |
|    policy_gradient_loss | -0.0129    |
|    value_loss           | 0.705      |
----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.7       |
|    ep_rew_mean          | 7.88       |
| time/                   |            |
|    fps                  | 227        |
|    iterations           | 443        |
|    time_elapsed         | 3993       |
|    total_timesteps      | 907264     |
| train/                  |            |
|    approx_kl            | 0.03445573 |
|    clip_fraction        | 0.0924     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.834      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.233      |
|    n_updates            | 14180      |
|    policy_gradient_loss | -0.0145    |
|    value_loss           | 0.944      |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.2        |
|    ep_rew_mean          | 8.44        |
| time/                   |             |
|    fps                  | 227         |
|    iterations           | 444         |
|    time_elapsed         | 4003        |
|    total_timesteps      | 909312      |
| train/                  |             |
|    approx_kl            | 0.035780508 |
|    clip_fraction        | 0.079       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.845       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.43        |
|    n_updates            | 14190       |
|    policy_gradient_loss | -0.00705    |
|    value_loss           | 0.864       |
-----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=910000, episode_reward=-0.61 +/- 7.43
Episode length: 37.67 +/- 17.44
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 37.7       |
|    mean_reward          | -0.611     |
| time/                   |            |
|    total_timesteps      | 910000     |
| train/                  |            |
|    approx_kl            | 0.13236515 |
|    clip_fraction        | 0.088      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0757    |
|    explained_variance   | 0.763      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.383      |
|    n_updates            | 14200      |
|    policy_gradient_loss | -0.0144    |
|    value_loss           | 1.23       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.7     |
|    ep_rew_mean     | 7.51     |
| time/              |          |
|    fps             | 227      |
|    iterations      | 445      |
|    time_elapsed    | 4014     |
|    total_timesteps | 911360   |
---------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.2       |
|    ep_rew_mean          | 8          |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 446        |
|    time_elapsed         | 4025       |
|    total_timesteps      | 913408     |
| train/                  |            |
|    approx_kl            | 0.03828413 |
|    clip_fraction        | 0.0711     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0855    |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.236      |
|    n_updates            | 14210      |
|    policy_gradient_loss | -0.00465   |
|    value_loss           | 0.86       |
----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.5        |
|    ep_rew_mean          | 7.58        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 447         |
|    time_elapsed         | 4036        |
|    total_timesteps      | 915456      |
| train/                  |             |
|    approx_kl            | 0.086854495 |
|    clip_fraction        | 0.0917      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.106      |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.231       |
|    n_updates            | 14220       |
|    policy_gradient_loss | -0.00706    |
|    value_loss           | 1.14        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 7.36        |
| time/                   |             |
|    fps                  | 226         |
|    iterations           | 448         |
|    time_elapsed         | 4050        |
|    total_timesteps      | 917504      |
| train/                  |             |
|    approx_kl            | 0.053475197 |
|    clip_fraction        | 0.0964      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.11       |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.415       |
|    n_updates            | 14230       |
|    policy_gradient_loss | -0.0122     |
|    value_loss           | 1.03        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.6       |
|    ep_rew_mean          | 8.24       |
| time/                   |            |
|    fps                  | 226        |
|    iterations           | 449        |
|    time_elapsed         | 4062       |
|    total_timesteps      | 919552     |
| train/                  |            |
|    approx_kl            | 0.03374297 |
|    clip_fraction        | 0.0907     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.384      |
|    n_updates            | 14240      |
|    policy_gradient_loss | -0.00564   |
|    value_loss           | 1.36       |
----------------------------------------
Eval num_timesteps=920000, episode_reward=9.40 +/- 2.36
Episode length: 15.00 +/- 0.82
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 9.4         |
| time/                   |             |
|    total_timesteps      | 920000      |
| train/                  |             |
|    approx_kl            | 0.028441072 |
|    clip_fraction        | 0.0599      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0948     |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.446       |
|    n_updates            | 14250       |
|    policy_gradient_loss | -0.00292    |
|    value_loss           | 1.19        |
-----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.8     |
|    ep_rew_mean     | 7.27     |
| time/              |          |
|    fps             | 225      |
|    iterations      | 450      |
|    time_elapsed    | 4078     |
|    total_timesteps | 921600   |
---------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | 6.7         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 451         |
|    time_elapsed         | 4090        |
|    total_timesteps      | 923648      |
| train/                  |             |
|    approx_kl            | 0.032383047 |
|    clip_fraction        | 0.0895      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.117      |
|    explained_variance   | 0.729       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.521       |
|    n_updates            | 14260       |
|    policy_gradient_loss | -0.00164    |
|    value_loss           | 1.5         |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.2        |
|    ep_rew_mean          | 6.82        |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 452         |
|    time_elapsed         | 4103        |
|    total_timesteps      | 925696      |
| train/                  |             |
|    approx_kl            | 0.048853662 |
|    clip_fraction        | 0.117       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.16       |
|    explained_variance   | 0.736       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.585       |
|    n_updates            | 14270       |
|    policy_gradient_loss | -0.0107     |
|    value_loss           | 1.67        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.3       |
|    ep_rew_mean          | 6.96       |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 453        |
|    time_elapsed         | 4115       |
|    total_timesteps      | 927744     |
| train/                  |            |
|    approx_kl            | 0.04129212 |
|    clip_fraction        | 0.122      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.165     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.317      |
|    n_updates            | 14280      |
|    policy_gradient_loss | -0.0156    |
|    value_loss           | 1.24       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.7       |
|    ep_rew_mean          | 7.85       |
| time/                   |            |
|    fps                  | 225        |
|    iterations           | 454        |
|    time_elapsed         | 4125       |
|    total_timesteps      | 929792     |
| train/                  |            |
|    approx_kl            | 0.03690387 |
|    clip_fraction        | 0.0888     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.133     |
|    explained_variance   | 0.795      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.472      |
|    n_updates            | 14290      |
|    policy_gradient_loss | -0.0103    |
|    value_loss           | 0.94       |
----------------------------------------
Eval num_timesteps=930000, episode_reward=11.06 +/- 4.25
Episode length: 15.00 +/- 2.16
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 15          |
|    mean_reward          | 11.1        |
| time/                   |             |
|    total_timesteps      | 930000      |
| train/                  |             |
|    approx_kl            | 0.038951784 |
|    clip_fraction        | 0.0981      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.129      |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.199       |
|    n_updates            | 14300       |
|    policy_gradient_loss | -0.012      |
|    value_loss           | 0.89        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.9     |
|    ep_rew_mean     | 7.82     |
| time/              |          |
|    fps             | 225      |
|    iterations      | 455      |
|    time_elapsed    | 4137     |
|    total_timesteps | 931840   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 7.8         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 456         |
|    time_elapsed         | 4149        |
|    total_timesteps      | 933888      |
| train/                  |             |
|    approx_kl            | 0.031469345 |
|    clip_fraction        | 0.0735      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.799       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.117       |
|    n_updates            | 14310       |
|    policy_gradient_loss | -0.00309    |
|    value_loss           | 1.12        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 7.3         |
| time/                   |             |
|    fps                  | 225         |
|    iterations           | 457         |
|    time_elapsed         | 4159        |
|    total_timesteps      | 935936      |
| train/                  |             |
|    approx_kl            | 0.093840465 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.5         |
|    n_updates            | 14320       |
|    policy_gradient_loss | -0.00739    |
|    value_loss           | 1.23        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.6        |
|    ep_rew_mean          | 6.8         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 458         |
|    time_elapsed         | 4170        |
|    total_timesteps      | 937984      |
| train/                  |             |
|    approx_kl            | 0.039690144 |
|    clip_fraction        | 0.0985      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.773       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.555       |
|    n_updates            | 14330       |
|    policy_gradient_loss | -0.0159     |
|    value_loss           | 0.96        |
-----------------------------------------
Eval num_timesteps=940000, episode_reward=7.95 +/- 0.36
Episode length: 14.00 +/- 1.63
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14          |
|    mean_reward          | 7.95        |
| time/                   |             |
|    total_timesteps      | 940000      |
| train/                  |             |
|    approx_kl            | 0.035632588 |
|    clip_fraction        | 0.0803      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.794       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.289       |
|    n_updates            | 14340       |
|    policy_gradient_loss | -0.00973    |
|    value_loss           | 0.83        |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.5     |
|    ep_rew_mean     | 7.67     |
| time/              |          |
|    fps             | 224      |
|    iterations      | 459      |
|    time_elapsed    | 4180     |
|    total_timesteps | 940032   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.8        |
|    ep_rew_mean          | 7.3         |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 460         |
|    time_elapsed         | 4192        |
|    total_timesteps      | 942080      |
| train/                  |             |
|    approx_kl            | 0.046915516 |
|    clip_fraction        | 0.0938      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.136      |
|    explained_variance   | 0.834       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.324       |
|    n_updates            | 14350       |
|    policy_gradient_loss | -0.00508    |
|    value_loss           | 0.742       |
-----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18         |
|    ep_rew_mean          | 6.97       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 461        |
|    time_elapsed         | 4204       |
|    total_timesteps      | 944128     |
| train/                  |            |
|    approx_kl            | 0.24626116 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.153     |
|    explained_variance   | 0.768      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.911      |
|    n_updates            | 14360      |
|    policy_gradient_loss | -0.0205    |
|    value_loss           | 1.16       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.2       |
|    ep_rew_mean          | 6.76       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 462        |
|    time_elapsed         | 4213       |
|    total_timesteps      | 946176     |
| train/                  |            |
|    approx_kl            | 0.04675775 |
|    clip_fraction        | 0.134      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.156     |
|    explained_variance   | 0.713      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.381      |
|    n_updates            | 14370      |
|    policy_gradient_loss | 4.27e-05   |
|    value_loss           | 1.16       |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.6       |
|    ep_rew_mean          | 7.56       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 463        |
|    time_elapsed         | 4225       |
|    total_timesteps      | 948224     |
| train/                  |            |
|    approx_kl            | 0.22492939 |
|    clip_fraction        | 0.125      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.112     |
|    explained_variance   | 0.796      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.447      |
|    n_updates            | 14380      |
|    policy_gradient_loss | -0.00894   |
|    value_loss           | 0.927      |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=950000, episode_reward=0.89 +/- 7.76
Episode length: 26.33 +/- 16.78
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 26.3        |
|    mean_reward          | 0.893       |
| time/                   |             |
|    total_timesteps      | 950000      |
| train/                  |             |
|    approx_kl            | 0.037204757 |
|    clip_fraction        | 0.088       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.137      |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.314       |
|    n_updates            | 14390       |
|    policy_gradient_loss | -0.00482    |
|    value_loss           | 1           |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.5     |
|    ep_rew_mean     | 7        |
| time/              |          |
|    fps             | 224      |
|    iterations      | 464      |
|    time_elapsed    | 4235     |
|    total_timesteps | 950272   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 7.76       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 465        |
|    time_elapsed         | 4244       |
|    total_timesteps      | 952320     |
| train/                  |            |
|    approx_kl            | 0.02781843 |
|    clip_fraction        | 0.118      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.163     |
|    explained_variance   | 0.759      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.34       |
|    n_updates            | 14400      |
|    policy_gradient_loss | -0.0211    |
|    value_loss           | 1.26       |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 7.06       |
| time/                   |            |
|    fps                  | 224        |
|    iterations           | 466        |
|    time_elapsed         | 4256       |
|    total_timesteps      | 954368     |
| train/                  |            |
|    approx_kl            | 0.04255315 |
|    clip_fraction        | 0.0993     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.136     |
|    explained_variance   | 0.803      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.369      |
|    n_updates            | 14410      |
|    policy_gradient_loss | -0.00694   |
|    value_loss           | 1.27       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.5        |
|    ep_rew_mean          | 6.67        |
| time/                   |             |
|    fps                  | 224         |
|    iterations           | 467         |
|    time_elapsed         | 4268        |
|    total_timesteps      | 956416      |
| train/                  |             |
|    approx_kl            | 0.042509742 |
|    clip_fraction        | 0.116       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.176      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.543       |
|    n_updates            | 14420       |
|    policy_gradient_loss | -0.0177     |
|    value_loss           | 1.34        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 468        |
|    time_elapsed         | 4279       |
|    total_timesteps      | 958464     |
| train/                  |            |
|    approx_kl            | 0.02678843 |
|    clip_fraction        | 0.0852     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.139     |
|    explained_variance   | 0.754      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.2        |
|    n_updates            | 14430      |
|    policy_gradient_loss | -0.00772   |
|    value_loss           | 0.969      |
----------------------------------------
reached max steps=100
Eval num_timesteps=960000, episode_reward=4.76 +/- 2.25
Episode length: 13.33 +/- 0.47
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 13.3        |
|    mean_reward          | 4.76        |
| time/                   |             |
|    total_timesteps      | 960000      |
| train/                  |             |
|    approx_kl            | 0.042025898 |
|    clip_fraction        | 0.118       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.168      |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.385       |
|    n_updates            | 14440       |
|    policy_gradient_loss | -0.011      |
|    value_loss           | 0.854       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.9     |
|    ep_rew_mean     | 7.06     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 469      |
|    time_elapsed    | 4294     |
|    total_timesteps | 960512   |
---------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.1       |
|    ep_rew_mean          | 6.99       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 470        |
|    time_elapsed         | 4308       |
|    total_timesteps      | 962560     |
| train/                  |            |
|    approx_kl            | 0.07193457 |
|    clip_fraction        | 0.131      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.153     |
|    explained_variance   | 0.771      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.5        |
|    n_updates            | 14450      |
|    policy_gradient_loss | -0.00529   |
|    value_loss           | 1.28       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17          |
|    ep_rew_mean          | 7.18        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 471         |
|    time_elapsed         | 4318        |
|    total_timesteps      | 964608      |
| train/                  |             |
|    approx_kl            | 0.038683392 |
|    clip_fraction        | 0.0926      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.144      |
|    explained_variance   | 0.783       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.411       |
|    n_updates            | 14460       |
|    policy_gradient_loss | -0.00873    |
|    value_loss           | 1.2         |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.8        |
|    ep_rew_mean          | 8.44        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 472         |
|    time_elapsed         | 4329        |
|    total_timesteps      | 966656      |
| train/                  |             |
|    approx_kl            | 0.049011104 |
|    clip_fraction        | 0.0857      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.125      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.329       |
|    n_updates            | 14470       |
|    policy_gradient_loss | -0.00275    |
|    value_loss           | 1.03        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 17.8       |
|    ep_rew_mean          | 7.26       |
| time/                   |            |
|    fps                  | 223        |
|    iterations           | 473        |
|    time_elapsed         | 4340       |
|    total_timesteps      | 968704     |
| train/                  |            |
|    approx_kl            | 0.04753852 |
|    clip_fraction        | 0.102      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.128     |
|    explained_variance   | 0.775      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.316      |
|    n_updates            | 14480      |
|    policy_gradient_loss | -0.00894   |
|    value_loss           | 1.19       |
----------------------------------------
reached max steps=100
reached max steps=100
Eval num_timesteps=970000, episode_reward=0.37 +/- 7.59
Episode length: 25.67 +/- 17.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 25.7      |
|    mean_reward          | 0.371     |
| time/                   |           |
|    total_timesteps      | 970000    |
| train/                  |           |
|    approx_kl            | 0.1409578 |
|    clip_fraction        | 0.145     |
|    clip_range           | 0.2       |
|    entropy_loss         | -0.149    |
|    explained_variance   | 0.641     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.38      |
|    n_updates            | 14490     |
|    policy_gradient_loss | -0.00982  |
|    value_loss           | 1.7       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 16.8     |
|    ep_rew_mean     | 7.31     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 474      |
|    time_elapsed    | 4352     |
|    total_timesteps | 970752   |
---------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 6.53        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 475         |
|    time_elapsed         | 4362        |
|    total_timesteps      | 972800      |
| train/                  |             |
|    approx_kl            | 0.035309747 |
|    clip_fraction        | 0.0908      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.111      |
|    explained_variance   | 0.623       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.378       |
|    n_updates            | 14500       |
|    policy_gradient_loss | -0.0185     |
|    value_loss           | 1.31        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.4       |
|    ep_rew_mean          | 7.79       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 476        |
|    time_elapsed         | 4373       |
|    total_timesteps      | 974848     |
| train/                  |            |
|    approx_kl            | 0.14335328 |
|    clip_fraction        | 0.138      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.151     |
|    explained_variance   | 0.688      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.395      |
|    n_updates            | 14510      |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 1.19       |
----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | 6.81       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 477        |
|    time_elapsed         | 4383       |
|    total_timesteps      | 976896     |
| train/                  |            |
|    approx_kl            | 0.05984799 |
|    clip_fraction        | 0.0768     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.0908    |
|    explained_variance   | 0.781      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.405      |
|    n_updates            | 14520      |
|    policy_gradient_loss | -0.00417   |
|    value_loss           | 0.889      |
----------------------------------------
reached max steps=100
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.3       |
|    ep_rew_mean          | 7.36       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 478        |
|    time_elapsed         | 4393       |
|    total_timesteps      | 978944     |
| train/                  |            |
|    approx_kl            | 0.08895996 |
|    clip_fraction        | 0.115      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.13      |
|    explained_variance   | 0.742      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.731      |
|    n_updates            | 14530      |
|    policy_gradient_loss | -0.0106    |
|    value_loss           | 1.16       |
----------------------------------------
reached max steps=100
Eval num_timesteps=980000, episode_reward=8.82 +/- 2.18
Episode length: 17.67 +/- 1.89
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 17.7       |
|    mean_reward          | 8.82       |
| time/                   |            |
|    total_timesteps      | 980000     |
| train/                  |            |
|    approx_kl            | 0.07071176 |
|    clip_fraction        | 0.121      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.144     |
|    explained_variance   | 0.628      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.504      |
|    n_updates            | 14540      |
|    policy_gradient_loss | 0.0156     |
|    value_loss           | 1.81       |
----------------------------------------
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18       |
|    ep_rew_mean     | 7.44     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 479      |
|    time_elapsed    | 4402     |
|    total_timesteps | 980992   |
---------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.5        |
|    ep_rew_mean          | 6.66        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 480         |
|    time_elapsed         | 4409        |
|    total_timesteps      | 983040      |
| train/                  |             |
|    approx_kl            | 0.071694344 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.141      |
|    explained_variance   | 0.746       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.163       |
|    n_updates            | 14550       |
|    policy_gradient_loss | -0.00334    |
|    value_loss           | 1.25        |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.7        |
|    ep_rew_mean          | 7.63        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 481         |
|    time_elapsed         | 4418        |
|    total_timesteps      | 985088      |
| train/                  |             |
|    approx_kl            | 0.045464408 |
|    clip_fraction        | 0.077       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0929     |
|    explained_variance   | 0.838       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.16        |
|    n_updates            | 14560       |
|    policy_gradient_loss | -0.0132     |
|    value_loss           | 0.824       |
-----------------------------------------
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.1        |
|    ep_rew_mean          | 7.48        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 482         |
|    time_elapsed         | 4425        |
|    total_timesteps      | 987136      |
| train/                  |             |
|    approx_kl            | 0.035798594 |
|    clip_fraction        | 0.0891      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | 0.663       |
|    n_updates            | 14570       |
|    policy_gradient_loss | -0.00732    |
|    value_loss           | 1.4         |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.4        |
|    ep_rew_mean          | 8.11        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 483         |
|    time_elapsed         | 4434        |
|    total_timesteps      | 989184      |
| train/                  |             |
|    approx_kl            | 0.049111426 |
|    clip_fraction        | 0.0899      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.102      |
|    explained_variance   | 0.797       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.613       |
|    n_updates            | 14580       |
|    policy_gradient_loss | -0.00701    |
|    value_loss           | 1.15        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
reached max steps=100
Eval num_timesteps=990000, episode_reward=-2.57 +/- 10.51
Episode length: 39.00 +/- 15.56
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 39         |
|    mean_reward          | -2.57      |
| time/                   |            |
|    total_timesteps      | 990000     |
| train/                  |            |
|    approx_kl            | 0.06507869 |
|    clip_fraction        | 0.111      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.124     |
|    explained_variance   | 0.737      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.505      |
|    n_updates            | 14590      |
|    policy_gradient_loss | -0.0141    |
|    value_loss           | 1.46       |
----------------------------------------
reached max steps=100
reached max steps=100
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 18       |
|    ep_rew_mean     | 7.49     |
| time/              |          |
|    fps             | 223      |
|    iterations      | 484      |
|    time_elapsed    | 4444     |
|    total_timesteps | 991232   |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 17.6        |
|    ep_rew_mean          | 6.96        |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 485         |
|    time_elapsed         | 4453        |
|    total_timesteps      | 993280      |
| train/                  |             |
|    approx_kl            | 0.057253733 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.14       |
|    explained_variance   | 0.759       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.524       |
|    n_updates            | 14600       |
|    policy_gradient_loss | -0.0236     |
|    value_loss           | 1.42        |
-----------------------------------------
reached max steps=100
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18          |
|    ep_rew_mean          | 6.23        |
| time/                   |             |
|    fps                  | 222         |
|    iterations           | 486         |
|    time_elapsed         | 4463        |
|    total_timesteps      | 995328      |
| train/                  |             |
|    approx_kl            | 0.036453925 |
|    clip_fraction        | 0.0748      |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.0963     |
|    explained_variance   | 0.761       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.519       |
|    n_updates            | 14610       |
|    policy_gradient_loss | -0.00644    |
|    value_loss           | 1.06        |
-----------------------------------------
reached max steps=100
reached max steps=100
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 16.8        |
|    ep_rew_mean          | 7           |
| time/                   |             |
|    fps                  | 223         |
|    iterations           | 487         |
|    time_elapsed         | 4472        |
|    total_timesteps      | 997376      |
| train/                  |             |
|    approx_kl            | 0.074581504 |
|    clip_fraction        | 0.111       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.135      |
|    explained_variance   | 0.678       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.401       |
|    n_updates            | 14620       |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 1.24        |
-----------------------------------------
reached max steps=100
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 16.9       |
|    ep_rew_mean          | 6.98       |
| time/                   |            |
|    fps                  | 222        |
|    iterations           | 488        |
|    time_elapsed         | 4482       |
|    total_timesteps      | 999424     |
| train/                  |            |
|    approx_kl            | 0.13015911 |
|    clip_fraction        | 0.0983     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.119     |
|    explained_variance   | 0.73       |
|    learning_rate        | 0.0003     |
|    loss                 | 0.168      |
|    n_updates            | 14630      |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 1.15       |
----------------------------------------
Eval num_timesteps=1000000, episode_reward=6.43 +/- 4.46
Episode length: 13.33 +/- 1.25
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 13.3       |
|    mean_reward          | 6.43       |
| time/                   |            |
|    total_timesteps      | 1000000    |
| train/                  |            |
|    approx_kl            | 0.05256951 |
|    clip_fraction        | 0.0776     |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.105     |
|    explained_variance   | 0.772      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.298      |
|    n_updates            | 14640      |
|    policy_gradient_loss | -0.0068    |
|    value_loss           | 1.05       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 15.8     |
|    ep_rew_mean     | 7.46     |
| time/              |          |
|    fps             | 222      |
|    iterations      | 489      |
|    time_elapsed    | 4491     |
|    total_timesteps | 1001472  |
---------------------------------
