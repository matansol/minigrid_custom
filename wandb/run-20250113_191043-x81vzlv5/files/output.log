Using cuda device
Wrapping the env in a DummyVecEnv.
Wrapping the env in a VecTransposeImage.
Observation space: Dict('image': Box(0, 255, (3, 7, 7), uint8))
cuda:0
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py:418: UserWarning: Training and eval env are not of the same type<stable_baselines3.common.vec_env.vec_transpose.VecTransposeImage object at 0x0000026465A215D0> != <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x0000026465A21360>
  warnings.warn("Training and eval env are not of the same type" f"{self.training_env} != {self.eval_env}")
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.grid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.grid` for environment variables or `env.get_wrapper_attr('grid')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.front_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.front_pos` for environment variables or `env.get_wrapper_attr('front_pos')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.[0m
  logger.warn(
C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.agent_pos to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.agent_pos` for environment variables or `env.get_wrapper_attr('agent_pos')` that will search the reminding wrappers.[0m
  logger.warn(
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 140      |
|    ep_rew_mean     | -28.4    |
| time/              |          |
|    fps             | 235      |
|    iterations      | 1        |
|    time_elapsed    | 8        |
|    total_timesteps | 2048     |
---------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 140        |
|    ep_rew_mean          | -29.9      |
| time/                   |            |
|    fps                  | 180        |
|    iterations           | 2          |
|    time_elapsed         | 22         |
|    total_timesteps      | 4096       |
| train/                  |            |
|    approx_kl            | 0.01344811 |
|    clip_fraction        | 0.0947     |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.94      |
|    explained_variance   | -0.00422   |
|    learning_rate        | 0.001      |
|    loss                 | -0.0567    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.00545   |
|    value_loss           | 0.393      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 141        |
|    ep_rew_mean          | -30        |
| time/                   |            |
|    fps                  | 165        |
|    iterations           | 3          |
|    time_elapsed         | 37         |
|    total_timesteps      | 6144       |
| train/                  |            |
|    approx_kl            | 0.01528268 |
|    clip_fraction        | 0.143      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.93      |
|    explained_variance   | 0.0504     |
|    learning_rate        | 0.001      |
|    loss                 | -0.0275    |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0215    |
|    value_loss           | 0.546      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 143          |
|    ep_rew_mean          | -30.2        |
| time/                   |              |
|    fps                  | 167          |
|    iterations           | 4            |
|    time_elapsed         | 48           |
|    total_timesteps      | 8192         |
| train/                  |              |
|    approx_kl            | 0.0140082305 |
|    clip_fraction        | 0.15         |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.92        |
|    explained_variance   | -0.023       |
|    learning_rate        | 0.001        |
|    loss                 | -0.0476      |
|    n_updates            | 30           |
|    policy_gradient_loss | -0.0174      |
|    value_loss           | 0.285        |
------------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
Eval num_timesteps=10000, episode_reward=-30.00 +/- 0.00
Episode length: 150.00 +/- 0.00
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 150        |
|    mean_reward          | -30        |
| time/                   |            |
|    total_timesteps      | 10000      |
| train/                  |            |
|    approx_kl            | 0.01760916 |
|    clip_fraction        | 0.183      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.9       |
|    explained_variance   | -0.392     |
|    learning_rate        | 0.001      |
|    loss                 | -0.0218    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0173    |
|    value_loss           | 0.143      |
----------------------------------------
New best mean reward!
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 139      |
|    ep_rew_mean     | -29      |
| time/              |          |
|    fps             | 163      |
|    iterations      | 5        |
|    time_elapsed    | 62       |
|    total_timesteps | 10240    |
---------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 132         |
|    ep_rew_mean          | -26.8       |
| time/                   |             |
|    fps                  | 163         |
|    iterations           | 6           |
|    time_elapsed         | 75          |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.018317875 |
|    clip_fraction        | 0.202       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.88       |
|    explained_variance   | -0.0308     |
|    learning_rate        | 0.001       |
|    loss                 | 0.432       |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0213     |
|    value_loss           | 0.356       |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 127        |
|    ep_rew_mean          | -25.1      |
| time/                   |            |
|    fps                  | 165        |
|    iterations           | 7          |
|    time_elapsed         | 86         |
|    total_timesteps      | 14336      |
| train/                  |            |
|    approx_kl            | 0.01983136 |
|    clip_fraction        | 0.237      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.87      |
|    explained_variance   | 0.204      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0463     |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0151    |
|    value_loss           | 0.255      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 123         |
|    ep_rew_mean          | -23.3       |
| time/                   |             |
|    fps                  | 166         |
|    iterations           | 8           |
|    time_elapsed         | 98          |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.024279982 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.541       |
|    learning_rate        | 0.001       |
|    loss                 | 0.00812     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0251     |
|    value_loss           | 0.247       |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 116         |
|    ep_rew_mean          | -20.9       |
| time/                   |             |
|    fps                  | 164         |
|    iterations           | 9           |
|    time_elapsed         | 111         |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.030637166 |
|    clip_fraction        | 0.298       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.83       |
|    explained_variance   | 0.407       |
|    learning_rate        | 0.001       |
|    loss                 | 0.00494     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.12        |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
Eval num_timesteps=20000, episode_reward=-30.00 +/- 0.00
Episode length: 150.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 150         |
|    mean_reward          | -30         |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.031254932 |
|    clip_fraction        | 0.267       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.502       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0531     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.206       |
-----------------------------------------
reached max steps=300
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 111      |
|    ep_rew_mean     | -19      |
| time/              |          |
|    fps             | 162      |
|    iterations      | 10       |
|    time_elapsed    | 125      |
|    total_timesteps | 20480    |
---------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 108         |
|    ep_rew_mean          | -17.5       |
| time/                   |             |
|    fps                  | 162         |
|    iterations           | 11          |
|    time_elapsed         | 138         |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.030777693 |
|    clip_fraction        | 0.273       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.77       |
|    explained_variance   | 0.546       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0181      |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0165     |
|    value_loss           | 0.178       |
-----------------------------------------
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 99.8        |
|    ep_rew_mean          | -15.4       |
| time/                   |             |
|    fps                  | 160         |
|    iterations           | 12          |
|    time_elapsed         | 153         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.026622694 |
|    clip_fraction        | 0.315       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.76       |
|    explained_variance   | 0.664       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0101      |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0257     |
|    value_loss           | 0.189       |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 96.4        |
|    ep_rew_mean          | -14.8       |
| time/                   |             |
|    fps                  | 158         |
|    iterations           | 13          |
|    time_elapsed         | 168         |
|    total_timesteps      | 26624       |
| train/                  |             |
|    approx_kl            | 0.031469986 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.69       |
|    explained_variance   | 0.619       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0953      |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0281     |
|    value_loss           | 0.251       |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 92.6        |
|    ep_rew_mean          | -14         |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 14          |
|    time_elapsed         | 184         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.034527346 |
|    clip_fraction        | 0.313       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.71       |
|    explained_variance   | 0.603       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0157     |
|    n_updates            | 130         |
|    policy_gradient_loss | -0.022      |
|    value_loss           | 0.202       |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
Eval num_timesteps=30000, episode_reward=-23.29 +/- 13.41
Episode length: 122.40 +/- 55.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 122         |
|    mean_reward          | -23.3       |
| time/                   |             |
|    total_timesteps      | 30000       |
| train/                  |             |
|    approx_kl            | 0.046767607 |
|    clip_fraction        | 0.336       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.66       |
|    explained_variance   | 0.58        |
|    learning_rate        | 0.001       |
|    loss                 | -0.0448     |
|    n_updates            | 140         |
|    policy_gradient_loss | -0.0232     |
|    value_loss           | 0.231       |
-----------------------------------------
New best mean reward!
reached max steps=300
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 84       |
|    ep_rew_mean     | -12.2    |
| time/              |          |
|    fps             | 153      |
|    iterations      | 15       |
|    time_elapsed    | 200      |
|    total_timesteps | 30720    |
---------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 83.7        |
|    ep_rew_mean          | -12.2       |
| time/                   |             |
|    fps                  | 154         |
|    iterations           | 16          |
|    time_elapsed         | 212         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.040796176 |
|    clip_fraction        | 0.35        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.59       |
|    explained_variance   | 0.732       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0623      |
|    n_updates            | 150         |
|    policy_gradient_loss | -0.0174     |
|    value_loss           | 0.21        |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 78.7       |
|    ep_rew_mean          | -10.7      |
| time/                   |            |
|    fps                  | 154        |
|    iterations           | 17         |
|    time_elapsed         | 225        |
|    total_timesteps      | 34816      |
| train/                  |            |
|    approx_kl            | 0.04010514 |
|    clip_fraction        | 0.368      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.58      |
|    explained_variance   | 0.709      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0589    |
|    n_updates            | 160        |
|    policy_gradient_loss | -0.0254    |
|    value_loss           | 0.196      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 66.3        |
|    ep_rew_mean          | -7.87       |
| time/                   |             |
|    fps                  | 155         |
|    iterations           | 18          |
|    time_elapsed         | 237         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.040012933 |
|    clip_fraction        | 0.368       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.52       |
|    explained_variance   | 0.742       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0406      |
|    n_updates            | 170         |
|    policy_gradient_loss | -0.0171     |
|    value_loss           | 0.205       |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 57.4       |
|    ep_rew_mean          | -5.64      |
| time/                   |            |
|    fps                  | 156        |
|    iterations           | 19         |
|    time_elapsed         | 249        |
|    total_timesteps      | 38912      |
| train/                  |            |
|    approx_kl            | 0.04248796 |
|    clip_fraction        | 0.375      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.42      |
|    explained_variance   | 0.756      |
|    learning_rate        | 0.001      |
|    loss                 | 0.112      |
|    n_updates            | 180        |
|    policy_gradient_loss | -0.0198    |
|    value_loss           | 0.27       |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
Eval num_timesteps=40000, episode_reward=-9.92 +/- 16.39
Episode length: 67.20 +/- 67.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.2        |
|    mean_reward          | -9.92       |
| time/                   |             |
|    total_timesteps      | 40000       |
| train/                  |             |
|    approx_kl            | 0.042965714 |
|    clip_fraction        | 0.373       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.39       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0331     |
|    n_updates            | 190         |
|    policy_gradient_loss | -0.016      |
|    value_loss           | 0.204       |
-----------------------------------------
New best mean reward!
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 55.7     |
|    ep_rew_mean     | -5.49    |
| time/              |          |
|    fps             | 157      |
|    iterations      | 20       |
|    time_elapsed    | 259      |
|    total_timesteps | 40960    |
---------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 58         |
|    ep_rew_mean          | -5.96      |
| time/                   |            |
|    fps                  | 159        |
|    iterations           | 21         |
|    time_elapsed         | 270        |
|    total_timesteps      | 43008      |
| train/                  |            |
|    approx_kl            | 0.06201899 |
|    clip_fraction        | 0.373      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.817      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0195    |
|    n_updates            | 200        |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.217      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 58.3        |
|    ep_rew_mean          | -6.3        |
| time/                   |             |
|    fps                  | 160         |
|    iterations           | 22          |
|    time_elapsed         | 280         |
|    total_timesteps      | 45056       |
| train/                  |             |
|    approx_kl            | 0.044553608 |
|    clip_fraction        | 0.353       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.28       |
|    explained_variance   | 0.792       |
|    learning_rate        | 0.001       |
|    loss                 | -0.00544    |
|    n_updates            | 210         |
|    policy_gradient_loss | -0.0152     |
|    value_loss           | 0.217       |
-----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 54.5       |
|    ep_rew_mean          | -5.18      |
| time/                   |            |
|    fps                  | 161        |
|    iterations           | 23         |
|    time_elapsed         | 291        |
|    total_timesteps      | 47104      |
| train/                  |            |
|    approx_kl            | 0.04994741 |
|    clip_fraction        | 0.393      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.37      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.001      |
|    loss                 | 0.00327    |
|    n_updates            | 220        |
|    policy_gradient_loss | -0.0233    |
|    value_loss           | 0.111      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 50.8       |
|    ep_rew_mean          | -4.18      |
| time/                   |            |
|    fps                  | 163        |
|    iterations           | 24         |
|    time_elapsed         | 301        |
|    total_timesteps      | 49152      |
| train/                  |            |
|    approx_kl            | 0.10030981 |
|    clip_fraction        | 0.38       |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.25      |
|    explained_variance   | 0.758      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0391     |
|    n_updates            | 230        |
|    policy_gradient_loss | -0.0303    |
|    value_loss           | 0.294      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
Eval num_timesteps=50000, episode_reward=-15.79 +/- 15.84
Episode length: 94.80 +/- 67.61
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 94.8       |
|    mean_reward          | -15.8      |
| time/                   |            |
|    total_timesteps      | 50000      |
| train/                  |            |
|    approx_kl            | 0.04781628 |
|    clip_fraction        | 0.355      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.32      |
|    explained_variance   | 0.822      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0677     |
|    n_updates            | 240        |
|    policy_gradient_loss | -0.0146    |
|    value_loss           | 0.19       |
----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.7     |
|    ep_rew_mean     | -0.898   |
| time/              |          |
|    fps             | 164      |
|    iterations      | 25       |
|    time_elapsed    | 311      |
|    total_timesteps | 51200    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 27.4        |
|    ep_rew_mean          | 1.15        |
| time/                   |             |
|    fps                  | 163         |
|    iterations           | 26          |
|    time_elapsed         | 325         |
|    total_timesteps      | 53248       |
| train/                  |             |
|    approx_kl            | 0.054448057 |
|    clip_fraction        | 0.325       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.11       |
|    explained_variance   | 0.733       |
|    learning_rate        | 0.001       |
|    loss                 | 0.234       |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.0212     |
|    value_loss           | 0.484       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 28         |
|    ep_rew_mean          | 0.955      |
| time/                   |            |
|    fps                  | 164        |
|    iterations           | 27         |
|    time_elapsed         | 336        |
|    total_timesteps      | 55296      |
| train/                  |            |
|    approx_kl            | 0.08207683 |
|    clip_fraction        | 0.387      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.02      |
|    explained_variance   | 0.679      |
|    learning_rate        | 0.001      |
|    loss                 | 0.145      |
|    n_updates            | 260        |
|    policy_gradient_loss | -0.0372    |
|    value_loss           | 0.394      |
----------------------------------------
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 25.9        |
|    ep_rew_mean          | 1.47        |
| time/                   |             |
|    fps                  | 165         |
|    iterations           | 28          |
|    time_elapsed         | 347         |
|    total_timesteps      | 57344       |
| train/                  |             |
|    approx_kl            | 0.075504676 |
|    clip_fraction        | 0.343       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.968      |
|    explained_variance   | 0.745       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0367      |
|    n_updates            | 270         |
|    policy_gradient_loss | 0.0102      |
|    value_loss           | 0.367       |
-----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 24.5       |
|    ep_rew_mean          | 1.55       |
| time/                   |            |
|    fps                  | 166        |
|    iterations           | 29         |
|    time_elapsed         | 357        |
|    total_timesteps      | 59392      |
| train/                  |            |
|    approx_kl            | 0.06472404 |
|    clip_fraction        | 0.33       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.998     |
|    explained_variance   | 0.824      |
|    learning_rate        | 0.001      |
|    loss                 | 0.11       |
|    n_updates            | 280        |
|    policy_gradient_loss | -0.0204    |
|    value_loss           | 0.288      |
----------------------------------------
reached max steps=300
reached max steps=300
Eval num_timesteps=60000, episode_reward=-9.08 +/- 17.14
Episode length: 67.20 +/- 67.61
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 67.2        |
|    mean_reward          | -9.08       |
| time/                   |             |
|    total_timesteps      | 60000       |
| train/                  |             |
|    approx_kl            | 0.053420454 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.8        |
|    explained_variance   | 0.843       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0529     |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.0192     |
|    value_loss           | 0.328       |
-----------------------------------------
New best mean reward!
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.7     |
|    ep_rew_mean     | 2.25     |
| time/              |          |
|    fps             | 166      |
|    iterations      | 30       |
|    time_elapsed    | 369      |
|    total_timesteps | 61440    |
---------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.2       |
|    ep_rew_mean          | 1.55       |
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 31         |
|    time_elapsed         | 379        |
|    total_timesteps      | 63488      |
| train/                  |            |
|    approx_kl            | 0.06369245 |
|    clip_fraction        | 0.275      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.661     |
|    explained_variance   | 0.877      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0157     |
|    n_updates            | 300        |
|    policy_gradient_loss | -0.0166    |
|    value_loss           | 0.288      |
----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 26.2       |
|    ep_rew_mean          | 1.05       |
| time/                   |            |
|    fps                  | 167        |
|    iterations           | 32         |
|    time_elapsed         | 391        |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.07926345 |
|    clip_fraction        | 0.325      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.815     |
|    explained_variance   | 0.858      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0438     |
|    n_updates            | 310        |
|    policy_gradient_loss | -0.0148    |
|    value_loss           | 0.27       |
----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 25.5       |
|    ep_rew_mean          | 1.1        |
| time/                   |            |
|    fps                  | 168        |
|    iterations           | 33         |
|    time_elapsed         | 401        |
|    total_timesteps      | 67584      |
| train/                  |            |
|    approx_kl            | 0.08017237 |
|    clip_fraction        | 0.321      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.72      |
|    explained_variance   | 0.883      |
|    learning_rate        | 0.001      |
|    loss                 | 0.158      |
|    n_updates            | 320        |
|    policy_gradient_loss | -0.0213    |
|    value_loss           | 0.207      |
----------------------------------------
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 28.5        |
|    ep_rew_mean          | 0.872       |
| time/                   |             |
|    fps                  | 169         |
|    iterations           | 34          |
|    time_elapsed         | 411         |
|    total_timesteps      | 69632       |
| train/                  |             |
|    approx_kl            | 0.072202444 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.804      |
|    explained_variance   | 0.864       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0177     |
|    n_updates            | 330         |
|    policy_gradient_loss | -0.00457    |
|    value_loss           | 0.216       |
-----------------------------------------
reached max steps=300
Eval num_timesteps=70000, episode_reward=3.55 +/- 0.25
Episode length: 11.80 +/- 1.17
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 11.8        |
|    mean_reward          | 3.55        |
| time/                   |             |
|    total_timesteps      | 70000       |
| train/                  |             |
|    approx_kl            | 0.051418908 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.809      |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0178     |
|    n_updates            | 340         |
|    policy_gradient_loss | -0.0105     |
|    value_loss           | 0.218       |
-----------------------------------------
New best mean reward!
reached max steps=300
reached max steps=300
reached max steps=300
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 29.3     |
|    ep_rew_mean     | 0.272    |
| time/              |          |
|    fps             | 169      |
|    iterations      | 35       |
|    time_elapsed    | 422      |
|    total_timesteps | 71680    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.1        |
|    ep_rew_mean          | 2.77        |
| time/                   |             |
|    fps                  | 169         |
|    iterations           | 36          |
|    time_elapsed         | 434         |
|    total_timesteps      | 73728       |
| train/                  |             |
|    approx_kl            | 0.054797865 |
|    clip_fraction        | 0.295       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.936      |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.001       |
|    loss                 | 0.096       |
|    n_updates            | 350         |
|    policy_gradient_loss | -0.0138     |
|    value_loss           | 0.212       |
-----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 23.6       |
|    ep_rew_mean          | 1.61       |
| time/                   |            |
|    fps                  | 169        |
|    iterations           | 37         |
|    time_elapsed         | 445        |
|    total_timesteps      | 75776      |
| train/                  |            |
|    approx_kl            | 0.07616948 |
|    clip_fraction        | 0.28       |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.626     |
|    explained_variance   | 0.842      |
|    learning_rate        | 0.001      |
|    loss                 | 0.149      |
|    n_updates            | 360        |
|    policy_gradient_loss | -0.0267    |
|    value_loss           | 0.32       |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.3       |
|    ep_rew_mean          | 0.127      |
| time/                   |            |
|    fps                  | 170        |
|    iterations           | 38         |
|    time_elapsed         | 457        |
|    total_timesteps      | 77824      |
| train/                  |            |
|    approx_kl            | 0.05432959 |
|    clip_fraction        | 0.307      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.814     |
|    explained_variance   | 0.848      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0662     |
|    n_updates            | 370        |
|    policy_gradient_loss | -0.0218    |
|    value_loss           | 0.334      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 25.2       |
|    ep_rew_mean          | 0.81       |
| time/                   |            |
|    fps                  | 170        |
|    iterations           | 39         |
|    time_elapsed         | 467        |
|    total_timesteps      | 79872      |
| train/                  |            |
|    approx_kl            | 0.08103797 |
|    clip_fraction        | 0.346      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.872     |
|    explained_variance   | 0.827      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0234     |
|    n_updates            | 380        |
|    policy_gradient_loss | -0.0214    |
|    value_loss           | 0.318      |
----------------------------------------
reached max steps=300
reached max steps=300
Eval num_timesteps=80000, episode_reward=-9.82 +/- 16.48
Episode length: 66.80 +/- 67.93
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 66.8       |
|    mean_reward          | -9.82      |
| time/                   |            |
|    total_timesteps      | 80000      |
| train/                  |            |
|    approx_kl            | 0.10492898 |
|    clip_fraction        | 0.319      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.781     |
|    explained_variance   | 0.891      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0934     |
|    n_updates            | 390        |
|    policy_gradient_loss | -0.0318    |
|    value_loss           | 0.217      |
----------------------------------------
reached max steps=300
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 21.5     |
|    ep_rew_mean     | 1.72     |
| time/              |          |
|    fps             | 171      |
|    iterations      | 40       |
|    time_elapsed    | 478      |
|    total_timesteps | 81920    |
---------------------------------
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 22.7        |
|    ep_rew_mean          | 1.6         |
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 41          |
|    time_elapsed         | 490         |
|    total_timesteps      | 83968       |
| train/                  |             |
|    approx_kl            | 0.071403496 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.748      |
|    explained_variance   | 0.897       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0467      |
|    n_updates            | 400         |
|    policy_gradient_loss | -0.0299     |
|    value_loss           | 0.247       |
-----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 19.8        |
|    ep_rew_mean          | 2.26        |
| time/                   |             |
|    fps                  | 171         |
|    iterations           | 42          |
|    time_elapsed         | 501         |
|    total_timesteps      | 86016       |
| train/                  |             |
|    approx_kl            | 0.047548935 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.82       |
|    explained_variance   | 0.92        |
|    learning_rate        | 0.001       |
|    loss                 | 0.189       |
|    n_updates            | 410         |
|    policy_gradient_loss | -0.0249     |
|    value_loss           | 0.18        |
-----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.6       |
|    ep_rew_mean          | 2.43       |
| time/                   |            |
|    fps                  | 171        |
|    iterations           | 43         |
|    time_elapsed         | 512        |
|    total_timesteps      | 88064      |
| train/                  |            |
|    approx_kl            | 0.05897353 |
|    clip_fraction        | 0.265      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.701     |
|    explained_variance   | 0.929      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0446    |
|    n_updates            | 420        |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 0.16       |
----------------------------------------
reached max steps=300
Eval num_timesteps=90000, episode_reward=-3.22 +/- 13.39
Episode length: 39.60 +/- 55.20
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 39.6        |
|    mean_reward          | -3.22       |
| time/                   |             |
|    total_timesteps      | 90000       |
| train/                  |             |
|    approx_kl            | 0.061893903 |
|    clip_fraction        | 0.276       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.535      |
|    explained_variance   | 0.887       |
|    learning_rate        | 0.001       |
|    loss                 | -0.0319     |
|    n_updates            | 430         |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 0.163       |
-----------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 19.2     |
|    ep_rew_mean     | 2.86     |
| time/              |          |
|    fps             | 172      |
|    iterations      | 44       |
|    time_elapsed    | 523      |
|    total_timesteps | 90112    |
---------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 23.4        |
|    ep_rew_mean          | 1.84        |
| time/                   |             |
|    fps                  | 172         |
|    iterations           | 45          |
|    time_elapsed         | 533         |
|    total_timesteps      | 92160       |
| train/                  |             |
|    approx_kl            | 0.076190114 |
|    clip_fraction        | 0.252       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.586      |
|    explained_variance   | 0.846       |
|    learning_rate        | 0.001       |
|    loss                 | 0.0432      |
|    n_updates            | 440         |
|    policy_gradient_loss | -0.0205     |
|    value_loss           | 0.315       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 20.3       |
|    ep_rew_mean          | 2.59       |
| time/                   |            |
|    fps                  | 173        |
|    iterations           | 46         |
|    time_elapsed         | 543        |
|    total_timesteps      | 94208      |
| train/                  |            |
|    approx_kl            | 0.09392417 |
|    clip_fraction        | 0.349      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.763     |
|    explained_variance   | 0.778      |
|    learning_rate        | 0.001      |
|    loss                 | 0.061      |
|    n_updates            | 450        |
|    policy_gradient_loss | -0.0251    |
|    value_loss           | 0.33       |
----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 21         |
|    ep_rew_mean          | 2.41       |
| time/                   |            |
|    fps                  | 173        |
|    iterations           | 47         |
|    time_elapsed         | 553        |
|    total_timesteps      | 96256      |
| train/                  |            |
|    approx_kl            | 0.07842989 |
|    clip_fraction        | 0.296      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.697     |
|    explained_variance   | 0.849      |
|    learning_rate        | 0.001      |
|    loss                 | 0.0786     |
|    n_updates            | 460        |
|    policy_gradient_loss | -0.0274    |
|    value_loss           | 0.292      |
----------------------------------------
reached max steps=300
reached max steps=300
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 18.4       |
|    ep_rew_mean          | 2.99       |
| time/                   |            |
|    fps                  | 173        |
|    iterations           | 48         |
|    time_elapsed         | 565        |
|    total_timesteps      | 98304      |
| train/                  |            |
|    approx_kl            | 0.07066615 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.731     |
|    explained_variance   | 0.861      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0469    |
|    n_updates            | 470        |
|    policy_gradient_loss | -0.0284    |
|    value_loss           | 0.207      |
----------------------------------------
reached max steps=300
reached max steps=300
reached max steps=300
Eval num_timesteps=100000, episode_reward=-17.08 +/- 15.82
Episode length: 97.20 +/- 64.67
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 97.2       |
|    mean_reward          | -17.1      |
| time/                   |            |
|    total_timesteps      | 100000     |
| train/                  |            |
|    approx_kl            | 0.07100986 |
|    clip_fraction        | 0.233      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.635     |
|    explained_variance   | 0.863      |
|    learning_rate        | 0.001      |
|    loss                 | -0.0162    |
|    n_updates            | 480        |
|    policy_gradient_loss | -0.0202    |
|    value_loss           | 0.263      |
----------------------------------------
Traceback (most recent call last):
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 359, in <module>
    main()
  File "C:\Users\matan\master_thesis\minigrid_custom\minigrid_custom_train.py", line 311, in main
    model.learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\ppo\ppo.py", line 311, in learn
    return super().learn(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 323, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\on_policy_algorithm.py", line 224, in collect_rollouts
    if not callback.on_step():
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 223, in _on_step
    continue_training = callback.on_step() and continue_training
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\stable_baselines3\common\callbacks.py", line 114, in on_step
    return self._on_step()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\integration\sb3\sb3.py", line 136, in _on_step
    self.save_model()
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\integration\sb3\sb3.py", line 145, in save_model
    wandb.save(self.path, base_path=self.model_save_path)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 392, in wrapper_fn
    return func(self, *args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 382, in wrapper
    return func(self, *args, **kwargs)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 1963, in save
    return self._save(
  File "C:\Users\matan\anaconda3\envs\master_env\lib\site-packages\wandb\sdk\wandb_run.py", line 2022, in _save
    target_path.symlink_to(source_path)
  File "C:\Users\matan\anaconda3\envs\master_env\lib\pathlib.py", line 1255, in symlink_to
    self._accessor.symlink(target, self, target_is_directory)
OSError: [WinError 1314] A required privilege is not held by the client: 'C:\\Users\\matan\\master_thesis\\minigrid_custom\\models\\wandb_models\\-0.1,4,-0.1,-4,0.2\\model.zip' -> 'C:\\Users\\matan\\master_thesis\\minigrid_custom\\wandb\\run-20250113_191043-x81vzlv5\\files\\model.zip'
